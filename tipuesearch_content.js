var tipuesearch = {"pages":[{"title":"Predicting the winner for English Premier League football matches","text":"The goal of this project is to predict the winner of a English Premier League match using machine learning. In a previous project, we web-scrapped the relevant data for EPL matches. The project can be found here . The web-scrapped data was saved into a file named \"matches.csv\". We directly use this file to import the data and build a machine learning model to predict the winner of a match. In [2]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline In [3]: match_df = pd . read_csv ( \"matches.csv\" , index_col = 0 ) Exploratory Data Analysis In [4]: match_df . head () Out[4]: date time comp round day venue result gf ga opponent ... match report notes sh sot dist fk pk pkatt season team 1 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham ... Match Report NaN 18.0 4.0 17.3 1.0 0.0 0.0 2022 Manchester City 2 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City ... Match Report NaN 16.0 4.0 18.5 1.0 0.0 0.0 2022 Manchester City 3 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal ... Match Report NaN 25.0 10.0 14.8 0.0 0.0 0.0 2022 Manchester City 4 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City ... Match Report NaN 25.0 8.0 14.3 0.0 0.0 0.0 2022 Manchester City 6 2021-09-18 15:00 Premier League Matchweek 5 Sat Home D 0 0 Southampton ... Match Report NaN 16.0 1.0 16.4 1.0 0.0 0.0 2022 Manchester City 5 rows × 27 columns In [5]: match_df . tail () Out[5]: date time comp round day venue result gf ga opponent ... match report notes sh sot dist fk pk pkatt season team 36 2018-04-14 15:00 Premier League Matchweek 34 Sat Away L 0 1 Huddersfield ... Match Report NaN 4.0 2.0 18.9 NaN 0.0 0.0 2018 Norwich City 37 2018-04-21 15:00 Premier League Matchweek 35 Sat Home D 0 0 Crystal Palace ... Match Report NaN 14.0 4.0 17.5 NaN 0.0 0.0 2018 Norwich City 38 2018-04-30 20:00 Premier League Matchweek 36 Mon Away L 0 2 Tottenham ... Match Report NaN 13.0 5.0 15.2 NaN 0.0 0.0 2018 Norwich City 39 2018-05-05 15:00 Premier League Matchweek 37 Sat Home W 2 1 Newcastle Utd ... Match Report NaN 10.0 7.0 15.7 NaN 0.0 1.0 2018 Norwich City 40 2018-05-13 15:00 Premier League Matchweek 38 Sun Away L 0 1 Manchester Utd ... Match Report NaN 7.0 3.0 16.3 NaN 0.0 0.0 2018 Norwich City 5 rows × 27 columns In [6]: match_df . shape Out[6]: (3458, 27) As we can notice, the index labels are not correct. Currently, it represents the match number of each season. We are not interested in that. So, we reset the index and drop the exiting column 0 which was infered as index column while reading data. In [7]: match_df . reset_index ( drop = True , inplace = True ) In [8]: match_df . head () Out[8]: date time comp round day venue result gf ga opponent ... match report notes sh sot dist fk pk pkatt season team 0 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham ... Match Report NaN 18.0 4.0 17.3 1.0 0.0 0.0 2022 Manchester City 1 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City ... Match Report NaN 16.0 4.0 18.5 1.0 0.0 0.0 2022 Manchester City 2 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal ... Match Report NaN 25.0 10.0 14.8 0.0 0.0 0.0 2022 Manchester City 3 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City ... Match Report NaN 25.0 8.0 14.3 0.0 0.0 0.0 2022 Manchester City 4 2021-09-18 15:00 Premier League Matchweek 5 Sat Home D 0 0 Southampton ... Match Report NaN 16.0 1.0 16.4 1.0 0.0 0.0 2022 Manchester City 5 rows × 27 columns In [9]: match_df . tail () Out[9]: date time comp round day venue result gf ga opponent ... match report notes sh sot dist fk pk pkatt season team 3453 2018-04-14 15:00 Premier League Matchweek 34 Sat Away L 0 1 Huddersfield ... Match Report NaN 4.0 2.0 18.9 NaN 0.0 0.0 2018 Norwich City 3454 2018-04-21 15:00 Premier League Matchweek 35 Sat Home D 0 0 Crystal Palace ... Match Report NaN 14.0 4.0 17.5 NaN 0.0 0.0 2018 Norwich City 3455 2018-04-30 20:00 Premier League Matchweek 36 Mon Away L 0 2 Tottenham ... Match Report NaN 13.0 5.0 15.2 NaN 0.0 0.0 2018 Norwich City 3456 2018-05-05 15:00 Premier League Matchweek 37 Sat Home W 2 1 Newcastle Utd ... Match Report NaN 10.0 7.0 15.7 NaN 0.0 1.0 2018 Norwich City 3457 2018-05-13 15:00 Premier League Matchweek 38 Sun Away L 0 1 Manchester Utd ... Match Report NaN 7.0 3.0 16.3 NaN 0.0 0.0 2018 Norwich City 5 rows × 27 columns As our goal is to make prediction for winners, we enough data for each time. It is not the case that the same 20 teams contest with each other in every season. It operates on a system of promotion and relegation with the English Football League (EFL). However, in this project, we only want to make predictions for those teams, which regularly participate in each season. For these teams, we will have enough data to make accurate prediction In [10]: match_df . groupby ([ \"team\" , \"season\" ])[ \"date\" ] . count () . unstack () Out[10]: season 2018 2019 2020 2021 2022 team Arsenal 38.0 38.0 38.0 38.0 38.0 Aston Villa 38.0 NaN 38.0 38.0 38.0 Brentford 38.0 NaN NaN NaN 38.0 Brighton 38.0 38.0 38.0 38.0 38.0 Burnley 38.0 38.0 38.0 38.0 38.0 Chelsea 38.0 38.0 38.0 38.0 38.0 Crystal Palace 38.0 38.0 38.0 38.0 38.0 Everton 38.0 38.0 38.0 38.0 38.0 Leeds United 38.0 NaN NaN 38.0 38.0 Leicester City 38.0 38.0 38.0 38.0 38.0 Liverpool 38.0 38.0 38.0 38.0 38.0 Manchester City 38.0 38.0 38.0 38.0 38.0 Manchester Utd 38.0 38.0 38.0 38.0 38.0 Newcastle Utd 38.0 38.0 38.0 38.0 38.0 Norwich City 38.0 NaN 38.0 NaN 38.0 Southampton 38.0 38.0 38.0 38.0 38.0 Tottenham 38.0 38.0 38.0 38.0 38.0 Watford 38.0 38.0 38.0 NaN 38.0 West Ham 38.0 38.0 38.0 38.0 38.0 Wolves 38.0 38.0 38.0 38.0 38.0 As we can observe, there are 5 teams in these dataset, who didn't play each season. We can delete the data for these teams from our dataset. In [11]: all_season_teams = match_df . groupby ([ \"team\" , \"season\" ])[ \"date\" ] . count () . unstack () . dropna () . index . to_list () all_season_teams . remove ( 'Wolves' ) In [12]: match_df = match_df [ match_df [ \"team\" ] . isin ( all_season_teams ) & match_df [ \"opponent\" ] . isin ( all_season_teams )] In [13]: match_df . info () <class 'pandas.core.frame.DataFrame'> Int64Index: 1820 entries, 0 to 3380 Data columns (total 27 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 date 1820 non-null object 1 time 1820 non-null object 2 comp 1820 non-null object 3 round 1820 non-null object 4 day 1820 non-null object 5 venue 1820 non-null object 6 result 1820 non-null object 7 gf 1820 non-null int64 8 ga 1820 non-null int64 9 opponent 1820 non-null object 10 xg 1820 non-null float64 11 xga 1820 non-null float64 12 poss 1820 non-null float64 13 attendance 1406 non-null float64 14 captain 1820 non-null object 15 formation 1820 non-null object 16 referee 1820 non-null object 17 match report 1820 non-null object 18 notes 0 non-null float64 19 sh 1820 non-null float64 20 sot 1820 non-null float64 21 dist 1819 non-null float64 22 fk 1742 non-null float64 23 pk 1820 non-null float64 24 pkatt 1820 non-null float64 25 season 1820 non-null int64 26 team 1820 non-null object dtypes: float64(11), int64(3), object(13) memory usage: 398.1+ KB As we can observe, there are some missing values for four columns: attendance, notes, dist, fk. The column 'notes' has no values in it. We can safely drop this column. For other columns, we need to take a more deep look to handle the missing data. Moreover, the datatype of 'date' column needs to be converted to a datetime object. The format of values in the 'time' column needs to be corrected. In [14]: match_df . drop ( 'notes' , axis = 1 , inplace = True ) /usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy return super().drop( In [15]: match_df [ 'hour' ] = match_df [ 'time' ] . str . replace ( r ':.+' , \"\" , regex = True ) . astype ( int ) match_df . drop ( 'time' , axis = 1 , inplace = True ) <ipython-input-15-88fa29fedc93>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy match_df['hour'] = match_df['time'].str.replace(r':.+', \"\", regex=True).astype(int) In [16]: match_df [ 'date' ] = pd . to_datetime ( match_df [ 'date' ]) <ipython-input-16-1ebb7af4446e>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy match_df['date'] = pd.to_datetime(match_df['date']) Now let's analyze the missing data. As we observed during the exploratory data analysis, there are some missing values for three columns: attendance, dist, fk. As only one data is missing from the 'dist' column, it is probably missing completely at random. As the 'dist' column had some outliers, we can replace the missing value with the median value of the 'dist' column. We will do that later in the feature engineering stage. Now let's focus on the other two columns. A lot of data are missing. This can't happen at random. Let's find the source of the missing data In [17]: match_df [ \"fk_Null\" ] = np . where ( match_df [ \"fk\" ] . isnull (), 1 , 0 ) match_df [ \"attendance_Null\" ] = np . where ( match_df [ \"attendance\" ] . isnull (), 1 , 0 ) <ipython-input-17-f8e5e056f8b3>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy match_df[\"fk_Null\"] = np.where(match_df[\"fk\"].isnull(), 1, 0) <ipython-input-17-f8e5e056f8b3>:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy match_df[\"attendance_Null\"] = np.where(match_df[\"attendance\"].isnull(), 1, 0) In [18]: match_df . groupby ([ \"season\" ])[[ \"fk_Null\" , \"attendance_Null\" ]] . sum () Out[18]: fk_Null attendance_Null season 2018 78 0 2019 0 0 2020 0 82 2021 0 332 2022 0 0 As we can notice, all the missing data in the column 'fk' comes from the match records for season 2018. A similar pattern is observed for the 'attendance' column. There may be some problem with the data collection. For this reason, we again web-scrap the match data for 2018, 2020, and 2021. In [19]: match_df . drop ([ \"fk_Null\" , \"attendance_Null\" ], axis = 1 , inplace = True ) In [20]: season18 = pd . read_csv ( \"matches18.csv\" , index_col = 0 ) season18 . head () Out[20]: date time comp round day venue result gf ga opponent ... match report notes sh sot dist fk pk pkatt season team 0 2017-08-12 17:30 Premier League Matchweek 1 Sat Away W 2 0 Brighton ... Match Report NaN 14.0 4.0 19.5 2.0 0 0 2022 Manchester City 1 2017-08-21 20:00 Premier League Matchweek 2 Mon Home D 1 1 Everton ... Match Report NaN 19.0 6.0 20.0 1.0 0 0 2022 Manchester City 2 2017-08-26 12:30 Premier League Matchweek 3 Sat Away W 2 1 Bournemouth ... Match Report NaN 19.0 8.0 16.2 1.0 0 0 2022 Manchester City 3 2017-09-09 12:30 Premier League Matchweek 4 Sat Home W 5 0 Liverpool ... Match Report NaN 13.0 10.0 14.0 0.0 0 0 2022 Manchester City 5 2017-09-16 15:00 Premier League Matchweek 5 Sat Away W 6 0 Watford ... Match Report NaN 27.0 9.0 17.2 0.0 1 1 2022 Manchester City 5 rows × 27 columns In [21]: season18 [ \"fk\" ] . isnull () . sum () Out[21]: 0 As we can observe, this dataset has no missing value in the 'fk' column. So, we can replace the missing values of 'fk' column in 'match_df' using the values from this dataset In [22]: season18 = season18 [ season18 [ \"team\" ] . isin ( all_season_teams ) & season18 [ \"opponent\" ] . isin ( all_season_teams )] match_df . loc [ match_df [ 'season' ] == 2018 , 'fk' ] = season18 [ 'fk' ] . to_list () match_df [ 'fk' ] . isnull () . sum () Out[22]: 0 Now let's focus on the 'attendance' column. It is noticeable that the data is missing for season 2020 and 2021. This missing values can directly be attributed to the COVID-19 restrictions imposed by the UK government. Therefore, no actual data exists for correcting these missing values. Thus we need to impute the missing values for this column later in the feature engineering stage Now let's observe the statistics of category variables In [23]: match_df . describe ( include = 'O' ) Out[23]: comp round day venue result opponent captain formation referee match report team count 1820 1820 1820 1820 1820 1820 1820 1820 1820 1820 1820 unique 1 38 7 2 3 14 106 19 29 1 14 top Premier League Matchweek 10 Sat Away L Tottenham Hugo Lloris 4-2-3-1 Anthony Taylor Match Report Manchester City freq 1820 52 782 910 693 130 115 427 166 1820 130 As we can observe, the column 'comp' and 'match report' has only one unique category. So, these columns won't add any valuable information in the prediction process. Moreover, the features 'referee' and 'captain' are not generalizable. It is highly liklely that a new referee or captain can be found in the test dataset, which may hurt the performance of machine learning algorithm. We can safely drop these columns. Other categorical columns need to be encoded properly in the feature engineering stage. In [24]: match_df . drop ([ 'comp' , 'match report' , 'referee' , 'captain' ], axis = 1 , inplace = True ) In [25]: match_df . head () Out[25]: date round day venue result gf ga opponent xg xga ... formation sh sot dist fk pk pkatt season team hour 0 2021-08-15 Matchweek 1 Sun Away L 0 1 Tottenham 2.0 1.0 ... 4-3-3 18.0 4.0 17.3 1.0 0.0 0.0 2022 Manchester City 16 2 2021-08-28 Matchweek 3 Sat Home W 5 0 Arsenal 4.0 0.2 ... 4-3-3 25.0 10.0 14.8 0.0 0.0 0.0 2022 Manchester City 12 3 2021-09-11 Matchweek 4 Sat Away W 1 0 Leicester City 3.3 0.6 ... 4-3-3 25.0 8.0 14.3 0.0 0.0 0.0 2022 Manchester City 15 4 2021-09-18 Matchweek 5 Sat Home D 0 0 Southampton 1.2 0.5 ... 4-3-3 16.0 1.0 16.4 1.0 0.0 0.0 2022 Manchester City 15 5 2021-09-25 Matchweek 6 Sat Away W 1 0 Chelsea 1.4 0.2 ... 4-3-3 15.0 3.0 17.1 0.0 0.0 0.0 2022 Manchester City 12 5 rows × 22 columns It's time to perform exploratory data analysis on numerical variables In [26]: match_df . describe ( exclude = [ 'O' , 'datetime64' ]) Out[26]: gf ga xg xga poss attendance sh sot dist fk pk pkatt season hour count 1820.000000 1820.000000 1820.000000 1820.000000 1820.000000 1406.000000 1820.000000 1820.000000 1819.000000 1820.000000 1820.000000 1820.000000 1820.000000 1820.000000 mean 1.414835 1.414835 1.350989 1.350989 50.001648 43946.085349 12.280220 4.110989 17.683837 0.456044 0.106593 0.136813 2020.000000 16.169231 std 1.266518 1.266518 0.806021 0.806021 13.375718 16739.265305 5.564213 2.442025 3.146458 0.670383 0.327686 0.365449 1.414602 2.534412 min 0.000000 0.000000 0.000000 0.000000 18.000000 2000.000000 0.000000 0.000000 5.300000 0.000000 0.000000 0.000000 2018.000000 12.000000 25% 0.000000 0.000000 0.800000 0.800000 39.000000 30665.250000 8.000000 2.000000 15.600000 0.000000 0.000000 0.000000 2019.000000 15.000000 50% 1.000000 1.000000 1.200000 1.200000 50.000000 41219.000000 12.000000 4.000000 17.600000 0.000000 0.000000 0.000000 2020.000000 16.000000 75% 2.000000 2.000000 1.800000 1.800000 61.000000 56903.500000 16.000000 6.000000 19.500000 1.000000 0.000000 0.000000 2021.000000 19.000000 max 9.000000 9.000000 4.700000 4.700000 82.000000 83222.000000 36.000000 14.000000 35.000000 4.000000 3.000000 3.000000 2022.000000 20.000000 In [27]: numerical_columns = match_df . select_dtypes ( include = np . number ) . columns . tolist () numerical_columns Out[27]: ['gf', 'ga', 'xg', 'xga', 'poss', 'attendance', 'sh', 'sot', 'dist', 'fk', 'pk', 'pkatt', 'season', 'hour'] In [28]: nrow = 7 ncol = 2 index = 0 fig , axs = plt . subplots ( nrow , ncol , figsize = ( 10 , 25 )) for i in range ( nrow ): for j in range ( ncol ): label = numerical_columns [ index ] sns . boxplot ( match_df [ label ], ax = axs [ i ][ j ]) . set_title ( label ) index += 1 fig . tight_layout () plt . show () /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( In [29]: nrow = 7 ncol = 2 index = 0 fig , axs = plt . subplots ( nrow , ncol , figsize = ( 10 , 25 )) for i in range ( nrow ): for j in range ( ncol ): label = numerical_columns [ index ] sns . histplot ( match_df [ label ], ax = axs [ i ][ j ]) . set_title ( label ) index += 1 fig . tight_layout () plt . show () From the boxplots and histograms, it is clear that outliers are present in the dataset. But as these data are collected from actual matches, and there was no data collection error, we cannot just remove the outliers. Instead, we need to use a classification algorithm that is not sensitive to outliers. In [30]: match_df . columns Out[30]: Index(['date', 'round', 'day', 'venue', 'result', 'gf', 'ga', 'opponent', 'xg', 'xga', 'poss', 'attendance', 'formation', 'sh', 'sot', 'dist', 'fk', 'pk', 'pkatt', 'season', 'team', 'hour'], dtype='object') Before moving to the feature engineering step, let's split the dataset into training and test data. In [31]: match_df . groupby ([ \"season\" ])[ 'date' ] . count () Out[31]: season 2018 364 2019 364 2020 364 2021 364 2022 364 Name: date, dtype: int64 We reserve the match records for season 2022 as the test data. All other seasons are included in the training dataset In [32]: train_data = match_df [ match_df [ \"season\" ] != 2022 ] test_data = match_df [ match_df [ \"season\" ] == 2022 ] In [33]: train_data . reset_index ( drop = True , inplace = True ) test_data . reset_index ( drop = True , inplace = True ) print ( train_data . shape ) print ( test_data . shape ) (1456, 22) (364, 22) In [34]: train_data . head () Out[34]: date round day venue result gf ga opponent xg xga ... formation sh sot dist fk pk pkatt season team hour 0 2020-09-27 Matchweek 3 Sun Home L 2 5 Leicester City 0.9 2.9 ... 4-2-3-1 16.0 5.0 19.8 1.0 0.0 0.0 2021 Manchester City 16 1 2020-10-17 Matchweek 5 Sat Home W 1 0 Arsenal 1.3 0.9 ... 3-1-4-2 13.0 5.0 17.7 0.0 0.0 0.0 2021 Manchester City 17 2 2020-10-24 Matchweek 6 Sat Away D 1 1 West Ham 1.0 0.3 ... 4-3-3 14.0 7.0 20.9 1.0 0.0 0.0 2021 Manchester City 12 3 2020-11-08 Matchweek 8 Sun Home D 1 1 Liverpool 1.4 1.2 ... 4-2-3-1 6.0 2.0 20.6 0.0 0.0 1.0 2021 Manchester City 16 4 2020-11-21 Matchweek 9 Sat Away L 0 2 Tottenham 1.4 0.7 ... 4-3-3 22.0 5.0 16.0 0.0 0.0 0.0 2021 Manchester City 17 5 rows × 22 columns Feature Engineering Now let's focus on the 'attendance' column. It is noticeable that the data is missing for season 2020 and 2021. This missing values can directly be attributed to the COVID-19 restrictions imposed by the UK government. Therefore, no actual data exists for correcting these missing values. However, we can estimate the match attendance for these missing values based on prior data. For that purpose, we try to build a regression model. Before that, we need encode the categorical variables. In [35]: categorical_columns = train_data . select_dtypes ( include = \"O\" ) . columns . tolist () categorical_columns Out[35]: ['round', 'day', 'venue', 'result', 'opponent', 'formation', 'team'] In [36]: train_data [ \"round\" ] . unique () Out[36]: array(['Matchweek 3', 'Matchweek 5', 'Matchweek 6', 'Matchweek 8', 'Matchweek 9', 'Matchweek 10', 'Matchweek 12', 'Matchweek 14', 'Matchweek 15', 'Matchweek 17', 'Matchweek 18', 'Matchweek 19', 'Matchweek 22', 'Matchweek 23', 'Matchweek 24', 'Matchweek 16', 'Matchweek 25', 'Matchweek 26', 'Matchweek 27', 'Matchweek 33', 'Matchweek 30', 'Matchweek 34', 'Matchweek 35', 'Matchweek 36', 'Matchweek 37', 'Matchweek 38', 'Matchweek 2', 'Matchweek 7', 'Matchweek 13', 'Matchweek 20', 'Matchweek 21', 'Matchweek 29', 'Matchweek 1', 'Matchweek 4', 'Matchweek 31', 'Matchweek 32', 'Matchweek 11', 'Matchweek 28'], dtype=object) As we can observe, the matchweek number is the only valuable information in this column. Thus, we can created a new column named \"MatchWeek\" and fill that column with the matchweek number corresponding to each match. After that, we can drop this column. We need to do the same for X_test In [37]: train_data [ \"match_week\" ] = train_data [ \"round\" ] . str . split () . str [ 1 ] . astype ( int ) <ipython-input-37-343085114d01>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_data[\"match_week\"] = train_data[\"round\"].str.split().str[1].astype(int) In [38]: train_data . head () Out[38]: date round day venue result gf ga opponent xg xga ... sh sot dist fk pk pkatt season team hour match_week 0 2020-09-27 Matchweek 3 Sun Home L 2 5 Leicester City 0.9 2.9 ... 16.0 5.0 19.8 1.0 0.0 0.0 2021 Manchester City 16 3 1 2020-10-17 Matchweek 5 Sat Home W 1 0 Arsenal 1.3 0.9 ... 13.0 5.0 17.7 0.0 0.0 0.0 2021 Manchester City 17 5 2 2020-10-24 Matchweek 6 Sat Away D 1 1 West Ham 1.0 0.3 ... 14.0 7.0 20.9 1.0 0.0 0.0 2021 Manchester City 12 6 3 2020-11-08 Matchweek 8 Sun Home D 1 1 Liverpool 1.4 1.2 ... 6.0 2.0 20.6 0.0 0.0 1.0 2021 Manchester City 16 8 4 2020-11-21 Matchweek 9 Sat Away L 0 2 Tottenham 1.4 0.7 ... 22.0 5.0 16.0 0.0 0.0 0.0 2021 Manchester City 17 9 5 rows × 23 columns In [39]: train_data [ \"day\" ] . unique () Out[39]: array(['Sun', 'Sat', 'Wed', 'Fri', 'Tue', 'Mon', 'Thu'], dtype=object) This column tells us about the day of each match. We can assign each day of the week an unique number In [40]: train_data [ \"dayofweek\" ] = train_data [ \"date\" ] . dt . dayofweek <ipython-input-40-1b7b79f95047>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_data[\"dayofweek\"] = train_data[\"date\"].dt.dayofweek In [41]: train_data . head () Out[41]: date round day venue result gf ga opponent xg xga ... sot dist fk pk pkatt season team hour match_week dayofweek 0 2020-09-27 Matchweek 3 Sun Home L 2 5 Leicester City 0.9 2.9 ... 5.0 19.8 1.0 0.0 0.0 2021 Manchester City 16 3 6 1 2020-10-17 Matchweek 5 Sat Home W 1 0 Arsenal 1.3 0.9 ... 5.0 17.7 0.0 0.0 0.0 2021 Manchester City 17 5 5 2 2020-10-24 Matchweek 6 Sat Away D 1 1 West Ham 1.0 0.3 ... 7.0 20.9 1.0 0.0 0.0 2021 Manchester City 12 6 5 3 2020-11-08 Matchweek 8 Sun Home D 1 1 Liverpool 1.4 1.2 ... 2.0 20.6 0.0 0.0 1.0 2021 Manchester City 16 8 6 4 2020-11-21 Matchweek 9 Sat Away L 0 2 Tottenham 1.4 0.7 ... 5.0 16.0 0.0 0.0 0.0 2021 Manchester City 17 9 5 5 rows × 24 columns However, it is still not the right way to encode days of the week if we want to use the data to train machine learning models! In reality, Saturday is closer to Monday than Wednesday. Encoding days of the week as numbers changes the sense of data. We don't want to lose the information about the circular nature of weeks and the actual distance between the days. Therefore, we can encode the day of week feature as \"points\" on a circle: 0° = Monday, 51.5° = Tuesday, etc. There is one problem. We know that it is a circle, but for a machine learning model, the difference between Sunday and Monday is 308.5° instead of 51.5°. That is wrong. To solve the problem we have to calculate the cosinus and sinus values of the degree. We need both because both functions produce duplicate outputs for difference inputs, but when we use them together we get unique pairs of values: In [42]: train_data [ 'day_sin' ] = np . sin ( train_data [ 'dayofweek' ] * ( 2 * np . pi / 7 )) train_data [ 'day_cos' ] = np . cos ( train_data [ 'dayofweek' ] * ( 2 * np . pi / 7 )) <ipython-input-42-c0df836840e7>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_data['day_sin'] = np.sin(train_data['dayofweek'] * (2 * np.pi / 7)) <ipython-input-42-c0df836840e7>:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_data['day_cos'] = np.cos(train_data['dayofweek'] * (2 * np.pi / 7)) In [43]: train_data . drop ( \"dayofweek\" , axis = 1 , inplace = True ) /usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy return super().drop( In [44]: train_data [ \"venue\" ] . unique () Out[44]: array(['Home', 'Away'], dtype=object) As there are only two unique values and there is no sense of order in this column, we can use one-hot encoding to encode the venue type In [45]: train_data = pd . get_dummies ( train_data , columns = [ \"venue\" ], drop_first = True ) In [46]: train_data . head () Out[46]: date round day result gf ga opponent xg xga poss ... fk pk pkatt season team hour match_week day_sin day_cos venue_Home 0 2020-09-27 Matchweek 3 Sun L 2 5 Leicester City 0.9 2.9 72.0 ... 1.0 0.0 0.0 2021 Manchester City 16 3 -0.781831 0.623490 1 1 2020-10-17 Matchweek 5 Sat W 1 0 Arsenal 1.3 0.9 58.0 ... 0.0 0.0 0.0 2021 Manchester City 17 5 -0.974928 -0.222521 1 2 2020-10-24 Matchweek 6 Sat D 1 1 West Ham 1.0 0.3 69.0 ... 1.0 0.0 0.0 2021 Manchester City 12 6 -0.974928 -0.222521 0 3 2020-11-08 Matchweek 8 Sun D 1 1 Liverpool 1.4 1.2 54.0 ... 0.0 0.0 1.0 2021 Manchester City 16 8 -0.781831 0.623490 1 4 2020-11-21 Matchweek 9 Sat L 0 2 Tottenham 1.4 0.7 66.0 ... 0.0 0.0 0.0 2021 Manchester City 17 9 -0.974928 -0.222521 0 5 rows × 25 columns In [47]: ! pip install category_encoders Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting category_encoders Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB) |████████████████████████████████| 72 kB 814 kB/s Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.5.3) Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.3.5) Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.0.2) Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.12.2) Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.7.3) Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.21.6) Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2) Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2022.6) Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0) Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0) Installing collected packages: category-encoders Successfully installed category-encoders-2.5.1.post0 In [48]: from category_encoders.hashing import HashingEncoder In [49]: train_data [ \"opponent\" ] . unique () Out[49]: array(['Leicester City', 'Arsenal', 'West Ham', 'Liverpool', 'Tottenham', 'Burnley', 'Manchester Utd', 'Southampton', 'Newcastle Utd', 'Chelsea', 'Brighton', 'Crystal Palace', 'Everton', 'Manchester City'], dtype=object) In [50]: train_data [ \"team\" ] . unique () Out[50]: array(['Manchester City', 'Liverpool', 'Chelsea', 'Tottenham', 'Arsenal', 'Manchester Utd', 'West Ham', 'Leicester City', 'Brighton', 'Newcastle Utd', 'Crystal Palace', 'Southampton', 'Everton', 'Burnley'], dtype=object) In [51]: def hash_encode ( col , hash_size = 8 ): ec = HashingEncoder ([ col ], n_components = hash_size ) ec . fit ( train_data ) return ec In [52]: transformed_opponent = hash_encode ( \"opponent\" , 5 ) . transform ( train_data ) transformed_team = hash_encode ( \"team\" , 5 ) . transform ( train_data ) In [53]: name_map = lambda x : { \"col_0\" : x + \"_0\" , \"col_1\" : x + \"_1\" , \"col_2\" : x + \"_2\" , \"col_3\" : x + \"_3\" , \"col_4\" : x + \"_4\" } In [54]: opponent_map = name_map ( \"opponent\" ) team_map = name_map ( \"team\" ) In [55]: transformed_opponent . rename ( opponent_map , axis = 1 , inplace = True ) transformed_team . rename ( team_map , axis = 1 , inplace = True ) In [56]: train_data = pd . concat ([ train_data , transformed_opponent [[ 'opponent_0' , 'opponent_1' , 'opponent_2' , 'opponent_3' , 'opponent_4' ]]], axis = 1 ) train_data = pd . concat ([ train_data , transformed_team [[ 'team_0' , 'team_1' , 'team_2' , 'team_3' , 'team_4' ]]], axis = 1 ) In [57]: train_data . columns Out[57]: Index(['date', 'round', 'day', 'result', 'gf', 'ga', 'opponent', 'xg', 'xga', 'poss', 'attendance', 'formation', 'sh', 'sot', 'dist', 'fk', 'pk', 'pkatt', 'season', 'team', 'hour', 'match_week', 'day_sin', 'day_cos', 'venue_Home', 'opponent_0', 'opponent_1', 'opponent_2', 'opponent_3', 'opponent_4', 'team_0', 'team_1', 'team_2', 'team_3', 'team_4'], dtype='object') In [58]: train_data . head () Out[58]: date round day result gf ga opponent xg xga poss ... opponent_0 opponent_1 opponent_2 opponent_3 opponent_4 team_0 team_1 team_2 team_3 team_4 0 2020-09-27 Matchweek 3 Sun L 2 5 Leicester City 0.9 2.9 72.0 ... 3 1 1 0 1 3 1 1 0 1 1 2020-10-17 Matchweek 5 Sat W 1 0 Arsenal 1.3 0.9 58.0 ... 1 1 2 0 2 1 1 2 0 2 2 2020-10-24 Matchweek 6 Sat D 1 1 West Ham 1.0 0.3 69.0 ... 2 2 1 1 0 2 2 1 1 0 3 2020-11-08 Matchweek 8 Sun D 1 1 Liverpool 1.4 1.2 54.0 ... 4 1 1 0 0 4 1 1 0 0 4 2020-11-21 Matchweek 9 Sat L 0 2 Tottenham 1.4 0.7 66.0 ... 2 0 1 1 2 2 0 1 1 2 5 rows × 35 columns In [59]: train_data [ \"formation\" ] . unique () Out[59]: array(['4-2-3-1', '3-1-4-2', '4-3-3', '3-4-3◆', '4-4-1-1', '4-3-1-2', '4-4-2', '4-2-2-2', '3-4-3', '3-4-1-2', '4-1-2-1-2◆', '5-4-1', '4-1-4-1', '3-5-2', '3-5-1-1', '5-3-2', '4-3-2-1', '4-5-1', '4-1-3-2'], dtype=object) We can observe there are some non-numeric characters. We need to remove this characters. Moreover, we can extract the number of defenders, midfielders, and strikers from this column. In [60]: train_data [ \"formation\" ] = train_data [ \"formation\" ] . str . replace ( \"◆\" , \"\" ) In [61]: train_data [ \"num_defender\" ] = train_data [ \"formation\" ] . str . split ( pat = \"-\" ) . str [ 0 ] . astype ( int ) In [62]: train_data [ \"num_striker\" ] = train_data [ \"formation\" ] . str . split ( pat = \"-\" ) . str [ - 1 ] . astype ( int ) In [63]: offensive_midfield_mapper = lambda x : x [ 4 ] if len ( x ) == 7 else 0 center_midfield_mapper = lambda x : x [ 2 ] if len ( x ) == 5 else 0 In [64]: train_data [ \"offensive_midfielder\" ] = train_data [ \"formation\" ] . apply ( offensive_midfield_mapper ) . astype ( int ) In [65]: train_data [ \"center_midfielder\" ] = train_data [ \"formation\" ] . apply ( center_midfield_mapper ) . astype ( int ) In [66]: train_data . head () Out[66]: date round day result gf ga opponent xg xga poss ... opponent_4 team_0 team_1 team_2 team_3 team_4 num_defender num_striker offensive_midfielder center_midfielder 0 2020-09-27 Matchweek 3 Sun L 2 5 Leicester City 0.9 2.9 72.0 ... 1 3 1 1 0 1 4 1 3 0 1 2020-10-17 Matchweek 5 Sat W 1 0 Arsenal 1.3 0.9 58.0 ... 2 1 1 2 0 2 3 2 4 0 2 2020-10-24 Matchweek 6 Sat D 1 1 West Ham 1.0 0.3 69.0 ... 0 2 2 1 1 0 4 3 0 3 3 2020-11-08 Matchweek 8 Sun D 1 1 Liverpool 1.4 1.2 54.0 ... 0 4 1 1 0 0 4 1 3 0 4 2020-11-21 Matchweek 9 Sat L 0 2 Tottenham 1.4 0.7 66.0 ... 2 2 0 1 1 2 4 3 0 3 5 rows × 39 columns In [67]: train_data [ \"result\" ] . unique () Out[67]: array(['L', 'W', 'D'], dtype=object) As the results are ordinal data (L < D < W), we can use ordinal encoding for the target variable In [68]: result_map = { \"L\" : 0 , \"D\" : 1 , \"W\" : 2 } train_data [ \"target\" ] = train_data [ \"result\" ] . map ( result_map ) Finally, we are done with feature encoding. As we found during the exploratory data analysis, there are some missing values left in two columns: attendance, and dist. Let's deal with these missing values one by one. In [69]: train_data . isnull () . sum () Out[69]: date 0 round 0 day 0 result 0 gf 0 ga 0 opponent 0 xg 0 xga 0 poss 0 attendance 414 formation 0 sh 0 sot 0 dist 1 fk 0 pk 0 pkatt 0 season 0 team 0 hour 0 match_week 0 day_sin 0 day_cos 0 venue_Home 0 opponent_0 0 opponent_1 0 opponent_2 0 opponent_3 0 opponent_4 0 team_0 0 team_1 0 team_2 0 team_3 0 team_4 0 num_defender 0 num_striker 0 offensive_midfielder 0 center_midfielder 0 target 0 dtype: int64 As only one data is missing from the 'dist' column, it is probably missing completely at random. As the 'dist' column had some outliers, we can replace the missing value with the median value of the 'dist' column In [70]: median_dist = train_data [ \"dist\" ] . median () In [71]: median_dist Out[71]: 17.7 In [72]: train_data [ \"dist\" ] . fillna ( median_dist , inplace = True ) In [73]: train_data [ \"dist\" ] . isnull () . sum () Out[73]: 0 Now let's focus on the 'attendance' column. The match attendance depends on a number of other factors like day, team, opponent, venue_type, etc. So, we can use MCIE method to impute the missing values for this column. In [74]: from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.linear_model import HuberRegressor In [75]: numerical_columns = train_data . select_dtypes ( include = np . number ) . columns . tolist () numerical_columns Out[75]: ['gf', 'ga', 'xg', 'xga', 'poss', 'attendance', 'sh', 'sot', 'dist', 'fk', 'pk', 'pkatt', 'season', 'hour', 'match_week', 'day_sin', 'day_cos', 'venue_Home', 'opponent_0', 'opponent_1', 'opponent_2', 'opponent_3', 'opponent_4', 'team_0', 'team_1', 'team_2', 'team_3', 'team_4', 'num_defender', 'num_striker', 'offensive_midfielder', 'center_midfielder', 'target'] In [76]: estimator = HuberRegressor ( max_iter = 15000 ) imputer = IterativeImputer ( estimator = estimator , random_state = 2022 , skip_complete = True ) imputed_train_data = imputer . fit_transform ( train_data [ numerical_columns ]) In [77]: train_data [ 'attendance' ] = imputed_train_data [:, 5 ] train_data . head () Out[77]: date round day result gf ga opponent xg xga poss ... team_0 team_1 team_2 team_3 team_4 num_defender num_striker offensive_midfielder center_midfielder target 0 2020-09-27 Matchweek 3 Sun L 2 5 Leicester City 0.9 2.9 72.0 ... 3 1 1 0 1 4 1 3 0 0 1 2020-10-17 Matchweek 5 Sat W 1 0 Arsenal 1.3 0.9 58.0 ... 1 1 2 0 2 3 2 4 0 2 2 2020-10-24 Matchweek 6 Sat D 1 1 West Ham 1.0 0.3 69.0 ... 2 2 1 1 0 4 3 0 3 1 3 2020-11-08 Matchweek 8 Sun D 1 1 Liverpool 1.4 1.2 54.0 ... 4 1 1 0 0 4 1 3 0 1 4 2020-11-21 Matchweek 9 Sat L 0 2 Tottenham 1.4 0.7 66.0 ... 2 0 1 1 2 4 3 0 3 0 5 rows × 40 columns In [78]: train_data [ \"attendance\" ] . plot . hist ( bins = 30 ) Out[78]: <matplotlib.axes._subplots.AxesSubplot at 0x7f351d90fbe0> In [79]: train_data . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1456 entries, 0 to 1455 Data columns (total 40 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 date 1456 non-null datetime64[ns] 1 round 1456 non-null object 2 day 1456 non-null object 3 result 1456 non-null object 4 gf 1456 non-null int64 5 ga 1456 non-null int64 6 opponent 1456 non-null object 7 xg 1456 non-null float64 8 xga 1456 non-null float64 9 poss 1456 non-null float64 10 attendance 1456 non-null float64 11 formation 1456 non-null object 12 sh 1456 non-null float64 13 sot 1456 non-null float64 14 dist 1456 non-null float64 15 fk 1456 non-null float64 16 pk 1456 non-null float64 17 pkatt 1456 non-null float64 18 season 1456 non-null int64 19 team 1456 non-null object 20 hour 1456 non-null int64 21 match_week 1456 non-null int64 22 day_sin 1456 non-null float64 23 day_cos 1456 non-null float64 24 venue_Home 1456 non-null uint8 25 opponent_0 1456 non-null int64 26 opponent_1 1456 non-null int64 27 opponent_2 1456 non-null int64 28 opponent_3 1456 non-null int64 29 opponent_4 1456 non-null int64 30 team_0 1456 non-null int64 31 team_1 1456 non-null int64 32 team_2 1456 non-null int64 33 team_3 1456 non-null int64 34 team_4 1456 non-null int64 35 num_defender 1456 non-null int64 36 num_striker 1456 non-null int64 37 offensive_midfielder 1456 non-null int64 38 center_midfielder 1456 non-null int64 39 target 1456 non-null int64 dtypes: datetime64[ns](1), float64(12), int64(20), object(6), uint8(1) memory usage: 445.2+ KB As we can notice, there are no missing values in the dataset. However, the dataset doesn't have any column capturing the past performance of a teamHowever, we haven't yet considered the historical performance of a team. It may have a significant influence on the winner prediction. We use the 'rolling average' method to capture the past performance of each time. For that purpose, we temporarily merge the training and test dataset. In [80]: def rolling_averages ( group , cols , new_cols ): group = group . sort_values ( 'date' ) rolling_stats = group [ cols ] . rolling ( 3 , closed = 'left' ) . mean () group [ new_cols ] = rolling_stats group = group . dropna ( subset = new_cols ) return group In [81]: cols = [ \"gf\" , 'ga' , 'sh' , 'sot' , 'dist' , 'poss' , 'fk' , 'pk' , 'pkatt' ] new_cols = [ f \" { c } _rolling\" for c in cols ] train_data = train_data . groupby ( \"team\" ) . apply ( lambda x : rolling_averages ( x , cols , new_cols )) In [82]: train_data . index = train_data . index . droplevel () In [83]: train_data . index = range ( train_data . shape [ 0 ]) In [84]: train_data . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 1414 entries, 0 to 1413 Data columns (total 49 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 date 1414 non-null datetime64[ns] 1 round 1414 non-null object 2 day 1414 non-null object 3 result 1414 non-null object 4 gf 1414 non-null int64 5 ga 1414 non-null int64 6 opponent 1414 non-null object 7 xg 1414 non-null float64 8 xga 1414 non-null float64 9 poss 1414 non-null float64 10 attendance 1414 non-null float64 11 formation 1414 non-null object 12 sh 1414 non-null float64 13 sot 1414 non-null float64 14 dist 1414 non-null float64 15 fk 1414 non-null float64 16 pk 1414 non-null float64 17 pkatt 1414 non-null float64 18 season 1414 non-null int64 19 team 1414 non-null object 20 hour 1414 non-null int64 21 match_week 1414 non-null int64 22 day_sin 1414 non-null float64 23 day_cos 1414 non-null float64 24 venue_Home 1414 non-null uint8 25 opponent_0 1414 non-null int64 26 opponent_1 1414 non-null int64 27 opponent_2 1414 non-null int64 28 opponent_3 1414 non-null int64 29 opponent_4 1414 non-null int64 30 team_0 1414 non-null int64 31 team_1 1414 non-null int64 32 team_2 1414 non-null int64 33 team_3 1414 non-null int64 34 team_4 1414 non-null int64 35 num_defender 1414 non-null int64 36 num_striker 1414 non-null int64 37 offensive_midfielder 1414 non-null int64 38 center_midfielder 1414 non-null int64 39 target 1414 non-null int64 40 gf_rolling 1414 non-null float64 41 ga_rolling 1414 non-null float64 42 sh_rolling 1414 non-null float64 43 sot_rolling 1414 non-null float64 44 dist_rolling 1414 non-null float64 45 poss_rolling 1414 non-null float64 46 fk_rolling 1414 non-null float64 47 pk_rolling 1414 non-null float64 48 pkatt_rolling 1414 non-null float64 dtypes: datetime64[ns](1), float64(21), int64(20), object(6), uint8(1) memory usage: 531.8+ KB In [85]: train_data . head () Out[85]: date round day result gf ga opponent xg xga poss ... target gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling pk_rolling pkatt_rolling 0 2017-10-01 Matchweek 7 Sun W 2 0 Brighton 2.4 0.4 64.0 ... 2 1.333333 2.333333 15.333333 4.000000 17.933333 56.333333 0.666667 0.0 0.0 1 2017-10-22 Matchweek 9 Sun W 5 2 Everton 3.5 1.0 67.0 ... 2 0.666667 1.333333 14.666667 3.333333 17.500000 55.000000 0.666667 0.0 0.0 2 2017-11-05 Matchweek 11 Sun L 1 3 Manchester City 0.3 1.8 43.0 ... 0 2.333333 0.666667 22.000000 8.000000 17.500000 60.000000 0.333333 0.0 0.0 3 2017-11-18 Matchweek 12 Sat W 2 0 Tottenham 2.1 0.7 43.0 ... 2 2.666667 1.666667 20.333333 8.333333 18.500000 58.000000 0.333333 0.0 0.0 4 2017-11-26 Matchweek 13 Sun W 1 0 Burnley 1.8 0.4 64.0 ... 2 2.666667 1.666667 16.666667 7.333333 17.833333 51.000000 0.333333 0.0 0.0 5 rows × 49 columns In [86]: train_data . tail () Out[86]: date round day result gf ga opponent xg xga poss ... target gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling pk_rolling pkatt_rolling 1409 2021-04-24 Matchweek 33 Sat L 0 1 Chelsea 0.4 2.5 45.0 ... 0 2.666667 2.666667 11.333333 4.666667 15.866667 47.000000 0.000000 0.333333 0.333333 1410 2021-05-03 Matchweek 34 Mon W 2 1 Burnley 2.3 2.0 55.0 ... 2 1.666667 2.000000 9.333333 3.666667 19.300000 49.333333 0.333333 0.333333 0.333333 1411 2021-05-09 Matchweek 35 Sun L 0 1 Everton 1.3 1.5 68.0 ... 0 1.333333 1.666667 15.333333 3.666667 19.800000 55.333333 0.666667 0.333333 0.333333 1412 2021-05-15 Matchweek 36 Sat D 1 1 Brighton 1.7 0.8 49.0 ... 1 0.666667 1.000000 14.000000 2.000000 18.633333 56.000000 0.666667 0.000000 0.000000 1413 2021-05-23 Matchweek 38 Sun W 3 0 Southampton 1.3 1.5 38.0 ... 2 1.000000 1.000000 16.000000 2.000000 17.100000 57.333333 0.333333 0.000000 0.000000 5 rows × 49 columns It is important to realize that we want to predict the result of current match based on the data of previous matches. For that purpose, we have just added some features that represent past performance. Now, we can delete all those data for current matches that may provide the machine learning model a hint about the winner In [87]: train_data . drop ([ \"gf\" , 'ga' , 'sh' , 'sot' , 'dist' , 'poss' , 'fk' , 'pk' , 'pkatt' ], axis = 1 , inplace = True ) In [88]: print ( train_data . shape ) print ( test_data . shape ) (1414, 40) (364, 22) In [89]: drop_cols = categorical_columns + [ \"season\" ] drop_cols . remove ( \"venue\" ) train_data . drop ( drop_cols , axis = 1 , inplace = True ) In [90]: print ( train_data . shape ) (1414, 33) Now, we perform standardization to complete the feature engineering process. As outliers are present in the dataset, we apply robustscaling method In [91]: X_train = train_data . drop ( \"target\" , axis = 1 ) y_train = train_data [ \"target\" ] In [92]: from sklearn.preprocessing import RobustScaler cols = X_train . columns . to_list () cols . remove ( \"date\" ) index_labels = X_train [ 'date' ] scaler = RobustScaler ( unit_variance = True ) X_train = pd . DataFrame ( scaler . fit_transform ( X_train . drop ( \"date\" , axis = 1 )), columns = cols , index = index_labels ) In [93]: X_train . head () Out[93]: xg xga attendance hour match_week day_sin day_cos venue_Home opponent_0 opponent_1 ... center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling pk_rolling pkatt_rolling date 2017-10-01 1.471614 -0.981076 1.067667 -1.348980 -0.974263 0.000000 1.34898 1.34898 -1.348980 1.34898 ... 0.337245 0.000000 1.34898 0.856495 0.000000 0.141998 0.577104 0.67449 0.0 0.0 2017-10-22 2.820594 -0.245269 -0.214305 -1.011735 -0.824376 0.000000 1.34898 0.00000 -1.348980 1.34898 ... 0.337245 -0.674490 0.00000 0.685196 -0.385423 -0.088749 0.461683 0.67449 0.0 0.0 2017-11-05 -1.103711 0.735807 0.744333 -0.674490 -0.674490 0.000000 1.34898 0.00000 1.348980 -1.34898 ... 0.337245 1.011735 -0.89932 2.569485 2.312536 -0.088749 0.894510 0.00000 0.0 0.0 2017-11-18 1.103711 -0.613173 1.077319 -1.348980 -0.599546 -0.267182 0.00000 1.34898 -2.697959 0.00000 ... 0.337245 1.348980 0.44966 2.141237 2.505248 0.443743 0.721379 0.00000 0.0 0.0 2017-11-26 0.735807 -0.981076 -1.323435 -0.674490 -0.524603 0.000000 1.34898 0.00000 -1.348980 1.34898 ... 0.337245 1.348980 0.44966 1.199093 1.927114 0.088749 0.115421 0.00000 0.0 0.0 5 rows × 31 columns In [94]: X_train . describe () Out[94]: xg xga attendance hour match_week day_sin day_cos venue_Home opponent_0 opponent_1 ... center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling pk_rolling pkatt_rolling count 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 ... 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 1414.000000 mean 0.141368 0.142495 0.062910 0.081330 -0.005618 0.394912 0.495031 0.673536 -0.592444 0.397825 ... -0.306955 0.062250 0.082681 0.047671 0.057241 0.038763 0.030875 0.279050 0.433124 0.551422 std 0.963921 0.965150 0.932177 0.863651 0.801858 0.961781 0.818042 0.674728 1.319672 1.210626 ... 0.633861 0.795442 1.025703 0.963029 0.943689 1.045559 0.867709 0.830252 0.764844 0.870009 min -1.471614 -1.471614 -2.575754 -1.348980 -1.423923 -0.267182 -1.081798 0.000000 -2.697959 -1.348980 ... -1.011735 -1.348980 -1.798639 -2.398186 -2.312536 -3.265950 -2.106428 -0.674490 0.000000 0.000000 25% -0.613173 -0.613173 -0.666479 -0.337245 -0.674490 -0.267182 0.000000 0.000000 -1.348980 0.000000 ... -1.011735 -0.674490 -0.449660 -0.685196 -0.578134 -0.674490 -0.656455 -0.674490 0.000000 0.000000 50% 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 75% 0.735807 0.735807 0.682501 1.011735 0.674490 1.081798 1.348980 1.348980 0.000000 1.348980 ... 0.337245 0.674490 0.899320 0.663784 0.770845 0.674490 0.692524 0.674490 1.348980 1.348980 max 4.292208 4.292208 2.581727 1.348980 1.348980 2.430777 1.949332 1.348980 4.046939 4.046939 ... 0.674490 3.709694 4.946258 3.340330 3.083382 4.401933 2.135283 4.046939 4.046939 5.395918 8 rows × 31 columns In [95]: ax = X_train . plot . box ( figsize = ( 15 , 10 )) ax . set_xticklabels ( labels = cols , rotation = 90 ) Out[95]: [Text(0, 0, 'xg'), Text(0, 0, 'xga'), Text(0, 0, 'attendance'), Text(0, 0, 'hour'), Text(0, 0, 'match_week'), Text(0, 0, 'day_sin'), Text(0, 0, 'day_cos'), Text(0, 0, 'venue_Home'), Text(0, 0, 'opponent_0'), Text(0, 0, 'opponent_1'), Text(0, 0, 'opponent_2'), Text(0, 0, 'opponent_3'), Text(0, 0, 'opponent_4'), Text(0, 0, 'team_0'), Text(0, 0, 'team_1'), Text(0, 0, 'team_2'), Text(0, 0, 'team_3'), Text(0, 0, 'team_4'), Text(0, 0, 'num_defender'), Text(0, 0, 'num_striker'), Text(0, 0, 'offensive_midfielder'), Text(0, 0, 'center_midfielder'), Text(0, 0, 'gf_rolling'), Text(0, 0, 'ga_rolling'), Text(0, 0, 'sh_rolling'), Text(0, 0, 'sot_rolling'), Text(0, 0, 'dist_rolling'), Text(0, 0, 'poss_rolling'), Text(0, 0, 'fk_rolling'), Text(0, 0, 'pk_rolling'), Text(0, 0, 'pkatt_rolling')] Feature Selection In [96]: corr = X_train . corr () In [97]: mask = np . triu ( np . ones_like ( corr , dtype = bool )) cmap = sns . diverging_palette ( h_neg = 10 , h_pos = 240 , as_cmap = True ) sns . heatmap ( corr , mask = mask , center = 0 , cmap = cmap , linewidths = 1 , annot = False , fmt = \".2f\" ) Out[97]: <matplotlib.axes._subplots.AxesSubplot at 0x7f351d547bb0> In [98]: # Create positive correlation matrix corr_df = X_train . corr () . abs () # Create and apply mask mask = np . triu ( np . ones_like ( corr_df , dtype = bool )) tri_df = corr_df . mask ( mask ) to_drop = [ c for c in tri_df . columns if any ( tri_df [ c ] > 0.95 )] print ( to_drop ) ['opponent_0', 'opponent_1', 'opponent_2', 'opponent_3', 'opponent_4'] Before taking any action, let's double check if these columns have very high variance inflation factor In [99]: from statsmodels.stats.outliers_influence import variance_inflation_factor In [100]: def find_vif ( df ): vif_data = pd . DataFrame () vif_data [ \"feature\" ] = df . columns vif_data [ \"VIF\" ] = [ variance_inflation_factor ( df . values , i ) for i in range ( len ( df . columns ))] return vif_data In [101]: vif_xtrain = find_vif ( X_train ) print ( vif_xtrain ) feature VIF 0 xg 1.367425 1 xga 1.179430 2 attendance 1.099349 3 hour 1.641997 4 match_week 1.132262 5 day_sin 1.836828 6 day_cos 1.308681 7 venue_Home 1.683703 8 opponent_0 inf 9 opponent_1 inf 10 opponent_2 inf 11 opponent_3 inf 12 opponent_4 inf 13 team_0 inf 14 team_1 inf 15 team_2 inf 16 team_3 inf 17 team_4 inf 18 num_defender 1.427096 19 num_striker 3.185223 20 offensive_midfielder 6.661029 21 center_midfielder 4.563972 22 gf_rolling 2.122998 23 ga_rolling 1.179673 24 sh_rolling 3.197413 25 sot_rolling 3.560707 26 dist_rolling 1.150289 27 poss_rolling 2.408564 28 fk_rolling 1.261109 29 pk_rolling 7.046177 30 pkatt_rolling 7.183374 /usr/local/lib/python3.8/dist-packages/statsmodels/stats/outliers_influence.py:193: RuntimeWarning: divide by zero encountered in double_scalars vif = 1. / (1. - r_squared_i) In [102]: X_train_reduced = X_train . drop ([ 'opponent_0' , 'opponent_1' , 'opponent_2' , 'opponent_3' , 'opponent_4' ], axis = 1 ) vif_xtrain_reduced = find_vif ( X_train_reduced ) print ( vif_xtrain_reduced ) feature VIF 0 xg 1.367425 1 xga 1.179430 2 attendance 1.099349 3 hour 1.641997 4 match_week 1.132262 5 day_sin 1.836828 6 day_cos 1.308681 7 venue_Home 1.683703 8 team_0 inf 9 team_1 inf 10 team_2 inf 11 team_3 inf 12 team_4 inf 13 num_defender 1.427096 14 num_striker 3.185223 15 offensive_midfielder 6.661029 16 center_midfielder 4.563972 17 gf_rolling 2.122998 18 ga_rolling 1.179673 19 sh_rolling 3.197413 20 sot_rolling 3.560707 21 dist_rolling 1.150289 22 poss_rolling 2.408564 23 fk_rolling 1.261109 24 pk_rolling 7.046177 25 pkatt_rolling 7.183374 /usr/local/lib/python3.8/dist-packages/statsmodels/stats/outliers_influence.py:193: RuntimeWarning: divide by zero encountered in double_scalars vif = 1. / (1. - r_squared_i) Even after removing all feature encodings for the 'oponnent' column, the VIF is extremely high for the 'team' column. IT suggests that some part of the hash encoding for the 'team' column may be redundant. So, we try dropping 'team_0' from the columns In [103]: X_train_reduced = X_train . drop ([ 'opponent_0' , 'opponent_1' , 'opponent_2' , 'opponent_3' , 'opponent_4' ] + [ \"team_0\" ], axis = 1 ) vif_xtrain_reduced = find_vif ( X_train_reduced ) print ( vif_xtrain_reduced ) feature VIF 0 xg 1.367425 1 xga 1.179430 2 attendance 1.099349 3 hour 1.641997 4 match_week 1.132262 5 day_sin 1.836828 6 day_cos 1.308681 7 venue_Home 1.683703 8 team_1 1.588378 9 team_2 1.951065 10 team_3 2.296598 11 team_4 1.638009 12 num_defender 1.427096 13 num_striker 3.185223 14 offensive_midfielder 6.661029 15 center_midfielder 4.563972 16 gf_rolling 2.122998 17 ga_rolling 1.179673 18 sh_rolling 3.197413 19 sot_rolling 3.560707 20 dist_rolling 1.150289 21 poss_rolling 2.408564 22 fk_rolling 1.261109 23 pk_rolling 7.046177 24 pkatt_rolling 7.183374 In [104]: vif_xtrain_reduced [ \"VIF\" ] . mean () Out[104]: 2.546853952159468 Now, the VIF for each feature is less than 10. Moreover, the mean VIF is less than 6. So, we don't have any multi-collinear column in the dataset. In [105]: X_train = X_train_reduced . copy () In [106]: X_train . head () Out[106]: xg xga attendance hour match_week day_sin day_cos venue_Home team_1 team_2 ... center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling pk_rolling pkatt_rolling date 2017-10-01 1.471614 -0.981076 1.067667 -1.348980 -0.974263 0.000000 1.34898 1.34898 1.34898 1.34898 ... 0.337245 0.000000 1.34898 0.856495 0.000000 0.141998 0.577104 0.67449 0.0 0.0 2017-10-22 2.820594 -0.245269 -0.214305 -1.011735 -0.824376 0.000000 1.34898 0.00000 1.34898 0.00000 ... 0.337245 -0.674490 0.00000 0.685196 -0.385423 -0.088749 0.461683 0.67449 0.0 0.0 2017-11-05 -1.103711 0.735807 0.744333 -0.674490 -0.674490 0.000000 1.34898 0.00000 -1.34898 1.34898 ... 0.337245 1.011735 -0.89932 2.569485 2.312536 -0.088749 0.894510 0.00000 0.0 0.0 2017-11-18 1.103711 -0.613173 1.077319 -1.348980 -0.599546 -0.267182 0.00000 1.34898 0.00000 1.34898 ... 0.337245 1.348980 0.44966 2.141237 2.505248 0.443743 0.721379 0.00000 0.0 0.0 2017-11-26 0.735807 -0.981076 -1.323435 -0.674490 -0.524603 0.000000 1.34898 0.00000 1.34898 0.00000 ... 0.337245 1.348980 0.44966 1.199093 1.927114 0.088749 0.115421 0.00000 0.0 0.0 5 rows × 25 columns Now, let's try other feature selection method to check if the number of features can be further reduced In [107]: from sklearn.feature_selection import mutual_info_classif In [108]: mutual_info = mutual_info_classif ( X_train , y_train ) print ( mutual_info ) [9.85143324e-02 9.41025067e-02 7.46116039e-04 0.00000000e+00 0.00000000e+00 2.72880758e-02 0.00000000e+00 1.53765577e-03 1.73446657e-01 0.00000000e+00 0.00000000e+00 2.26306621e-02 3.23308455e-03 7.93408435e-03 1.38475805e-04 3.83061653e-02 6.49905140e-03 1.63290469e-02 9.45459925e-03 2.15715804e-02 1.65272167e-04 6.06740108e-03 0.00000000e+00 4.99437543e-03 6.45069413e-03] In [109]: mutual_info = pd . Series ( mutual_info ) mutual_info . index = X_train . columns mutual_info . sort_values ( ascending = False ) Out[109]: team_1 0.173447 xg 0.098514 xga 0.094103 center_midfielder 0.038306 day_sin 0.027288 team_4 0.022631 sot_rolling 0.021572 ga_rolling 0.016329 sh_rolling 0.009455 num_striker 0.007934 gf_rolling 0.006499 pkatt_rolling 0.006451 poss_rolling 0.006067 pk_rolling 0.004994 num_defender 0.003233 venue_Home 0.001538 attendance 0.000746 dist_rolling 0.000165 offensive_midfielder 0.000138 team_3 0.000000 team_2 0.000000 day_cos 0.000000 match_week 0.000000 hour 0.000000 fk_rolling 0.000000 dtype: float64 In [110]: mask_MI = ~ ( X_train . columns . isin ( mutual_info [ mutual_info == 0 ] . index )) mask_MI Out[110]: array([ True, True, True, False, False, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True]) In [111]: X_train . columns [ mask_MI ] Out[111]: Index(['xg', 'xga', 'attendance', 'day_sin', 'venue_Home', 'team_1', 'team_4', 'num_defender', 'num_striker', 'offensive_midfielder', 'center_midfielder', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'pk_rolling', 'pkatt_rolling'], dtype='object') As it is clear, there are a few columns which provide zero gain in mutual information. These are the candidate columns that can be dropped. But before taking action, let'try other methods In [112]: from sklearn.feature_selection import RFE from sklearn.ensemble import RandomForestClassifier rfe_rf = RFE ( estimator = RandomForestClassifier (), n_features_to_select = 20 , verbose = 0 ) rfe_rf . fit ( X_train , y_train ) rf_mask = rfe_rf . support_ In [113]: X_train . columns [ rf_mask ] Out[113]: Index(['xg', 'xga', 'attendance', 'hour', 'match_week', 'day_sin', 'day_cos', 'team_1', 'team_2', 'team_3', 'team_4', 'num_striker', 'center_midfielder', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'fk_rolling'], dtype='object') In [114]: from sklearn.ensemble import GradientBoostingRegressor rfe_gb = RFE ( estimator = GradientBoostingRegressor (), n_features_to_select = 20 , step = 5 ) rfe_gb . fit ( X_train , y_train ) gb_mask = rfe_gb . support_ In [115]: X_train . columns [ gb_mask ] Out[115]: Index(['xg', 'xga', 'attendance', 'hour', 'match_week', 'day_sin', 'venue_Home', 'team_1', 'team_2', 'team_3', 'team_4', 'num_striker', 'center_midfielder', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'fk_rolling'], dtype='object') In [116]: from sklearn.ensemble import ExtraTreesClassifier etc = ExtraTreesClassifier () etc . fit ( X_train , y_train ) ranked_features = pd . Series ( etc . feature_importances_ , index = X_train . columns ) top_features = ranked_features . nlargest ( 20 ) etc_mask = X_train . columns . isin ( top_features . index ) In [117]: X_train . columns [ etc_mask ] Out[117]: Index(['xg', 'xga', 'attendance', 'hour', 'match_week', 'day_sin', 'day_cos', 'team_1', 'team_2', 'team_3', 'team_4', 'num_striker', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'fk_rolling', 'pkatt_rolling'], dtype='object') In [118]: votes = np . sum ([ mask_MI , rf_mask , gb_mask , etc_mask ], axis = 0 ) In [119]: mask = votes >= 3 In [120]: selected_col = X_train . columns [ mask ] . to_list () In [121]: selected_col Out[121]: ['xg', 'xga', 'attendance', 'hour', 'match_week', 'day_sin', 'team_1', 'team_2', 'team_3', 'team_4', 'num_striker', 'center_midfielder', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'fk_rolling'] In [122]: selected_col . extend ([ \"team_3\" ]) selected_col Out[122]: ['xg', 'xga', 'attendance', 'hour', 'match_week', 'day_sin', 'team_1', 'team_2', 'team_3', 'team_4', 'num_striker', 'center_midfielder', 'gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'dist_rolling', 'poss_rolling', 'fk_rolling', 'team_3'] In [123]: X_train = X_train [ selected_col ] In [124]: X_train . shape Out[124]: (1414, 20) Pipeline Creation Now we have completed all the preprocessing steps on the training data. However, we need to apply all these same steps on the validation data and test. And we need to do so without data leakage. To automate the preprocessing step and prevent data leakge, we build a pipeline for machine learning model. For that purpose, we again start with the unprocessed data, which will be fed to the pipeline. In [125]: match_df [ \"dayofweek\" ] = match_df [ \"date\" ] . dt . dayofweek match_df . drop ( \"day\" , axis = 1 , inplace = True ) train_data = match_df [ match_df [ \"season\" ] != 2022 ] test_data = match_df [ match_df [ \"season\" ] == 2022 ] train_data . reset_index ( drop = True , inplace = True ) test_data . reset_index ( drop = True , inplace = True ) print ( train_data . shape ) print ( test_data . shape ) (1456, 22) (364, 22) In [126]: from sklearn.pipeline import Pipeline , FeatureUnion , clone , make_pipeline from sklearn.base import BaseEstimator , TransformerMixin from sklearn.impute import SimpleImputer , IterativeImputer from sklearn.compose import ColumnTransformer , make_column_selector In [127]: class SelectColumnsTransfomer ( BaseEstimator , TransformerMixin ): \"\"\" A DataFrame transformer that provides column selection Allows to select columns by name from pandas dataframes in scikit-learn pipelines. Parameters ---------- columns : list of str, names of the dataframe columns to select Default: [] \"\"\" def __init__ ( self , columns = []): self . columns = columns def transform ( self , X , ** transform_params ): \"\"\" Selects columns of a DataFrame Parameters ---------- X : pandas DataFrame Returns ---------- trans : pandas DataFrame contains selected columns of X \"\"\" trans = X [ self . columns ] . copy () return trans def fit ( self , X , y = None , ** fit_params ): \"\"\" Do nothing function Parameters ---------- X : pandas DataFrame y : default None Returns ---------- self \"\"\" return self class DataFrameFunctionTransformer ( BaseEstimator , TransformerMixin ): \"\"\" A DataFrame transformer providing imputation or function application Parameters ---------- impute : Boolean, default False func : function that acts on an array of the form [n_elements, 1] \"\"\" def __init__ ( self , func ): self . func = func def transform ( self , X , ** transformparams ): \"\"\" Transforms a DataFrame Parameters ---------- X : DataFrame Returns ---------- trans : pandas DataFrame Transformation of X \"\"\" trans = pd . DataFrame ( X ) . apply ( self . func ) . copy () return trans def fit ( self , X , y = None , ** fitparams ): \"\"\" Fixes the values to impute or does nothing Parameters ---------- X : pandas DataFrame y : not used, API requirement Returns ---------- self \"\"\" return self class DataFrameFeatureUnion ( BaseEstimator , TransformerMixin ): \"\"\" A DataFrame transformer that unites several DataFrame transformers Fit several DataFrame transformers and provides a concatenated Data Frame Parameters ---------- list_of_transformers : list of DataFrameTransformers \"\"\" def __init__ ( self , list_of_transformers ): self . list_of_transformers = list_of_transformers def transform ( self , X , ** transformparamn ): \"\"\" Applies the fitted transformers on a DataFrame Parameters ---------- X : pandas DataFrame Returns ---------- concatted : pandas DataFrame \"\"\" concatted = pd . concat ([ transformer . transform ( X ) for transformer in self . fitted_transformers_ ], axis = 1 ) . copy () return concatted def fit ( self , X , y = None , ** fitparams ): \"\"\" Fits several DataFrame Transformers Parameters ---------- X : pandas DataFrame y : not used, API requirement Returns ---------- self : object \"\"\" self . fitted_transformers_ = [] for transformer in self . list_of_transformers : fitted_trans = clone ( transformer ) . fit ( X , y = None , ** fitparams ) self . fitted_transformers_ . append ( fitted_trans ) return self class ToDummiesTransformer ( BaseEstimator , TransformerMixin ): \"\"\" A Dataframe transformer that provide dummy variable encoding \"\"\" def __init__ ( self ): self . drop_first = True def transform ( self , X , ** transformparams ): \"\"\" Returns a dummy variable encoded version of a DataFrame Parameters ---------- X : pandas DataFrame Returns ---------- trans : pandas DataFrame \"\"\" trans = pd . get_dummies ( X , drop_first = self . drop_first ) . copy () return trans def fit ( self , X , y = None , ** fitparams ): \"\"\" Do nothing operation Returns ---------- self : object \"\"\" return self In [128]: class RoundTransformer ( BaseEstimator , TransformerMixin ): \"\"\" A Dataframe transformer to extract match week from the 'round' column \"\"\" def fit ( self , X , y = None , ** fit_params ): return self def transform ( self , X , ** transformparams ): X = X . squeeze () # convert to a series to perform string operations match_week = X . str . split () . str [ 1 ] . astype ( int ) return match_week . to_frame ( name = \"match_week\" ) . copy () In [129]: class DayTransformer ( BaseEstimator , TransformerMixin ): \"\"\" A Dataframe transformer for feature encoding the 'day' column \"\"\" def fit ( self , X , y = None , ** fit_params ): return self def transform ( self , X , ** transformparams ): day_sin = np . sin ( X * ( 2 * np . pi / 7 )) day_cos = np . cos ( X * ( 2 * np . pi / 7 )) trans_day = np . concatenate (( day_sin , day_cos ), axis = 1 ) return pd . DataFrame ( trans_day , columns = [ \"day_sin\" , \"day_cos\" ]) . copy () In [130]: class FeatureHasher ( BaseEstimator , TransformerMixin ): def __init__ ( self , hash_size , name_map = None ): self . name_map = name_map self . hash_size = hash_size self . encoder = HashingEncoder ( n_components = self . hash_size , return_df = True ) def fit ( self , X , y = None , ** fit_params ): self . encoder . fit ( X ) return self def transform ( self , X , ** transformparams ): trans_x = self . encoder . transform ( X ) if self . name_map : trans_x . rename ( self . name_map , axis = 1 , inplace = True ) return trans_x . copy () In [131]: from category_encoders import OrdinalEncoder class TargetEncoder ( BaseEstimator , TransformerMixin ): def __init__ ( self , col_name , mapping ): self . col_name = col_name self . mapping = mapping self . cols_mapping = [{ \"col\" : self . col_name , \"mapping\" : self . mapping }] self . encoder = OrdinalEncoder ( mapping = self . cols_mapping , return_df = True ) def fit ( self , X , y = None , ** fit_params ): self . encoder . fit ( X ) return self def transform ( self , X , ** transformparams ): return self . encoder . transform ( X ) . copy () In [132]: class FormationTransformer ( BaseEstimator , TransformerMixin ): def fit ( self , X , y = None , ** fit_params ): return self def transform ( self , X , ** transformparams ): X = X . squeeze () formation = X . str . replace ( \"◆\" , \"\" ) num_defender = formation . str . split ( pat = \"-\" ) . str [ 0 ] . astype ( int ) num_striker = formation . str . split ( pat = \"-\" ) . str [ - 1 ] . astype ( int ) offensive_midfield_mapper = lambda x : x [ 4 ] if len ( x ) == 7 else 0 center_midfield_mapper = lambda x : x [ 2 ] if len ( x ) == 5 else 0 offensive_midfielder = formation . apply ( offensive_midfield_mapper ) . astype ( int ) center_midfielder = formation . apply ( center_midfield_mapper ) . astype ( int ) return pd . DataFrame ( dict ( num_defender = num_defender , num_striker = num_striker , \\ offensive_midfielder = offensive_midfielder , \\ center_midfielder = center_midfielder )) . copy () In [133]: class RollingAverageComputer ( BaseEstimator , TransformerMixin ): def __init__ ( self , columns , group_by , sort_by ): self . cols = columns self . group_by = group_by self . sort_by = sort_by def fit ( self , X , y = None , ** fit_params ): return self def transform ( self , X , ** transformparams ): def rolling_averages ( group , cols , new_cols , sort_by ): group = group . sort_values ( sort_by ) rolling_stats = group [ cols ] . rolling ( 3 , closed = 'left' ) . mean () group [ new_cols ] = rolling_stats group = group . dropna ( subset = new_cols ) return group new_cols = [ f \" { c } _rolling\" for c in self . cols ] trans_X = X . groupby ( self . group_by ) . apply ( lambda x : rolling_averages ( x , self . cols , new_cols , self . sort_by )) trans_X . index = trans_X . index . droplevel () trans_X . index = range ( trans_X . shape [ 0 ]) return trans_X . copy () In [134]: class FeatureSelector ( BaseEstimator , TransformerMixin ): def __init__ ( self , select_columns ): self . to_select = select_columns def fit ( self , X , y = None , ** fit_params ): return self def transform ( self , X , ** transformparams ): trans_X = X [ self . to_select ] return trans_X . copy () In [135]: from sklearn.preprocessing import RobustScaler class Standardizer ( BaseEstimator , TransformerMixin ): def __init__ ( self , scaler ): self . scaler = scaler def fit ( self , X , y = None , ** fit_params ): self . cols = X . columns self . scaler . fit ( X ) return self def transform ( self , X , ** transformparams ): trans_X = pd . DataFrame ( scaler . transform ( X ), columns = self . cols ) return trans_X . copy () In [136]: class ImputeTransformer ( BaseEstimator , TransformerMixin ): def __init__ ( self , imputer , columns ): self . imputer = imputer self . cols = columns def fit ( self , X , y = None , ** fit_params ): if not self . cols : self . cols = X . select_dtypes ( include = np . number ) . columns . tolist () self . imputer . fit ( X [ self . cols ]) self . other_cols = X . columns . drop ( self . cols ) . to_list () return self def transform ( self , X , ** transformparams ): array = self . imputer . transform ( X [ self . cols ]) trans_X = pd . DataFrame ( array , columns = self . cols ) trans_X = pd . concat ([ trans_X , X [ self . other_cols ]], axis = 1 ) return trans_X . copy () In [137]: numerical_cols = train_data . select_dtypes ( include = np . number ) . columns . tolist () rolling_cols = [ \"gf\" , 'ga' , 'sh' , 'sot' , 'dist' , 'poss' , 'fk' , 'pk' , 'pkatt' ] selected_features = [ 'xg' , 'xga' , 'attendance' , 'hour' , \\ 'match_week' , 'day_sin' , 'day_cos' , \\ 'team_1' , 'team_2' , 'team_3' , 'team_4' , \\ 'num_striker' , 'center_midfielder' , \\ 'gf_rolling' , 'ga_rolling' , 'sh_rolling' , \\ 'sot_rolling' , 'dist_rolling' , 'poss_rolling' , \\ 'fk_rolling' , 'result' ] preprocessor = Pipeline ([ ( 'feature_encoding' , DataFrameFeatureUnion ([ Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'round' ])), ( 'week_day' , RoundTransformer ()), ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'dayofweek' ])), ( 'week_day' , DayTransformer ()) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'venue' ])), ( 'one_hot' , ToDummiesTransformer ()) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'opponent' ])), ( 'hasher' , FeatureHasher ( 5 , { \"col_0\" : \"opponent\" + \"_0\" , \"col_1\" : \\ \"opponent\" + \"_1\" , \"col_2\" : \"opponent\" + \"_2\" , \\ \"col_3\" : \"opponent\" + \"_3\" , \"col_4\" : \"opponent\" + \"_4\" })) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'team' ])), ( 'hasher' , FeatureHasher ( 5 , { \"col_0\" : 'team' + \"_0\" , \"col_1\" : \\ 'team' + \"_1\" , \"col_2\" : 'team' + \"_2\" , \\ \"col_3\" : 'team' + \"_3\" , \"col_4\" : 'team' + \"_4\" })) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'formation' ])), ( 'formation_transformer' , FormationTransformer ()) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ 'result' ])), ( 'formation_transformer' , TargetEncoder ( \"result\" , dict ([( 'L' , 0 ), ( 'D' , 1 ), ( 'W' , 2 )]))) ]), Pipeline ([ ( 'extract' , SelectColumnsTransfomer ([ \"team\" , \"date\" ] + numerical_cols )) ]) ])), ( 'imputation' , Pipeline ([ ( 'median_imputation' , ImputeTransformer ( SimpleImputer ( strategy = 'median' ), [ 'dist' ])), ( 'MICE' , ImputeTransformer ( IterativeImputer ( estimator = HuberRegressor ( max_iter = 15000 ), \\ random_state = 2022 , skip_complete = True ), None )) ])), ( 'rolling_average' , RollingAverageComputer ( rolling_cols , 'team' , 'date' )), ( 'feature_selection' , FeatureSelector ( select_columns = selected_features )) ]) In [138]: processed_train_data = preprocessor . fit_transform ( train_data ) processed_test_data = preprocessor . transform ( test_data ) processed_train_data . head () Out[138]: xg xga attendance hour match_week day_sin day_cos team_1 team_2 team_3 ... num_striker center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling result 0 2.4 0.4 59378.0 12.0 7.0 -0.781831 0.623490 0.0 1.0 0.0 ... 3.0 4.0 1.333333 2.333333 15.333333 4.000000 17.933333 56.333333 0.666667 2.0 1 3.5 1.0 39189.0 13.0 9.0 -0.781831 0.623490 0.0 1.0 0.0 ... 3.0 4.0 0.666667 1.333333 14.666667 3.333333 17.500000 55.000000 0.666667 2.0 2 0.3 1.8 54286.0 14.0 11.0 -0.781831 0.623490 0.0 1.0 0.0 ... 3.0 4.0 2.333333 0.666667 22.000000 8.000000 17.500000 60.000000 0.333333 0.0 3 2.1 0.7 59530.0 12.0 12.0 -0.974928 -0.222521 0.0 1.0 0.0 ... 3.0 4.0 2.666667 1.666667 20.333333 8.333333 18.500000 58.000000 0.333333 2.0 4 1.8 0.4 21722.0 14.0 13.0 -0.781831 0.623490 0.0 1.0 0.0 ... 3.0 4.0 2.666667 1.666667 16.666667 7.333333 17.833333 51.000000 0.333333 2.0 5 rows × 21 columns In [139]: processed_test_data . head () Out[139]: xg xga attendance hour match_week day_sin day_cos team_1 team_2 team_3 ... num_striker center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling result 0 1.1 1.0 59919.0 16.0 6.0 -0.781831 0.623490 0.0 1.0 0.0 ... 1.0 0.0 0.333333 2.333333 6.666667 2.000000 15.400000 36.333333 0.333333 2.0 1 0.6 1.6 31266.0 17.0 7.0 -0.974928 -0.222521 0.0 1.0 0.0 ... 1.0 0.0 1.333333 2.000000 8.666667 3.333333 15.066667 40.000000 0.333333 1.0 2 1.9 0.9 59475.0 20.0 8.0 0.000000 1.000000 0.0 1.0 0.0 ... 1.0 0.0 1.333333 0.333333 11.000000 4.000000 19.833333 47.333333 0.666667 1.0 3 0.9 1.4 32209.0 12.0 10.0 -0.974928 -0.222521 0.0 1.0 0.0 ... 1.0 0.0 1.666667 1.000000 12.333333 5.000000 18.800000 47.333333 0.666667 2.0 4 0.6 3.8 53092.0 17.0 12.0 -0.974928 -0.222521 0.0 1.0 0.0 ... 1.0 0.0 1.333333 0.666667 11.333333 4.333333 18.466667 44.000000 0.666667 0.0 5 rows × 21 columns In [140]: processed_train_data . shape Out[140]: (1414, 21) In [141]: processed_test_data . shape Out[141]: (322, 21) In [142]: X_train = processed_train_data . drop ( \"result\" , axis = 1 ) y_train = processed_train_data [ \"result\" ] X_test = processed_test_data . drop ( \"result\" , axis = 1 ) y_test = processed_test_data [ \"result\" ] Baseline Model In [143]: from sklearn.svm import SVC , LinearSVC , NuSVC from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier , GradientBoostingClassifier from sklearn.naive_bayes import GaussianNB , BernoulliNB In [144]: # Evaluation & CV Libraries from sklearn.metrics import precision_score , accuracy_score , recall_score , f1_score , confusion_matrix from sklearn.model_selection import RandomizedSearchCV , GridSearchCV , RepeatedStratifiedKFold In [145]: models = [( \"SVC\" , SVC ()),( 'KNN' , KNeighborsClassifier ( n_neighbors = 10 )), ( \"DTC\" , DecisionTreeClassifier ()),( \"GNB\" , GaussianNB ()), ( \"NuSVC\" , NuSVC ()),( \"BNB\" , BernoulliNB ()), ( 'RF' , RandomForestClassifier ()),( 'ADA' , AdaBoostClassifier ()), ( 'XGB' , GradientBoostingClassifier ())] results = [] names = [] finalResults = [] for name , classifier in models : model = Pipeline ([ ( 'standardizer' , RobustScaler ( unit_variance = True )), ( name , classifier ) ]) model . fit ( X_train , y_train ) model_results = model . predict ( X_test ) score = precision_score ( y_test , model_results , average = 'micro' ) results . append ( score ) names . append ( name ) finalResults . append (( name , score )) finalResults . sort ( key = lambda k : k [ 1 ], reverse = True ) In [146]: finalResults Out[146]: [('RF', 0.562111801242236), ('XGB', 0.5527950310559007), ('SVC', 0.5496894409937888), ('ADA', 0.5403726708074534), ('BNB', 0.531055900621118), ('NuSVC', 0.515527950310559), ('KNN', 0.5124223602484472), ('GNB', 0.5031055900621118), ('DTC', 0.4409937888198758)] Based on the performance of these baseline models, we select the top three ML algorithms and try improve their performance through hyperparameter optimization. Hyperparameter tuning Let's first define the hyperparameter space for each algorithm. In [147]: np . random . seed ( 2022 ) model_params = { 'RF' : { 'model' : RandomForestClassifier , 'params' : { 'max_depth' : [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 , 100 , None ], 'max_features' : [ 'log2' , 'sqrt' , None ], 'min_samples_leaf' : [ 1 , 2 , 4 ], 'min_samples_split' : [ 2 , 5 , 10 , 20 , 30 , 40 , 50 , 60 ], 'n_estimators' : [ 100 , 200 , 400 , 600 , 800 , 1000 , 1200 , 1400 ] } }, 'SVC' : { 'model' : SVC , 'params' : { 'kernel' : [ 'linear' , 'rbf' ], 'gamma' : [ 0.01 , 0.1 , 1 , 10 , 100 ], 'C' : [ 0.001 , 0.01 , 0.1 , 1 , 10 , 100 ] } }, 'ADA' : { 'model' : AdaBoostClassifier , 'params' : { 'learning_rate' : 10 ** ( - 4 * np . random . rand ( 20 )), 'n_estimators' : [ 10 , 50 , 100 , 200 , 350 , 500 ], 'algorithm' : [ 'SAMME' , 'SAMME.R' ] } }, 'XGB' : { 'model' : GradientBoostingClassifier , 'params' : { 'learning_rate' : 10 ** ( - 3 * np . random . rand ( 25 )), 'n_estimators' : [ 10 , 50 , 100 , 200 , 400 , 600 , 800 , 1000 ], 'max_depth' : np . random . randint ( 3 , 10 , 5 ), 'subsample' : np . random . uniform ( 0.6 , 1.0 , 20 ) } } } It is important to remember that we are dealing with time-series data. Here the chronological performance of each team matters. So, we need to be careful when performing cross-validation for hyperparameter tuning. We could have used the TimeSeriesSplit function provided by sklearn. But it doesn't allow us to define the number of records that we want to include in the training and validation set. We have found the following code from https://medium.com/eatpredlove/time-series-cross-validation-a-walk-forward-approach-in-python-8534dd1db51a very useful for our purpose. In [148]: import numpy as np class expanding_window ( object ): ''' Parameters ---------- Note that if you define a horizon that is too far, then subsequently the split will ignore horizon length such that there is validation data left. This similar to Prof Rob hyndman's TsCv initial: int initial train length horizon: int forecast horizon (forecast length). Default = 1 period: int length of train data to add each iteration ''' def __init__ ( self , initial = 1 , horizon = 1 , period = 1 ): self . initial = initial self . horizon = horizon self . period = period def split ( self , data ): ''' Parameters ---------- Data: Training data Returns ------- train_index ,test_index: index for train and valid set similar to sklearn model selection ''' self . data = data self . counter = 0 # for us to iterate and track later data_length = data . shape [ 0 ] # rows data_index = list ( np . arange ( data_length )) output_train = [] output_test = [] # append initial output_train . append ( list ( np . arange ( self . initial ))) progress = [ x for x in data_index if x not in list ( np . arange ( self . initial )) ] # indexes left to append to train output_test . append ([ x for x in data_index if x not in output_train [ self . counter ]][: self . horizon ] ) # clip initial indexes from progress since that is what we are left while len ( progress ) != 0 : temp = progress [: self . period ] to_add = output_train [ self . counter ] + temp # update the train index output_train . append ( to_add ) # increment counter self . counter += 1 # then we update the test index to_add_test = [ x for x in data_index if x not in output_train [ self . counter ] ][: self . horizon ] output_test . append ( to_add_test ) # update progress progress = [ x for x in data_index if x not in output_train [ self . counter ]] # clip the last element of output_train and output_test output_train = output_train [: - 1 ] output_test = output_test [: - 1 ] # mimic sklearn output index_output = [( train , test ) for train , test in zip ( output_train , output_test )] return index_output In [149]: from tqdm import tqdm from sklearn.model_selection import ParameterSampler tscv = expanding_window ( 500 , 100 , 100 ) index_output = tscv . split ( train_data ) best_param = {} best_score = {} for model_name , params in model_params . items (): print ( f \"Starting parameter tuning for { model_name } \" ) initializer = params [ 'model' ] param_distribution = params [ 'params' ] total_eval = 50 param_configs = ParameterSampler ( param_distribution , total_eval , random_state = 2022 ) # training_scores = [] # validation_scores = [] best_so_far = - np . inf for param in tqdm ( param_configs ): train_score_fold = [] dev_score_fold = [] for train_index , val_index in index_output : training_data = train_data . iloc [ train_index , :] validation_data = train_data . iloc [ val_index , :] validation_data = validation_data . reset_index ( drop = True ) processed_training_data = preprocessor . fit_transform ( training_data ) processed_validation_data = preprocessor . transform ( validation_data ) X_train , y_train = processed_training_data . drop ( \"result\" , axis = 1 ), processed_training_data [ \"result\" ] X_val , y_val = processed_validation_data . drop ( \"result\" , axis = 1 ), processed_validation_data [ \"result\" ] model = Pipeline ([ ( 'standardization' , RobustScaler ( unit_variance = True )), ( model_name , initializer ( ** param )) ]) model . fit ( X_train , y_train ) val_pred = model . predict ( X_val ) train_pred = model . predict ( X_train ) dev_score = precision_score ( y_val , val_pred , average = 'micro' ) train_score = precision_score ( y_train , train_pred , average = 'micro' ) train_score_fold . append ( train_score ) dev_score_fold . append ( dev_score ) mean_train_score = np . mean ( train_score_fold ) mean_dev_score = np . mean ( dev_score_fold ) print ( f \" { param } : { mean_dev_score } \" ) # training_scores.append(mean_train_score) # validation_scores.append(mean_dev_score) if mean_dev_score > best_so_far : print ( f \"Best score improved from { best_so_far } to { mean_dev_score } \" ) best_score [ model_name ] = mean_dev_score best_param [ model_name ] = param best_so_far = mean_dev_score print ( \" \\n \" ) Starting parameter tuning for RF 2%|▏ | 1/50 [00:52<42:42, 52.29s/it] {'n_estimators': 1000, 'min_samples_split': 60, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 90}: 0.5711498252307985 Best score improved from -inf to 0.5711498252307985 4%|▍ | 2/50 [01:41<40:19, 50.41s/it] {'n_estimators': 800, 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 30}: 0.5700143943142459 6%|▌ | 3/50 [02:34<40:29, 51.69s/it] {'n_estimators': 1000, 'min_samples_split': 40, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}: 0.5791461578558822 Best score improved from 0.5711498252307985 to 0.5791461578558822 8%|▊ | 4/50 [03:06<33:40, 43.93s/it] {'n_estimators': 100, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 90}: 0.5700799956076172 10%|█ | 5/50 [03:40<30:15, 40.35s/it] {'n_estimators': 200, 'min_samples_split': 50, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}: 0.567580306514221 12%|█▏ | 6/50 [05:11<42:15, 57.62s/it] {'n_estimators': 1400, 'min_samples_split': 50, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 90}: 0.5686928560277513 14%|█▍ | 7/50 [05:52<37:25, 52.23s/it] {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 40}: 0.5744412082227519 16%|█▌ | 8/50 [06:28<32:47, 46.84s/it] {'n_estimators': 100, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}: 0.56632623496072 18%|█▊ | 9/50 [07:00<28:52, 42.26s/it] {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80}: 0.5472938953819123 20%|██ | 10/50 [07:39<27:26, 41.15s/it] {'n_estimators': 200, 'min_samples_split': 30, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 50}: 0.5620845957023446 22%|██▏ | 11/50 [08:22<27:10, 41.81s/it] {'n_estimators': 600, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}: 0.5732510577439559 24%|██▍ | 12/50 [09:10<27:37, 43.63s/it] {'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}: 0.5643837500363829 26%|██▌ | 13/50 [10:12<30:21, 49.24s/it] {'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 40}: 0.5575783153889509 28%|██▊ | 14/50 [11:07<30:39, 51.09s/it] {'n_estimators': 1200, 'min_samples_split': 60, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 60}: 0.5755903190298655 30%|███ | 15/50 [12:06<31:05, 53.31s/it] {'n_estimators': 1200, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 90}: 0.5756577857628591 32%|███▏ | 16/50 [12:40<26:55, 47.51s/it] {'n_estimators': 100, 'min_samples_split': 40, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 90}: 0.5584502231912513 34%|███▍ | 17/50 [13:42<28:35, 51.99s/it] {'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 80}: 0.559864722552239 36%|███▌ | 18/50 [14:18<25:07, 47.11s/it] {'n_estimators': 100, 'min_samples_split': 50, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}: 0.5641183614825086 38%|███▊ | 19/50 [15:03<23:58, 46.40s/it] {'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}: 0.5723673354907165 40%|████ | 20/50 [16:28<29:07, 58.26s/it] {'n_estimators': 1200, 'min_samples_split': 30, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': None}: 0.5574990342050186 42%|████▏ | 21/50 [17:25<27:50, 57.59s/it] {'n_estimators': 1000, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 60}: 0.5688907778486321 44%|████▍ | 22/50 [18:01<23:51, 51.12s/it] {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50}: 0.5690632053280131 46%|████▌ | 23/50 [18:40<21:22, 47.52s/it] {'n_estimators': 400, 'min_samples_split': 60, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 70}: 0.5711635051213594 48%|████▊ | 24/50 [19:23<20:03, 46.29s/it] {'n_estimators': 400, 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 70}: 0.5724602965652096 50%|█████ | 25/50 [20:14<19:50, 47.64s/it] {'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60}: 0.5658657161303637 52%|█████▏ | 26/50 [21:16<20:51, 52.13s/it] {'n_estimators': 1200, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None}: 0.5710585443749718 54%|█████▍ | 27/50 [22:03<19:21, 50.51s/it] {'n_estimators': 200, 'min_samples_split': 30, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 40}: 0.5588333196622627 56%|█████▌ | 28/50 [22:49<18:01, 49.17s/it] {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 60}: 0.554537893561455 58%|█████▊ | 29/50 [23:24<15:42, 44.86s/it] {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 80}: 0.5579349847986517 60%|██████ | 30/50 [24:42<18:18, 54.91s/it] {'n_estimators': 1000, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 10}: 0.5561109619053416 62%|██████▏ | 31/50 [25:56<19:06, 60.37s/it] {'n_estimators': 1400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}: 0.563590375919159 64%|██████▍ | 32/50 [26:54<17:53, 59.65s/it] {'n_estimators': 1000, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 40}: 0.5767932166794116 66%|██████▌ | 33/50 [27:36<15:28, 54.61s/it] {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 60}: 0.5688898451288212 68%|██████▊ | 34/50 [28:54<16:22, 61.43s/it] {'n_estimators': 1400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 60}: 0.5780482209527237 70%|███████ | 35/50 [29:54<15:15, 61.04s/it] {'n_estimators': 1000, 'min_samples_split': 30, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}: 0.5757106398854805 72%|███████▏ | 36/50 [30:34<12:46, 54.75s/it] {'n_estimators': 100, 'min_samples_split': 60, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}: 0.5734225525035258 74%|███████▍ | 37/50 [31:18<11:08, 51.43s/it] {'n_estimators': 400, 'min_samples_split': 60, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}: 0.5711771850119203 76%|███████▌ | 38/50 [32:05<10:03, 50.29s/it] {'n_estimators': 400, 'min_samples_split': 60, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30}: 0.5700398886557456 78%|███████▊ | 39/50 [32:58<09:21, 51.07s/it] {'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}: 0.570924543628796 80%|████████ | 40/50 [33:58<08:56, 53.64s/it] {'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 40}: 0.5645689246865137 82%|████████▏ | 41/50 [34:59<08:23, 55.96s/it] {'n_estimators': 1000, 'min_samples_split': 40, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10}: 0.5768068965699725 84%|████████▍ | 42/50 [35:42<06:57, 52.16s/it] {'n_estimators': 200, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 50}: 0.5539175158165466 86%|████████▌ | 43/50 [36:26<05:47, 49.58s/it] {'n_estimators': 400, 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10}: 0.5698419668348649 88%|████████▊ | 44/50 [37:25<05:14, 52.44s/it] {'n_estimators': 800, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100}: 0.5587813982594523 90%|█████████ | 45/50 [38:31<04:42, 56.40s/it] {'n_estimators': 1200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20}: 0.5678466277879061 92%|█████████▏| 46/50 [39:05<03:18, 49.64s/it] {'n_estimators': 100, 'min_samples_split': 30, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100}: 0.555132413137987 94%|█████████▍| 47/50 [40:04<02:37, 52.50s/it] {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10}: 0.5611334067161118 96%|█████████▌| 48/50 [41:41<02:11, 65.98s/it] {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 80}: 0.5435956017961139 98%|█████████▊| 49/50 [42:34<01:02, 62.12s/it] {'n_estimators': 800, 'min_samples_split': 50, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60}: 0.574612702982322 100%|██████████| 50/50 [43:52<00:00, 52.65s/it] {'n_estimators': 1000, 'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 20}: 0.5560972820147807 Starting parameter tuning for SVC 2%|▏ | 1/50 [00:38<31:02, 38.01s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 0.001}: 0.361496968991366 Best score improved from -inf to 0.361496968991366 4%|▍ | 2/50 [01:14<29:30, 36.89s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 1}: 0.5688744850195938 Best score improved from 0.361496968991366 to 0.5688744850195938 6%|▌ | 3/50 [01:48<27:57, 35.69s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 1}: 0.5676989471511694 8%|▊ | 4/50 [02:22<26:59, 35.22s/it] {'kernel': 'rbf', 'gamma': 0.1, 'C': 0.001}: 0.361496968991366 10%|█ | 5/50 [02:56<25:52, 34.51s/it] {'kernel': 'rbf', 'gamma': 100, 'C': 0.001}: 0.361496968991366 12%|█▏ | 6/50 [03:30<25:21, 34.58s/it] {'kernel': 'linear', 'gamma': 10, 'C': 0.001}: 0.5198892246386206 14%|█▍ | 7/50 [04:05<24:41, 34.45s/it] {'kernel': 'rbf', 'gamma': 100, 'C': 1}: 0.361496968991366 16%|█▌ | 8/50 [04:38<23:54, 34.16s/it] {'kernel': 'linear', 'gamma': 1, 'C': 1}: 0.5676989471511694 18%|█▊ | 9/50 [05:14<23:48, 34.84s/it] {'kernel': 'rbf', 'gamma': 10, 'C': 0.001}: 0.361496968991366 20%|██ | 10/50 [05:52<23:45, 35.63s/it] {'kernel': 'rbf', 'gamma': 10, 'C': 1}: 0.361496968991366 22%|██▏ | 11/50 [06:27<22:58, 35.35s/it] {'kernel': 'rbf', 'gamma': 1, 'C': 0.1}: 0.361496968991366 24%|██▍ | 12/50 [07:00<21:57, 34.66s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 0.01}: 0.5676843345407976 26%|██▌ | 13/50 [07:32<20:55, 33.93s/it] {'kernel': 'linear', 'gamma': 1, 'C': 0.001}: 0.5198892246386206 28%|██▊ | 14/50 [08:05<20:16, 33.79s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 100}: 0.4990890701114236 30%|███ | 15/50 [08:38<19:26, 33.33s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 0.001}: 0.5198892246386206 32%|███▏ | 16/50 [09:10<18:44, 33.08s/it] {'kernel': 'rbf', 'gamma': 0.1, 'C': 10}: 0.4497739973592784 34%|███▍ | 17/50 [09:44<18:20, 33.34s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 0.1}: 0.564236069399646 36%|███▌ | 18/50 [10:20<18:11, 34.12s/it] {'kernel': 'rbf', 'gamma': 1, 'C': 1}: 0.3671002534881075 38%|███▊ | 19/50 [10:52<17:21, 33.58s/it] {'kernel': 'rbf', 'gamma': 0.1, 'C': 0.01}: 0.361496968991366 40%|████ | 20/50 [11:24<16:30, 33.03s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 0.01}: 0.5676843345407976 42%|████▏ | 21/50 [11:57<15:55, 32.95s/it] {'kernel': 'rbf', 'gamma': 1, 'C': 100}: 0.36920148600126473 44%|████▍ | 22/50 [13:03<20:04, 43.01s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 100}: 0.5663382346326143 46%|████▌ | 23/50 [13:35<17:51, 39.70s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 0.1}: 0.564236069399646 48%|████▊ | 24/50 [14:09<16:25, 37.90s/it] {'kernel': 'rbf', 'gamma': 10, 'C': 10}: 0.361496968991366 50%|█████ | 25/50 [15:18<19:44, 47.37s/it] {'kernel': 'linear', 'gamma': 1, 'C': 100}: 0.5663382346326143 52%|█████▏ | 26/50 [15:56<17:44, 44.34s/it] {'kernel': 'rbf', 'gamma': 100, 'C': 100}: 0.361496968991366 54%|█████▍ | 27/50 [16:30<15:47, 41.21s/it] {'kernel': 'rbf', 'gamma': 10, 'C': 0.01}: 0.361496968991366 56%|█████▌ | 28/50 [17:05<14:28, 39.49s/it] {'kernel': 'rbf', 'gamma': 1, 'C': 0.001}: 0.361496968991366 58%|█████▊ | 29/50 [17:40<13:17, 37.99s/it] {'kernel': 'rbf', 'gamma': 0.1, 'C': 0.1}: 0.5212772969382976 60%|██████ | 30/50 [18:14<12:20, 37.01s/it] {'kernel': 'rbf', 'gamma': 1, 'C': 0.01}: 0.361496968991366 62%|██████▏ | 31/50 [18:48<11:24, 36.03s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 0.1}: 0.5363270419948826 64%|██████▍ | 32/50 [19:21<10:30, 35.04s/it] {'kernel': 'linear', 'gamma': 10, 'C': 1}: 0.5676989471511694 66%|██████▌ | 33/50 [19:54<09:45, 34.47s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 0.001}: 0.5198892246386206 68%|██████▊ | 34/50 [20:35<09:42, 36.38s/it] {'kernel': 'linear', 'gamma': 1, 'C': 10}: 0.5674873454397279 70%|███████ | 35/50 [21:13<09:12, 36.80s/it] {'kernel': 'linear', 'gamma': 0.1, 'C': 10}: 0.5674873454397279 72%|███████▏ | 36/50 [21:46<08:20, 35.76s/it] {'kernel': 'linear', 'gamma': 10, 'C': 0.1}: 0.564236069399646 74%|███████▍ | 37/50 [22:57<10:04, 46.53s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 100}: 0.5663382346326143 76%|███████▌ | 38/50 [23:33<08:38, 43.21s/it] {'kernel': 'rbf', 'gamma': 100, 'C': 0.1}: 0.361496968991366 78%|███████▊ | 39/50 [24:08<07:28, 40.79s/it] {'kernel': 'linear', 'gamma': 1, 'C': 0.01}: 0.5676843345407976 80%|████████ | 40/50 [24:42<06:28, 38.83s/it] {'kernel': 'linear', 'gamma': 100, 'C': 0.001}: 0.5198892246386206 82%|████████▏ | 41/50 [25:19<05:43, 38.18s/it] {'kernel': 'linear', 'gamma': 100, 'C': 1}: 0.5676989471511694 84%|████████▍ | 42/50 [26:34<06:34, 49.28s/it] {'kernel': 'linear', 'gamma': 100, 'C': 100}: 0.5663382346326143 86%|████████▌ | 43/50 [27:23<05:44, 49.21s/it] {'kernel': 'linear', 'gamma': 10, 'C': 10}: 0.5674873454397279 88%|████████▊ | 44/50 [28:10<04:50, 48.36s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 10}: 0.5674873454397279 90%|█████████ | 45/50 [28:53<03:54, 47.00s/it] {'kernel': 'linear', 'gamma': 0.01, 'C': 1}: 0.5676989471511694 92%|█████████▏| 46/50 [29:38<03:05, 46.26s/it] {'kernel': 'rbf', 'gamma': 100, 'C': 0.01}: 0.361496968991366 94%|█████████▍| 47/50 [30:28<02:21, 47.32s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 0.01}: 0.361496968991366 96%|█████████▌| 48/50 [31:13<01:33, 46.65s/it] {'kernel': 'rbf', 'gamma': 10, 'C': 0.1}: 0.361496968991366 98%|█████████▊| 49/50 [31:44<00:42, 42.04s/it] {'kernel': 'rbf', 'gamma': 0.1, 'C': 1}: 0.5628105030865749 100%|██████████| 50/50 [32:15<00:00, 38.71s/it] {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}: 0.5540097293921843 Starting parameter tuning for ADA 2%|▏ | 1/50 [00:36<30:12, 37.00s/it] {'n_estimators': 350, 'learning_rate': 0.0002566676263138247, 'algorithm': 'SAMME.R'}: 0.5267574624199912 Best score improved from -inf to 0.5267574624199912 4%|▍ | 2/50 [01:10<27:57, 34.94s/it] {'n_estimators': 200, 'learning_rate': 0.0004631073393465007, 'algorithm': 'SAMME.R'}: 0.5267574624199912 6%|▌ | 3/50 [01:40<25:39, 32.76s/it] {'n_estimators': 100, 'learning_rate': 0.41140322275580926, 'algorithm': 'SAMME.R'}: 0.540212944563368 Best score improved from 0.5267574624199912 to 0.540212944563368 8%|▊ | 4/50 [02:10<24:15, 31.64s/it] {'n_estimators': 100, 'learning_rate': 0.0002583137408526859, 'algorithm': 'SAMME.R'}: 0.5255809918317559 10%|█ | 5/50 [02:42<23:44, 31.65s/it] {'n_estimators': 200, 'learning_rate': 0.0013045486609227748, 'algorithm': 'SAMME'}: 0.5303772223207128 12%|█▏ | 6/50 [03:11<22:42, 30.97s/it] {'n_estimators': 50, 'learning_rate': 0.35193725330487063, 'algorithm': 'SAMME.R'}: 0.5682422928237464 Best score improved from 0.540212944563368 to 0.5682422928237464 14%|█▍ | 7/50 [03:51<24:18, 33.92s/it] {'n_estimators': 350, 'learning_rate': 0.41140322275580926, 'algorithm': 'SAMME.R'}: 0.5159321310729321 16%|█▌ | 8/50 [04:21<22:46, 32.55s/it] {'n_estimators': 100, 'learning_rate': 0.0004631073393465007, 'algorithm': 'SAMME'}: 0.527708651406224 18%|█▊ | 9/50 [04:50<21:25, 31.35s/it] {'n_estimators': 50, 'learning_rate': 0.002571530721027458, 'algorithm': 'SAMME.R'}: 0.521075644238173 20%|██ | 10/50 [05:18<20:17, 30.45s/it] {'n_estimators': 50, 'learning_rate': 0.41140322275580926, 'algorithm': 'SAMME.R'}: 0.5749682610662906 Best score improved from 0.5682422928237464 to 0.5749682610662906 22%|██▏ | 11/50 [05:45<19:09, 29.49s/it] {'n_estimators': 10, 'learning_rate': 0.00012299329779415905, 'algorithm': 'SAMME'}: 0.527708651406224 24%|██▍ | 12/50 [06:15<18:37, 29.40s/it] {'n_estimators': 50, 'learning_rate': 0.0002583137408526859, 'algorithm': 'SAMME.R'}: 0.5255809918317559 26%|██▌ | 13/50 [06:49<18:58, 30.77s/it] {'n_estimators': 200, 'learning_rate': 0.04384924314248015, 'algorithm': 'SAMME.R'}: 0.5684392819248162 28%|██▊ | 14/50 [07:20<18:38, 31.06s/it] {'n_estimators': 50, 'learning_rate': 0.0004894718551298529, 'algorithm': 'SAMME.R'}: 0.5255809918317559 30%|███ | 15/50 [07:57<19:06, 32.76s/it] {'n_estimators': 350, 'learning_rate': 0.0033287616164786207, 'algorithm': 'SAMME.R'}: 0.5591761305754815 32%|███▏ | 16/50 [08:30<18:39, 32.93s/it] {'n_estimators': 200, 'learning_rate': 0.01048695683585267, 'algorithm': 'SAMME.R'}: 0.5661820999822718 34%|███▍ | 17/50 [09:00<17:33, 31.92s/it] {'n_estimators': 100, 'learning_rate': 0.6311083515408057, 'algorithm': 'SAMME'}: 0.5662851952890373 36%|███▌ | 18/50 [09:35<17:35, 32.98s/it] {'n_estimators': 350, 'learning_rate': 0.6311083515408057, 'algorithm': 'SAMME'}: 0.5613823238350262 38%|███▊ | 19/50 [10:08<16:57, 32.82s/it] {'n_estimators': 200, 'learning_rate': 0.9174143833298315, 'algorithm': 'SAMME'}: 0.5556476515305866 40%|████ | 20/50 [10:41<16:29, 33.00s/it] {'n_estimators': 100, 'learning_rate': 0.011273213372422462, 'algorithm': 'SAMME.R'}: 0.5591761305754815 42%|████▏ | 21/50 [11:09<15:14, 31.53s/it] {'n_estimators': 10, 'learning_rate': 0.0033287616164786207, 'algorithm': 'SAMME.R'}: 0.5255809918317559 44%|████▍ | 22/50 [11:38<14:21, 30.75s/it] {'n_estimators': 50, 'learning_rate': 0.0004631073393465007, 'algorithm': 'SAMME.R'}: 0.5255809918317559 46%|████▌ | 23/50 [12:09<13:47, 30.65s/it] {'n_estimators': 100, 'learning_rate': 0.0018128823595036272, 'algorithm': 'SAMME.R'}: 0.5222247550452865 48%|████▊ | 24/50 [12:40<13:19, 30.76s/it] {'n_estimators': 100, 'learning_rate': 0.002571530721027458, 'algorithm': 'SAMME.R'}: 0.5269681314116219 50%|█████ | 25/50 [13:18<13:46, 33.06s/it] {'n_estimators': 350, 'learning_rate': 0.9174143833298315, 'algorithm': 'SAMME.R'}: 0.49350792613388295 52%|█████▏ | 26/50 [14:01<14:27, 36.16s/it] {'n_estimators': 500, 'learning_rate': 0.011273213372422462, 'algorithm': 'SAMME.R'}: 0.5765268954057264 Best score improved from 0.5749682610662906 to 0.5765268954057264 54%|█████▍ | 27/50 [14:32<13:12, 34.47s/it] {'n_estimators': 50, 'learning_rate': 0.033714929197471814, 'algorithm': 'SAMME.R'}: 0.5603672137740887 56%|█████▌ | 28/50 [15:05<12:31, 34.15s/it] {'n_estimators': 200, 'learning_rate': 0.033714929197471814, 'algorithm': 'SAMME.R'}: 0.5765670023575981 Best score improved from 0.5765268954057264 to 0.5765670023575981 58%|█████▊ | 29/50 [15:37<11:38, 33.26s/it] {'n_estimators': 100, 'learning_rate': 0.010087156447247654, 'algorithm': 'SAMME.R'}: 0.5579595530353746 60%|██████ | 30/50 [16:10<11:08, 33.44s/it] {'n_estimators': 200, 'learning_rate': 0.0033287616164786207, 'algorithm': 'SAMME.R'}: 0.55317345677869 62%|██████▏ | 31/50 [16:39<10:09, 32.08s/it] {'n_estimators': 10, 'learning_rate': 0.0013045486609227748, 'algorithm': 'SAMME'}: 0.527708651406224 64%|██████▍ | 32/50 [17:09<09:24, 31.34s/it] {'n_estimators': 50, 'learning_rate': 0.00014853289894720248, 'algorithm': 'SAMME'}: 0.527708651406224 66%|██████▌ | 33/50 [17:41<08:54, 31.46s/it] {'n_estimators': 50, 'learning_rate': 0.0002583137408526859, 'algorithm': 'SAMME'}: 0.527708651406224 68%|██████▊ | 34/50 [18:21<09:05, 34.09s/it] {'n_estimators': 500, 'learning_rate': 0.002571530721027458, 'algorithm': 'SAMME'}: 0.5658202046956158 70%|███████ | 35/50 [18:54<08:25, 33.67s/it] {'n_estimators': 200, 'learning_rate': 0.41140322275580926, 'algorithm': 'SAMME'}: 0.5672236371045202 72%|███████▏ | 36/50 [19:24<07:38, 32.77s/it] {'n_estimators': 100, 'learning_rate': 0.35193725330487063, 'algorithm': 'SAMME.R'}: 0.5599942515353495 74%|███████▍ | 37/50 [19:55<06:57, 32.08s/it] {'n_estimators': 100, 'learning_rate': 0.01048695683585267, 'algorithm': 'SAMME.R'}: 0.5579595530353746 76%|███████▌ | 38/50 [20:24<06:16, 31.34s/it] {'n_estimators': 50, 'learning_rate': 0.002571530721027458, 'algorithm': 'SAMME'}: 0.527708651406224 78%|███████▊ | 39/50 [20:55<05:41, 31.01s/it] {'n_estimators': 100, 'learning_rate': 0.011273213372422462, 'algorithm': 'SAMME'}: 0.5646838410592523 80%|████████ | 40/50 [21:31<05:26, 32.66s/it] {'n_estimators': 200, 'learning_rate': 0.0004894718551298529, 'algorithm': 'SAMME.R'}: 0.521075644238173 82%|████████▏ | 41/50 [22:12<05:16, 35.18s/it] {'n_estimators': 500, 'learning_rate': 0.033714929197471814, 'algorithm': 'SAMME.R'}: 0.5557005056532083 84%|████████▍ | 42/50 [22:49<04:46, 35.81s/it] {'n_estimators': 350, 'learning_rate': 0.00012299329779415905, 'algorithm': 'SAMME.R'}: 0.5255809918317559 86%|████████▌ | 43/50 [23:31<04:22, 37.43s/it] {'n_estimators': 500, 'learning_rate': 0.6311083515408057, 'algorithm': 'SAMME.R'}: 0.495369264434666 88%|████████▊ | 44/50 [24:06<03:41, 36.84s/it] {'n_estimators': 350, 'learning_rate': 0.010087156447247654, 'algorithm': 'SAMME'}: 0.5651351517621127 90%|█████████ | 45/50 [24:47<03:10, 38.15s/it] {'n_estimators': 500, 'learning_rate': 0.0013045486609227748, 'algorithm': 'SAMME'}: 0.5520252786913874 92%|█████████▏| 46/50 [25:18<02:23, 35.81s/it] {'n_estimators': 10, 'learning_rate': 0.011273213372422462, 'algorithm': 'SAMME.R'}: 0.521075644238173 94%|█████████▍| 47/50 [25:46<01:40, 33.49s/it] {'n_estimators': 10, 'learning_rate': 0.0002566676263138247, 'algorithm': 'SAMME'}: 0.527708651406224 96%|█████████▌| 48/50 [26:18<01:06, 33.25s/it] {'n_estimators': 200, 'learning_rate': 0.0004631073393465007, 'algorithm': 'SAMME'}: 0.527708651406224 98%|█████████▊| 49/50 [26:48<00:32, 32.26s/it] {'n_estimators': 100, 'learning_rate': 0.0002566676263138247, 'algorithm': 'SAMME'}: 0.527708651406224 100%|██████████| 50/50 [27:27<00:00, 32.94s/it] {'n_estimators': 350, 'learning_rate': 0.0013045486609227748, 'algorithm': 'SAMME.R'}: 0.5370921236111735 Starting parameter tuning for XGB 2%|▏ | 1/50 [01:57<1:35:44, 117.23s/it] {'subsample': 0.9632314809178825, 'n_estimators': 800, 'max_depth': 4, 'learning_rate': 0.13258719344064257}: 0.512143482736084 Best score improved from -inf to 0.512143482736084 4%|▍ | 2/50 [02:46<1:01:52, 77.35s/it] {'subsample': 0.9680757820272262, 'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.5699577726079624}: 0.5333282221169696 Best score improved from 0.512143482736084 to 0.5333282221169696 6%|▌ | 3/50 [04:09<1:02:38, 79.98s/it] {'subsample': 0.8954075750658848, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.02684032016696159}: 0.5368295266016981 Best score improved from 0.5333282221169696 to 0.5368295266016981 8%|▊ | 4/50 [05:16<57:25, 74.91s/it] {'subsample': 0.7124569894027368, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.0018049786945696254}: 0.5482613771971835 Best score improved from 0.5368295266016981 to 0.5482613771971835 10%|█ | 5/50 [08:02<1:20:49, 107.78s/it] {'subsample': 0.9632314809178825, 'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.06815088175164626}: 0.5137669708700358 12%|█▏ | 6/50 [10:05<1:22:40, 112.74s/it] {'subsample': 0.7033756830063296, 'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.12704978870153852}: 0.5303678355872959 14%|█▍ | 7/50 [11:07<1:08:54, 96.15s/it] {'subsample': 0.614948794269934, 'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.5699577726079624}: 0.4984737859427879 16%|█▌ | 8/50 [13:01<1:11:14, 101.77s/it] {'subsample': 0.9905700183070416, 'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.07621013454002182}: 0.5158835634924206 18%|█▊ | 9/50 [13:31<54:14, 79.38s/it] {'subsample': 0.6304517747887591, 'n_estimators': 10, 'max_depth': 5, 'learning_rate': 0.0810754044404809}: 0.5468723721776957 20%|██ | 10/50 [14:22<47:04, 70.60s/it] {'subsample': 0.9607398473685813, 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.033824740502579095}: 0.5406252596400892 22%|██▏ | 11/50 [16:22<55:50, 85.92s/it] {'subsample': 0.6419845479822909, 'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.002550894270850008}: 0.5481939104641901 24%|██▍ | 12/50 [17:57<56:07, 88.62s/it] {'subsample': 0.618698066272185, 'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.08440189413042316}: 0.5184454934418551 26%|██▌ | 13/50 [18:42<46:32, 75.48s/it] {'subsample': 0.614948794269934, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.005853038263058632}: 0.5427109468230638 28%|██▊ | 14/50 [19:12<36:57, 61.59s/it] {'subsample': 0.6304517747887591, 'n_estimators': 10, 'max_depth': 4, 'learning_rate': 0.020229388086296835}: 0.5389588663948328 30%|███ | 15/50 [20:37<39:59, 68.55s/it] {'subsample': 0.6666043034111473, 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.033824740502579095}: 0.5289270877709188 32%|███▏ | 16/50 [21:37<37:26, 66.07s/it] {'subsample': 0.8781386465483189, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.7700871364368529}: 0.5132260595300151 34%|███▍ | 17/50 [22:15<31:42, 57.65s/it] {'subsample': 0.6233721378220499, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.8790403625996874}: 0.43932088075210285 36%|███▌ | 18/50 [23:05<29:30, 55.31s/it] {'subsample': 0.6233721378220499, 'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.004141817329684559}: 0.5461063578415938 38%|███▊ | 19/50 [23:46<26:23, 51.10s/it] {'subsample': 0.8984529939118093, 'n_estimators': 50, 'max_depth': 8, 'learning_rate': 0.06815088175164626}: 0.5300250246740772 40%|████ | 20/50 [25:07<29:56, 59.89s/it] {'subsample': 0.9905700183070416, 'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.12704978870153852}: 0.5314505909871483 42%|████▏ | 21/50 [25:48<26:16, 54.37s/it] {'subsample': 0.614948794269934, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.02684032016696159}: 0.5560597880013864 Best score improved from 0.5482613771971835 to 0.5560597880013864 44%|████▍ | 22/50 [26:20<22:10, 47.52s/it] {'subsample': 0.9680757820272262, 'n_estimators': 10, 'max_depth': 4, 'learning_rate': 0.033824740502579095}: 0.5471514406221306 46%|████▌ | 23/50 [27:01<20:30, 45.59s/it] {'subsample': 0.7959361211849242, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.7700871364368529}: 0.49027013814837267 48%|████▊ | 24/50 [27:59<21:26, 49.49s/it] {'subsample': 0.9905700183070416, 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.5699577726079624}: 0.5058688251964003 50%|█████ | 25/50 [29:23<24:52, 59.69s/it] {'subsample': 0.9607398473685813, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.230900275766295}: 0.5208267271192585 52%|█████▏ | 26/50 [32:06<36:17, 90.73s/it] {'subsample': 0.9504211163253288, 'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.08947714369255426}: 0.5125128993165348 54%|█████▍ | 27/50 [32:48<29:10, 76.13s/it] {'subsample': 0.6971076613790066, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.005853038263058632}: 0.5483927650048818 56%|█████▌ | 28/50 [33:17<22:43, 61.99s/it] {'subsample': 0.8781386465483189, 'n_estimators': 10, 'max_depth': 5, 'learning_rate': 0.00118106586876449}: 0.3636246285658341 58%|█████▊ | 29/50 [33:47<18:20, 52.42s/it] {'subsample': 0.6666043034111473, 'n_estimators': 10, 'max_depth': 8, 'learning_rate': 0.020229388086296835}: 0.5281974561224787 60%|██████ | 30/50 [35:19<21:23, 64.19s/it] {'subsample': 0.6666043034111473, 'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.7700871364368529}: 0.4839994694742636 62%|██████▏ | 31/50 [37:37<27:22, 86.45s/it] {'subsample': 0.9607398473685813, 'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.020229388086296835}: 0.5264828657386215 64%|██████▍ | 32/50 [40:06<31:33, 105.20s/it] {'subsample': 0.614948794269934, 'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.005703062636041717}: 0.5379121033956293 66%|██████▌ | 33/50 [42:03<30:49, 108.82s/it] {'subsample': 0.8984529939118093, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.13258719344064257}: 0.5275015214578478 68%|██████▊ | 34/50 [45:14<35:35, 133.48s/it] {'subsample': 0.614948794269934, 'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.005703062636041717}: 0.5311988823238349 70%|███████ | 35/50 [47:17<32:35, 130.37s/it] {'subsample': 0.7124569894027368, 'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.020229388086296835}: 0.5309634664895603 72%|███████▏ | 36/50 [49:10<29:09, 124.94s/it] {'subsample': 0.6419845479822909, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.004141817329684559}: 0.5517617489621011 74%|███████▍ | 37/50 [49:45<21:13, 97.97s/it] {'subsample': 0.8781386465483189, 'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.005703062636041717}: 0.5458308350554473 76%|███████▌ | 38/50 [51:23<19:35, 97.96s/it] {'subsample': 0.682857925902227, 'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.037536855386975675}: 0.5356275616719631 78%|███████▊ | 39/50 [52:49<17:18, 94.42s/it] {'subsample': 0.6666043034111473, 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.00118106586876449}: 0.5402914716334106 80%|████████ | 40/50 [55:00<17:34, 105.49s/it] {'subsample': 0.6304517747887591, 'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.13258719344064257}: 0.5296957150454983 82%|████████▏ | 41/50 [56:26<14:56, 99.58s/it] {'subsample': 0.618698066272185, 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.002550894270850008}: 0.5461063578415938 84%|████████▍ | 42/50 [57:24<11:38, 87.26s/it] {'subsample': 0.9680757820272262, 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.7458423280793531}: 0.5119747795209127 86%|████████▌ | 43/50 [58:38<09:41, 83.08s/it] {'subsample': 0.614948794269934, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.08440189413042316}: 0.5130146364244947 88%|████████▊ | 44/50 [1:00:34<09:18, 93.09s/it] {'subsample': 0.6304517747887591, 'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.02684032016696159}: 0.5468742376173176 90%|█████████ | 45/50 [1:01:24<06:39, 79.99s/it] {'subsample': 0.7959361211849242, 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.08947714369255426}: 0.535759696978517 92%|█████████▏| 46/50 [1:02:19<04:50, 72.66s/it] {'subsample': 0.6233721378220499, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.0810754044404809}: 0.5534277783804809 94%|█████████▍| 47/50 [1:04:15<04:17, 85.73s/it] {'subsample': 0.8984529939118093, 'n_estimators': 800, 'max_depth': 4, 'learning_rate': 0.004141817329684559}: 0.5610293786895353 Best score improved from 0.5560597880013864 to 0.5610293786895353 96%|█████████▌| 48/50 [1:05:51<02:57, 88.60s/it] {'subsample': 0.6419845479822909, 'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.033824740502579095}: 0.5253592492730077 98%|█████████▊| 49/50 [1:07:48<01:37, 97.17s/it] {'subsample': 0.614948794269934, 'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.037536855386975675}: 0.5403716855371539 100%|██████████| 50/50 [1:08:27<00:00, 82.15s/it] {'subsample': 0.9680757820272262, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.230900275766295}: 0.514031426704099 In [150]: best_param Out[150]: {'RF': {'n_estimators': 1000, 'min_samples_split': 40, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}, 'SVC': {'kernel': 'rbf', 'gamma': 0.01, 'C': 1}, 'ADA': {'n_estimators': 200, 'learning_rate': 0.033714929197471814, 'algorithm': 'SAMME.R'}, 'XGB': {'subsample': 0.8984529939118093, 'n_estimators': 800, 'max_depth': 4, 'learning_rate': 0.004141817329684559}} In [151]: best_score Out[151]: {'RF': 0.5791461578558822, 'SVC': 0.5688744850195938, 'ADA': 0.5765670023575981, 'XGB': 0.5610293786895353} As we can observe, Random Forest performs the best on the validation dataset after hyperparameter tuning. Let's build the final model using Random Forest algorithm with these hypertuned parameter values and evaluate the model on the test data after preprocessing In [152]: processed_train_data = preprocessor . fit_transform ( train_data ) processed_test_data = preprocessor . transform ( test_data ) X_train = processed_train_data . drop ( \"result\" , axis = 1 ) y_train = processed_train_data [ \"result\" ] X_test = processed_test_data . drop ( \"result\" , axis = 1 ) y_test = processed_test_data [ \"result\" ] In [153]: X_train . head () Out[153]: xg xga attendance hour match_week day_sin day_cos team_1 team_2 team_3 team_4 num_striker center_midfielder gf_rolling ga_rolling sh_rolling sot_rolling dist_rolling poss_rolling fk_rolling 0 2.4 0.4 59378.0 12.0 7.0 -0.781831 0.623490 0.0 1.0 0.0 0.0 3.0 4.0 1.333333 2.333333 15.333333 4.000000 17.933333 56.333333 0.666667 1 3.5 1.0 39189.0 13.0 9.0 -0.781831 0.623490 0.0 1.0 0.0 0.0 3.0 4.0 0.666667 1.333333 14.666667 3.333333 17.500000 55.000000 0.666667 2 0.3 1.8 54286.0 14.0 11.0 -0.781831 0.623490 0.0 1.0 0.0 0.0 3.0 4.0 2.333333 0.666667 22.000000 8.000000 17.500000 60.000000 0.333333 3 2.1 0.7 59530.0 12.0 12.0 -0.974928 -0.222521 0.0 1.0 0.0 0.0 3.0 4.0 2.666667 1.666667 20.333333 8.333333 18.500000 58.000000 0.333333 4 1.8 0.4 21722.0 14.0 13.0 -0.781831 0.623490 0.0 1.0 0.0 0.0 3.0 4.0 2.666667 1.666667 16.666667 7.333333 17.833333 51.000000 0.333333 In [154]: classifier = RandomForestClassifier ( ** best_param [ \"RF\" ]) best_model = Pipeline ([ ( 'standardizer' , RobustScaler ( unit_variance = True )), ( 'RF' , classifier ) ]) best_model . fit ( X_train , y_train ) best_model_results = best_model . predict ( X_test ) score = precision_score ( y_test , best_model_results , average = 'micro' ) print ( f \"Precision score: { score } \" ) Precision score: 0.546583850931677 In [156]: accuracy = accuracy_score ( y_test , best_model_results ) print ( f \"Accuracy score: { score } \" ) Accuracy score: 0.546583850931677 Conclusions Our best performing results seem to be on par with related work that had used multiple seasons' data, such as this one . We exceed their best performing model, which managed to have an accuracy of 0.52. Data extraction, cleaning, dealing with missing data, and feature engineering took a lot of our time and was definitely the most challenging and cumbersome aspect of the project. Using Random forest, we were able to get a precision score of 0.55. As observed from the precision score, it is very difficult to predict the winner of a EPL match. There are other factors like player transfers, managerial changes before a season's beginning, player morale etc that play a role but not accounted for by us.","tags":"Machine Learning","url":"https://apurba-saha.github.io/predicting-the-winner-for-english-premier-league-football-matches","loc":"https://apurba-saha.github.io/predicting-the-winner-for-english-premier-league-football-matches"},{"title":"Web scrapping data for English Premier League football matches","text":"In this project, we're going to complete a machine learning project on the English Premier League (EPL) football matches. The final goal of the project is to predict the winner of each football match. At first, we're going to use web scraping to get the necessary data on the EPL match results from this page . Let's download the HTML for that page and then explore it in the web browser's inspector. We want to extract the first table — League Table — that lists every team in the league and its stats. In particular, we need to fetch the URL for each team to be able to grab the match log for the season from each of them. In [ ]: import requests In [ ]: URL = \"https://fbref.com/en/comps/9/2021-2022/2021-2022-Premier-League-Stats\" response = requests . get ( URL ) if response . status_code == 200 : content = response . content else : print ( \"Couldn't download the web page\" ) Let's explore the page first in the web browser's inspector to identify which HTML tag is associated with the URLs of the teams. After some exploration, we identify the id of the 'League Table' In [ ]: from bs4 import BeautifulSoup parser = BeautifulSoup ( content , 'html.parser' ) league_table = parser . select ( '#results2021-202291_overall' )[ 0 ] We notice that the table rows that contain the the URL for each team statistics have a special attribute 'data-stat' = 'team' . We use this information to select the desired rows and finally scrap the URL from those rows In [ ]: team_data = league_table . find_all ( \"td\" , attrs = { 'data-stat' : 'team' }) team_stat_URL = {} for team in team_data : # The table only contains partial URL. We add the domain name to get the full URL URL = \"https://fbref.com\" + team . select ( 'a' )[ 0 ][ 'href' ] team_name = team . select ( 'a' )[ 0 ] . text team_stat_URL [ team_name ] = URL print ( team_stat_URL ) {'Manchester City': 'https://fbref.com/en/squads/b8fd03ef/2021-2022/Manchester-City-Stats', 'Liverpool': 'https://fbref.com/en/squads/822bd0ba/2021-2022/Liverpool-Stats', 'Chelsea': 'https://fbref.com/en/squads/cff3d9bb/2021-2022/Chelsea-Stats', 'Tottenham': 'https://fbref.com/en/squads/361ca564/2021-2022/Tottenham-Hotspur-Stats', 'Arsenal': 'https://fbref.com/en/squads/18bb7c10/2021-2022/Arsenal-Stats', 'Manchester Utd': 'https://fbref.com/en/squads/19538871/2021-2022/Manchester-United-Stats', 'West Ham': 'https://fbref.com/en/squads/7c21e445/2021-2022/West-Ham-United-Stats', 'Leicester City': 'https://fbref.com/en/squads/a2d435b3/2021-2022/Leicester-City-Stats', 'Brighton': 'https://fbref.com/en/squads/d07537b9/2021-2022/Brighton-and-Hove-Albion-Stats', 'Wolves': 'https://fbref.com/en/squads/8cec06e1/2021-2022/Wolverhampton-Wanderers-Stats', 'Newcastle Utd': 'https://fbref.com/en/squads/b2b47a98/2021-2022/Newcastle-United-Stats', 'Crystal Palace': 'https://fbref.com/en/squads/47c64c55/2021-2022/Crystal-Palace-Stats', 'Brentford': 'https://fbref.com/en/squads/cd051869/2021-2022/Brentford-Stats', 'Aston Villa': 'https://fbref.com/en/squads/8602292d/2021-2022/Aston-Villa-Stats', 'Southampton': 'https://fbref.com/en/squads/33c895d4/2021-2022/Southampton-Stats', 'Everton': 'https://fbref.com/en/squads/d3fd31cc/2021-2022/Everton-Stats', 'Leeds United': 'https://fbref.com/en/squads/5bfb9659/2021-2022/Leeds-United-Stats', 'Burnley': 'https://fbref.com/en/squads/943e8050/2021-2022/Burnley-Stats', 'Watford': 'https://fbref.com/en/squads/2abfe087/2021-2022/Watford-Stats', 'Norwich City': 'https://fbref.com/en/squads/1c781004/2021-2022/Norwich-City-Stats'} Now that we have a list of the URLs, one for each team, we can get the stats we want. Let's start with the first team: Manchester City. After exploring the web page for the team, we decide to parse the table named \"Scores & Fixture\" for our analysis. The parsed table is read into a pandas dataframe for our convenience. In [ ]: import pandas as pd link = team_stat_URL [ 'Manchester City' ] response_MC = requests . get ( link ) if response . status_code == 200 : content_MC = response_MC . content score_tables = pd . read_html ( content_MC , match = \"Scores & Fixtures\" ) score_df = score_tables [ 0 ] else : print ( \"The page couldn't be downloaded for team {} \" . format ( \"Manchester City\" )) In [ ]: score_df . head () Out[ ]: Date Time Comp Round Day Venue Result GF GA Opponent xG xGA Poss Attendance Captain Formation Referee Match Report Notes 0 2021-08-07 17:15 Community Shield FA Community Shield Sat Neutral L 0 1 Leicester City NaN NaN 57 NaN Fernandinho 4-3-3 Paul Tierney Match Report NaN 1 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham 2.0 1.0 65 58262.0 Fernandinho 4-3-3 Anthony Taylor Match Report NaN 2 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City 2.7 0.1 67 51437.0 İlkay Gündoğan 4-3-3 Graham Scott Match Report NaN 3 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal 4.0 0.2 80 52276.0 İlkay Gündoğan 4-3-3 Martin Atkinson Match Report NaN 4 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City 3.3 0.6 61 32087.0 İlkay Gündoğan 4-3-3 Paul Tierney Match Report NaN In [ ]: score_df . tail () Out[ ]: Date Time Comp Round Day Venue Result GF GA Opponent xG xGA Poss Attendance Captain Formation Referee Match Report Notes 53 2022-05-04 21:00 Champions Lg Semi-finals Wed Away L 1 3 es Real Madrid 1.4 2.3 55 61416.0 Rúben Dias 4-3-3 Daniele Orsato Match Report Leg 2 of 2; Real Madrid won; Required Extra Time 54 2022-05-08 16:30 Premier League Matchweek 36 Sun Home W 5 0 Newcastle Utd 3.3 0.8 71 53336.0 İlkay Gündoğan 4-2-3-1 Stuart Attwell Match Report NaN 55 2022-05-11 20:15 Premier League Matchweek 33 Wed Away W 5 1 Wolves 2.8 0.5 66 32000.0 Fernandinho 4-2-3-1 Martin Atkinson Match Report NaN 56 2022-05-15 14:00 Premier League Matchweek 37 Sun Away D 2 2 West Ham 2.9 1.8 78 59972.0 Fernandinho 4-3-3 Anthony Taylor Match Report NaN 57 2022-05-22 16:00 Premier League Matchweek 38 Sun Home W 3 2 Aston Villa 3.7 0.3 71 53395.0 Fernandinho 4-3-3 Michael Oliver Match Report NaN Here is a brief description of some columns in the table that can't be interpreted easily from their name Comp : Competition Round: Phase of competition GF : Goal for the team GA : Goal against the team xG : Expected goals xGA : Expected goals allowed Poss : Possession as a percentage of passes attempted As we can observe, there is something we don't have in the table with scores and fixtures: the details about each match, such as the number of shots, the number of shots on target, the number of free kicks, and the number of penalty kicks. We can find some of these stats in the table under the Shooting tab. Let's find and download the table containing the shooting stats for the Manchester City team and read it in a pandas DataFrame. In [ ]: MC_parser = BeautifulSoup ( content_MC , 'html.parser' ) parsed_links = MC_parser . select ( \".filter a\" ) # After exploring the web page, we find that the desired URL can be found inside the body of <div> tag with class=\"filter\" shooting_tab_link = [ \"https://fbref.com\" + link [ 'href' ] for link in parsed_links if link . text == \"Shooting\" ] print ( shooting_tab_link ) ['https://fbref.com/en/squads/b8fd03ef/2021-2022/matchlogs/all_comps/shooting/Manchester-City-Match-Logs-All-Competitions'] In [ ]: response_shooting = requests . get ( * shooting_tab_link ) if response_shooting . status_code == 200 : shooting_html = response_shooting . content shooting_tables = pd . read_html ( shooting_html , match = \"Shooting \" ) shooting_df = shooting_tables [ 0 ] else : print ( \"Couldn't download the shooting page\" ) In [ ]: shooting_df . head () Out[ ]: For Manchester City ... Standard Expected Unnamed: 25_level_0 Date Time Comp Round Day Venue Result GF GA Opponent ... Dist FK PK PKatt xG npxG npxG/Sh G-xG np:G-xG Match Report 0 2021-08-07 17:15 Community Shield FA Community Shield Sat Neutral L 0 1 Leicester City ... NaN NaN 0 0 NaN NaN NaN NaN NaN Match Report 1 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham ... 17.3 1.0 0 0 2.0 2.0 0.11 -2.0 -2.0 Match Report 2 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City ... 18.5 1.0 0 0 2.7 2.7 0.17 1.3 1.3 Match Report 3 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal ... 14.8 0.0 0 0 4.0 4.0 0.16 1.0 1.0 Match Report 4 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City ... 14.3 0.0 0 0 3.3 3.3 0.14 -2.3 -2.3 Match Report 5 rows × 26 columns The dataframe has multi-level index, which is not important for our purpose. So, we can drop the multi-level index. After that we have two DataFrames: the matches and shootings. Since both refer to the same matches, we can combine these DataFrames. In [ ]: shooting_df . columns = shooting_df . columns . droplevel () shooting_df . head () Out[ ]: Date Time Comp Round Day Venue Result GF GA Opponent ... Dist FK PK PKatt xG npxG npxG/Sh G-xG np:G-xG Match Report 0 2021-08-07 17:15 Community Shield FA Community Shield Sat Neutral L 0 1 Leicester City ... NaN NaN 0 0 NaN NaN NaN NaN NaN Match Report 1 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham ... 17.3 1.0 0 0 2.0 2.0 0.11 -2.0 -2.0 Match Report 2 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City ... 18.5 1.0 0 0 2.7 2.7 0.17 1.3 1.3 Match Report 3 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal ... 14.8 0.0 0 0 4.0 4.0 0.16 1.0 1.0 Match Report 4 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City ... 14.3 0.0 0 0 3.3 3.3 0.14 -2.3 -2.3 Match Report 5 rows × 26 columns Both score and shooting dataframe have multiple common columns. The unique columns in the shooting dataframe are listed below: Sh : Shots Total (Does not include penalty kicks) SoT : Shots on Target (Without penalty kicks) Dist : Average distance travelled by a shot FK : Number of free kicks PK : Pealty kicks mades PKatt: Penalty kicks attempted These unique columns are merged with the score dataframe In [ ]: team_data = score_df . merge ( shooting_df [[ \"Date\" , \"Sh\" , \"SoT\" , \"Dist\" , \"FK\" , \"PK\" , \"PKatt\" ]], on = \"Date\" ) In [ ]: team_data . head () Out[ ]: Date Time Comp Round Day Venue Result GF GA Opponent ... Formation Referee Match Report Notes Sh SoT Dist FK PK PKatt 0 2021-08-07 17:15 Community Shield FA Community Shield Sat Neutral L 0 1 Leicester City ... 4-3-3 Paul Tierney Match Report NaN 12 3 NaN NaN 0 0 1 2021-08-15 16:30 Premier League Matchweek 1 Sun Away L 0 1 Tottenham ... 4-3-3 Anthony Taylor Match Report NaN 18 4 17.3 1.0 0 0 2 2021-08-21 15:00 Premier League Matchweek 2 Sat Home W 5 0 Norwich City ... 4-3-3 Graham Scott Match Report NaN 16 4 18.5 1.0 0 0 3 2021-08-28 12:30 Premier League Matchweek 3 Sat Home W 5 0 Arsenal ... 4-3-3 Martin Atkinson Match Report NaN 25 10 14.8 0.0 0 0 4 2021-09-11 15:00 Premier League Matchweek 4 Sat Away W 1 0 Leicester City ... 4-3-3 Paul Tierney Match Report NaN 25 8 14.3 0.0 0 0 5 rows × 25 columns Now let's repeat these steps for each team who played last 5 seasons of EPL In [ ]: import time import re import pandas as pd years = list ( range ( 2022 , 2017 , - 1 )) all_matches = [] for year in years : for team in team_stat_URL : link = team_stat_URL [ team ] print ( link ) response_status_code = 401 while response_status_code != 200 : response = requests . get ( link ) if response . status_code == 200 : content = response . content score_tables = pd . read_html ( content , match = \"Scores & Fixtures\" ) score_df = score_tables [ 0 ] response_status_code = 200 else : print ( \"The page couldn't be downloaded for team {} . Trying again\" . format ( team )) time . sleep ( 1 ) parser = BeautifulSoup ( content , 'html.parser' ) parsed_links = parser . select ( \".filter a\" ) # After exploring the web page, we find that the desired URL can be found inside the body of <div> tag with class=\"filter\" shooting_tab_link = [ \"https://fbref.com\" + link [ 'href' ] for link in parsed_links if link . text == \"Shooting\" ] response_shooting_status_code = 401 while response_shooting_status_code != 200 : response_shooting = requests . get ( * shooting_tab_link ) if response_shooting . status_code == 200 : shooting_html = response_shooting . content shooting_tables = pd . read_html ( shooting_html , match = \"Shooting \" ) shooting_df = shooting_tables [ 0 ] response_shooting_status_code = 200 else : print ( \"Couldn't download the shooting page for team {} . Trying again\" . format ( team )) time . sleep ( 1 ) shooting_df . columns = shooting_df . columns . droplevel () try : team_data = score_df . merge ( shooting_df [[ \"Date\" , \"Sh\" , \"SoT\" , \"Dist\" , \"FK\" , \"PK\" , \"PKatt\" ]], on = \"Date\" ) print ( f \" { team } : { year } : { team_data . shape } \" ) except ValueError : continue except KeyError as e : print ( f \"Column { e . args [ 0 ] } missing from the dataframe. So adding an extra column for consistency\" ) team_data [ 'FK' ] = None # Our goal is to predict winners for EPL match. So, ignore any data not within our scope team_data = team_data [ team_data [ \"Comp\" ] == \"Premier League\" ] # Adding extra columns to keep track of the team name and season team_data [ \"Season\" ] = year team_data [ \"Team\" ] = team all_matches . append ( team_data ) time . sleep ( 1 ) # Update the links in team_stat_URL for scrapping data for previous season season_links = parser . select ( \".prevnext a\" ) prev_season_link = \"https://fbref.com\" + season_links [ 0 ][ 'href' ] team_stat_URL [ team ] = prev_season_link time . sleep ( 300 ) https://fbref.com/en/squads/b8fd03ef/2021-2022/Manchester-City-Stats Manchester City : 2022 : (58, 25) https://fbref.com/en/squads/822bd0ba/2021-2022/Liverpool-Stats Liverpool : 2022 : (63, 25) https://fbref.com/en/squads/cff3d9bb/2021-2022/Chelsea-Stats Chelsea : 2022 : (61, 25) https://fbref.com/en/squads/361ca564/2021-2022/Tottenham-Hotspur-Stats Tottenham : 2022 : (54, 25) https://fbref.com/en/squads/18bb7c10/2021-2022/Arsenal-Stats Arsenal : 2022 : (45, 25) https://fbref.com/en/squads/19538871/2021-2022/Manchester-United-Stats Manchester Utd : 2022 : (49, 25) https://fbref.com/en/squads/7c21e445/2021-2022/West-Ham-United-Stats West Ham : 2022 : (56, 25) https://fbref.com/en/squads/a2d435b3/2021-2022/Leicester-City-Stats Leicester City : 2022 : (58, 25) https://fbref.com/en/squads/d07537b9/2021-2022/Brighton-and-Hove-Albion-Stats Brighton : 2022 : (43, 25) https://fbref.com/en/squads/8cec06e1/2021-2022/Wolverhampton-Wanderers-Stats Wolves : 2022 : (42, 25) https://fbref.com/en/squads/b2b47a98/2021-2022/Newcastle-United-Stats Newcastle Utd : 2022 : (40, 25) https://fbref.com/en/squads/47c64c55/2021-2022/Crystal-Palace-Stats Crystal Palace : 2022 : (44, 25) https://fbref.com/en/squads/cd051869/2021-2022/Brentford-Stats Brentford : 2022 : (44, 25) https://fbref.com/en/squads/8602292d/2021-2022/Aston-Villa-Stats Aston Villa : 2022 : (41, 25) https://fbref.com/en/squads/33c895d4/2021-2022/Southampton-Stats Southampton : 2022 : (45, 25) https://fbref.com/en/squads/d3fd31cc/2021-2022/Everton-Stats Everton : 2022 : (44, 25) https://fbref.com/en/squads/5bfb9659/2021-2022/Leeds-United-Stats Leeds United : 2022 : (42, 25) https://fbref.com/en/squads/943e8050/2021-2022/Burnley-Stats Burnley : 2022 : (42, 25) https://fbref.com/en/squads/2abfe087/2021-2022/Watford-Stats Watford : 2022 : (41, 25) https://fbref.com/en/squads/1c781004/2021-2022/Norwich-City-Stats Norwich City : 2022 : (43, 25) https://fbref.com/en/squads/b8fd03ef/2020-2021/Manchester-City-Stats Manchester City : 2021 : (61, 25) https://fbref.com/en/squads/822bd0ba/2020-2021/Liverpool-Stats Liverpool : 2021 : (53, 25) https://fbref.com/en/squads/cff3d9bb/2020-2021/Chelsea-Stats Chelsea : 2021 : (59, 25) https://fbref.com/en/squads/361ca564/2020-2021/Tottenham-Hotspur-Stats Tottenham : 2021 : (59, 25) https://fbref.com/en/squads/18bb7c10/2020-2021/Arsenal-Stats Arsenal : 2021 : (58, 25) https://fbref.com/en/squads/19538871/2020-2021/Manchester-United-Stats Manchester Utd : 2021 : (61, 25) https://fbref.com/en/squads/7c21e445/2020-2021/West-Ham-United-Stats West Ham : 2021 : (44, 25) https://fbref.com/en/squads/a2d435b3/2020-2021/Leicester-City-Stats Leicester City : 2021 : (53, 25) https://fbref.com/en/squads/d07537b9/2020-2021/Brighton-and-Hove-Albion-Stats Brighton : 2021 : (44, 25) https://fbref.com/en/squads/8cec06e1/2020-2021/Wolverhampton-Wanderers-Stats Wolves : 2021 : (42, 25) https://fbref.com/en/squads/b2b47a98/2020-2021/Newcastle-United-Stats Newcastle Utd : 2021 : (43, 25) https://fbref.com/en/squads/47c64c55/2020-2021/Crystal-Palace-Stats Crystal Palace : 2021 : (40, 25) https://fbref.com/en/squads/cd051869/2020-2021/Brentford-Stats Brentford : 2021 : (57, 25) https://fbref.com/en/squads/8602292d/2020-2021/Aston-Villa-Stats Aston Villa : 2021 : (42, 25) https://fbref.com/en/squads/33c895d4/2020-2021/Southampton-Stats Southampton : 2021 : (44, 25) https://fbref.com/en/squads/d3fd31cc/2020-2021/Everton-Stats Everton : 2021 : (46, 25) https://fbref.com/en/squads/5bfb9659/2020-2021/Leeds-United-Stats Leeds United : 2021 : (40, 25) https://fbref.com/en/squads/943e8050/2020-2021/Burnley-Stats Burnley : 2021 : (44, 25) https://fbref.com/en/squads/2abfe087/2020-2021/Watford-Stats Watford : 2021 : (49, 25) https://fbref.com/en/squads/1c781004/2020-2021/Norwich-City-Stats Norwich City : 2021 : (49, 25) https://fbref.com/en/squads/b8fd03ef/2019-2020/Manchester-City-Stats Manchester City : 2020 : (59, 25) https://fbref.com/en/squads/822bd0ba/2019-2020/Liverpool-Stats Liverpool : 2020 : (55, 25) https://fbref.com/en/squads/cff3d9bb/2019-2020/Chelsea-Stats Chelsea : 2020 : (55, 25) https://fbref.com/en/squads/361ca564/2019-2020/Tottenham-Hotspur-Stats Tottenham : 2020 : (52, 25) https://fbref.com/en/squads/18bb7c10/2019-2020/Arsenal-Stats Arsenal : 2020 : (54, 25) https://fbref.com/en/squads/19538871/2019-2020/Manchester-United-Stats Manchester Utd : 2020 : (61, 25) https://fbref.com/en/squads/7c21e445/2019-2020/West-Ham-United-Stats West Ham : 2020 : (42, 25) https://fbref.com/en/squads/a2d435b3/2019-2020/Leicester-City-Stats Leicester City : 2020 : (48, 25) https://fbref.com/en/squads/d07537b9/2019-2020/Brighton-and-Hove-Albion-Stats Brighton : 2020 : (41, 25) https://fbref.com/en/squads/8cec06e1/2019-2020/Wolverhampton-Wanderers-Stats Wolves : 2020 : (59, 25) https://fbref.com/en/squads/b2b47a98/2019-2020/Newcastle-United-Stats Newcastle Utd : 2020 : (45, 25) https://fbref.com/en/squads/47c64c55/2019-2020/Crystal-Palace-Stats Crystal Palace : 2020 : (40, 25) https://fbref.com/en/squads/cd051869/2019-2020/Brentford-Stats Brentford : 2020 : (52, 25) https://fbref.com/en/squads/8602292d/2019-2020/Aston-Villa-Stats Aston Villa : 2020 : (46, 25) https://fbref.com/en/squads/33c895d4/2019-2020/Southampton-Stats Southampton : 2020 : (44, 25) https://fbref.com/en/squads/d3fd31cc/2019-2020/Everton-Stats Everton : 2020 : (43, 25) https://fbref.com/en/squads/5bfb9659/2019-2020/Leeds-United-Stats Leeds United : 2020 : (49, 25) https://fbref.com/en/squads/943e8050/2019-2020/Burnley-Stats Burnley : 2020 : (41, 25) https://fbref.com/en/squads/2abfe087/2019-2020/Watford-Stats Watford : 2020 : (43, 25) https://fbref.com/en/squads/1c781004/2019-2020/Norwich-City-Stats Norwich City : 2020 : (43, 25) https://fbref.com/en/squads/b8fd03ef/2018-2019/Manchester-City-Stats Manchester City : 2019 : (61, 25) https://fbref.com/en/squads/822bd0ba/2018-2019/Liverpool-Stats Liverpool : 2019 : (53, 25) https://fbref.com/en/squads/cff3d9bb/2018-2019/Chelsea-Stats Chelsea : 2019 : (63, 25) https://fbref.com/en/squads/361ca564/2018-2019/Tottenham-Hotspur-Stats Tottenham : 2019 : (58, 25) https://fbref.com/en/squads/18bb7c10/2018-2019/Arsenal-Stats Arsenal : 2019 : (58, 25) https://fbref.com/en/squads/19538871/2018-2019/Manchester-United-Stats Manchester Utd : 2019 : (53, 25) https://fbref.com/en/squads/7c21e445/2018-2019/West-Ham-United-Stats West Ham : 2019 : (43, 25) https://fbref.com/en/squads/a2d435b3/2018-2019/Leicester-City-Stats Leicester City : 2019 : (43, 25) https://fbref.com/en/squads/d07537b9/2018-2019/Brighton-and-Hove-Albion-Stats Brighton : 2019 : (45, 25) https://fbref.com/en/squads/8cec06e1/2018-2019/Wolverhampton-Wanderers-Stats Wolves : 2019 : (46, 25) https://fbref.com/en/squads/b2b47a98/2018-2019/Newcastle-United-Stats Newcastle Utd : 2019 : (42, 25) https://fbref.com/en/squads/47c64c55/2018-2019/Crystal-Palace-Stats Crystal Palace : 2019 : (45, 25) https://fbref.com/en/squads/cd051869/2018-2019/Brentford-Stats Brentford : 2019 : (53, 25) https://fbref.com/en/squads/8602292d/2018-2019/Aston-Villa-Stats Aston Villa : 2019 : (52, 25) https://fbref.com/en/squads/33c895d4/2018-2019/Southampton-Stats Southampton : 2019 : (43, 25) https://fbref.com/en/squads/d3fd31cc/2018-2019/Everton-Stats Everton : 2019 : (42, 25) https://fbref.com/en/squads/5bfb9659/2018-2019/Leeds-United-Stats Leeds United : 2019 : (51, 25) https://fbref.com/en/squads/943e8050/2018-2019/Burnley-Stats Burnley : 2019 : (47, 25) https://fbref.com/en/squads/2abfe087/2018-2019/Watford-Stats Watford : 2019 : (46, 25) https://fbref.com/en/squads/1c781004/2018-2019/Norwich-City-Stats Norwich City : 2019 : (51, 25) https://fbref.com/en/squads/b8fd03ef/2017-2018/Manchester-City-Stats Manchester City : 2018 : (57, 25) https://fbref.com/en/squads/822bd0ba/2017-2018/Liverpool-Stats Liverpool : 2018 : (56, 25) https://fbref.com/en/squads/cff3d9bb/2017-2018/Chelsea-Stats Chelsea : 2018 : (59, 25) https://fbref.com/en/squads/361ca564/2017-2018/Tottenham-Hotspur-Stats Tottenham : 2018 : (55, 25) https://fbref.com/en/squads/18bb7c10/2017-2018/Arsenal-Stats Arsenal : 2018 : (60, 25) https://fbref.com/en/squads/19538871/2017-2018/Manchester-United-Stats Manchester Utd : 2018 : (56, 25) https://fbref.com/en/squads/7c21e445/2017-2018/West-Ham-United-Stats West Ham : 2018 : (45, 25) https://fbref.com/en/squads/a2d435b3/2017-2018/Leicester-City-Stats Leicester City : 2018 : (47, 25) https://fbref.com/en/squads/d07537b9/2017-2018/Brighton-and-Hove-Albion-Stats Brighton : 2018 : (44, 25) https://fbref.com/en/squads/8cec06e1/2017-2018/Wolverhampton-Wanderers-Stats Column ['FK'] not in index missing from the dataframe. So adding an extra column for consistency https://fbref.com/en/squads/b2b47a98/2017-2018/Newcastle-United-Stats Newcastle Utd : 2018 : (41, 25) https://fbref.com/en/squads/47c64c55/2017-2018/Crystal-Palace-Stats Crystal Palace : 2018 : (42, 25) https://fbref.com/en/squads/cd051869/2017-2018/Brentford-Stats Column ['FK'] not in index missing from the dataframe. So adding an extra column for consistency https://fbref.com/en/squads/8602292d/2017-2018/Aston-Villa-Stats Column ['FK'] not in index missing from the dataframe. So adding an extra column for consistency https://fbref.com/en/squads/33c895d4/2017-2018/Southampton-Stats Southampton : 2018 : (44, 25) https://fbref.com/en/squads/d3fd31cc/2017-2018/Everton-Stats Everton : 2018 : (51, 25) https://fbref.com/en/squads/5bfb9659/2017-2018/Leeds-United-Stats Column ['FK'] not in index missing from the dataframe. So adding an extra column for consistency https://fbref.com/en/squads/943e8050/2017-2018/Burnley-Stats Burnley : 2018 : (41, 25) https://fbref.com/en/squads/2abfe087/2017-2018/Watford-Stats Watford : 2018 : (41, 25) https://fbref.com/en/squads/1c781004/2017-2018/Norwich-City-Stats Column ['FK'] not in index missing from the dataframe. So adding an extra column for consistency In [ ]: match_df = pd . concat ( all_matches ) match_df . columns = [ c . lower () for c in match_df . columns ] match_df . to_csv ( \"matches.csv\" )","tags":"Data Collection","url":"https://apurba-saha.github.io/web-scrapping-data-for-english-premier-league-football-matches","loc":"https://apurba-saha.github.io/web-scrapping-data-for-english-premier-league-football-matches"},{"title":"Finding Heavy Traffic Indicators","text":"Finding Heavy Traffic Indicators on I-94 In this project, we are going to analyze a dataset about the westbound traffic on the I-94 Interstate highway. The dataset is available here . The goal of this project is to determine a few indicators of heavy traffic on I-94. These indicators can be weather type, time of the day, time of the week, etc. For instance, we may find out that the traffic is usually heavier in the summer or when it snows. The dataset documentation mentions that a station located approximately midway between Minneapolis and Saint Paul recorded the traffic data. Also, the station only records westbound traffic (cars moving from east to west). This means that the results of our analysis will be about the westbound traffic in the proximity of that station. In other words, we should avoid generalizing our results for the entire I-94 highway. In [1]: # Import all libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline In [2]: # Read the dataset and get some insight traffic = pd . read_csv ( \"Metro_Interstate_Traffic_Volume.csv\" ) traffic . head () Out[2]: holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume 0 None 288.28 0.0 0.0 40 Clouds scattered clouds 2012-10-02 09:00:00 5545 1 None 289.36 0.0 0.0 75 Clouds broken clouds 2012-10-02 10:00:00 4516 2 None 289.58 0.0 0.0 90 Clouds overcast clouds 2012-10-02 11:00:00 4767 3 None 290.13 0.0 0.0 90 Clouds overcast clouds 2012-10-02 12:00:00 5026 4 None 291.14 0.0 0.0 75 Clouds broken clouds 2012-10-02 13:00:00 4918 In [3]: traffic . tail () Out[3]: holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume 48199 None 283.45 0.0 0.0 75 Clouds broken clouds 2018-09-30 19:00:00 3543 48200 None 282.76 0.0 0.0 90 Clouds overcast clouds 2018-09-30 20:00:00 2781 48201 None 282.73 0.0 0.0 90 Thunderstorm proximity thunderstorm 2018-09-30 21:00:00 2159 48202 None 282.09 0.0 0.0 90 Clouds overcast clouds 2018-09-30 22:00:00 1450 48203 None 282.12 0.0 0.0 90 Clouds overcast clouds 2018-09-30 23:00:00 954 In [4]: traffic . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 48204 entries, 0 to 48203 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 holiday 48204 non-null object 1 temp 48204 non-null float64 2 rain_1h 48204 non-null float64 3 snow_1h 48204 non-null float64 4 clouds_all 48204 non-null int64 5 weather_main 48204 non-null object 6 weather_description 48204 non-null object 7 date_time 48204 non-null object 8 traffic_volume 48204 non-null int64 dtypes: float64(3), int64(2), object(4) memory usage: 3.3+ MB In [5]: traffic . describe ( include = 'all' ) Out[5]: holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume count 48204 48204.000000 48204.000000 48204.000000 48204.000000 48204 48204 48204 48204.000000 unique 12 NaN NaN NaN NaN 11 38 40575 NaN top None NaN NaN NaN NaN Clouds sky is clear 2013-05-19 10:00:00 NaN freq 48143 NaN NaN NaN NaN 15164 11665 6 NaN mean NaN 281.205870 0.334264 0.000222 49.362231 NaN NaN NaN 3259.818355 std NaN 13.338232 44.789133 0.008168 39.015750 NaN NaN NaN 1986.860670 min NaN 0.000000 0.000000 0.000000 0.000000 NaN NaN NaN 0.000000 25% NaN 272.160000 0.000000 0.000000 1.000000 NaN NaN NaN 1193.000000 50% NaN 282.450000 0.000000 0.000000 64.000000 NaN NaN NaN 3380.000000 75% NaN 291.806000 0.000000 0.000000 90.000000 NaN NaN NaN 4933.000000 max NaN 310.070000 9831.300000 0.510000 100.000000 NaN NaN NaN 7280.000000 By just observing the descriptive statistics of the dataset, it is obvious that the 'temp' and 'rain_1h' columns have some outliers. The minimum temperature is 0k. Such a low temperature is not possible on earth. So, it doesn't make sense. Similarly, the mean value of 'rain_1h' column is 0.33mm. But the maximum value is 9831.30mm, which also doesn't make sense. Moreover, the date_time column holds non-numeric values. We need to convert them to either date-time object or integer values for future convenience. To get some insight about the the 'traffic_volume' columns, let's plot a histrogram. In [6]: traffic [ 'traffic_volume' ] . hist () Out[6]: <matplotlib.axes._subplots.AxesSubplot at 0x7fba6c910d60> As we can observe, the nature of the histogram is bimodal. It indicates that either the data has been collected from two different sources or the data can be seperated into two different parts. From descriptive statistics, we know that: About 25% of the time, there were 1,193 cars or fewer passing the station each hour — this probably occurs during the night, or when a road is under construction. About 25% of the time, the traffic volume was four times as much (4,933 cars or more). This possibility that nighttime and daytime might influence traffic volume gives our analysis an interesting direction: comparing daytime with nighttime data. For this purpose, we divide the dataset into two parts: Daytime data: hours from 7 a.m. to 7 p.m. (12 hours) Nighttime data: hours from 7 p.m. to 7 a.m. (12 hours) First, we transform the date_time column to datetime for our convenience In [7]: traffic [ 'date_time' ] = pd . to_datetime ( traffic [ 'date_time' ]) In [8]: traffic . head ( 2 ) Out[8]: holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume 0 None 288.28 0.0 0.0 40 Clouds scattered clouds 2012-10-02 09:00:00 5545 1 None 289.36 0.0 0.0 75 Clouds broken clouds 2012-10-02 10:00:00 4516 In [9]: traffic . tail ( 2 ) Out[9]: holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume 48202 None 282.09 0.0 0.0 90 Clouds overcast clouds 2018-09-30 22:00:00 1450 48203 None 282.12 0.0 0.0 90 Clouds overcast clouds 2018-09-30 23:00:00 954 In [10]: print ( traffic [ 'date_time' ] . dt . hour . head ( 2 )) type ( traffic [ 'date_time' ] . dt . hour ) 0 9 1 10 Name: date_time, dtype: int64 Out[10]: pandas.core.series.Series In [11]: daytime = traffic . copy ()[ traffic [ 'date_time' ] . dt . hour . between ( 6 , 19 , inclusive = False )] nighttime = traffic . copy ()[( traffic [ 'date_time' ] . dt . hour > 18 ) | ( traffic [ 'date_time' ] . dt . hour < 7 )] print ( daytime . head ( 2 )[ 'date_time' ]) print ( daytime . tail ( 2 )[ 'date_time' ]) print ( nighttime . head ( 2 )[ 'date_time' ]) print ( nighttime . tail ( 2 )[ 'date_time' ]) 0 2012-10-02 09:00:00 1 2012-10-02 10:00:00 Name: date_time, dtype: datetime64[ns] 48197 2018-09-30 17:00:00 48198 2018-09-30 18:00:00 Name: date_time, dtype: datetime64[ns] 10 2012-10-02 19:00:00 11 2012-10-02 20:00:00 Name: date_time, dtype: datetime64[ns] 48202 2018-09-30 22:00:00 48203 2018-09-30 23:00:00 Name: date_time, dtype: datetime64[ns] In [12]: # Plot the histograms of traffic_volume for both day and night plt . figure ( figsize = ( 8 , 3 )) plt . subplot ( 1 , 2 , 1 ) plt . hist ( daytime [ 'traffic_volume' ]) plt . title ( \"Day time traffic volume\" ) plt . xlabel ( \"Traffic Volume\" ) plt . ylabel ( \"Frequency\" ) plt . ylim ([ 0 , 8000 ]) plt . xlim ([ 0 , 8000 ]) plt . subplot ( 1 , 2 , 2 ) plt . hist ( nighttime [ 'traffic_volume' ]) plt . title ( \"Night time traffic volume\" ) plt . xlabel ( \"Traffic Volume\" ) plt . ylabel ( \"Frequency\" ) plt . ylim ([ 0 , 8000 ]) plt . xlim ([ 0 , 8000 ]) plt . tight_layout () plt . show () From the histogram above, it is obvious that the traffic at night is generally light. Our goal is to find indicators of heavy traffic, so we have decided to only focus on the daytime data moving forward. One of the possible indicators of heavy traffic is time. There might be more people on the road in a certain month, on a certain day, or at a certain time of the day. We're going to look at a few line plots showing how the traffic volume changed according to the following parameters: Month Day of the week Time of day Let's start by getting average traffic volume for each month and generating the line plot. In [13]: daytime [ 'month' ] = daytime [ 'date_time' ] . dt . month by_month = daytime . groupby ( 'month' ) . mean () by_month . head ( 12 ) Out[13]: temp rain_1h snow_1h clouds_all traffic_volume month 1 265.483409 0.015080 0.000631 58.554108 4495.613727 2 266.663789 0.004019 0.000000 51.550459 4711.198394 3 273.619940 0.015839 0.000000 56.827811 4889.409560 4 279.661071 0.105343 0.000000 59.221525 4906.894305 5 289.282668 0.130863 0.000000 56.548825 4911.121609 6 294.576068 0.271121 0.000000 48.612374 4898.019566 7 296.785052 4.412258 0.000000 42.176619 4595.035744 8 295.119191 0.228113 0.000000 42.556892 4928.302035 9 292.520287 0.289807 0.000000 45.184112 4870.783145 10 284.081011 0.016065 0.000000 53.497990 4921.234922 11 276.698007 0.006200 0.000000 56.697187 4704.094319 12 267.727846 0.035365 0.002213 66.942237 4374.834566 In [14]: plt . plot ( by_month . index , by_month [ 'traffic_volume' ]) plt . show () As we can see, the daytime traffic volume is usually less heavy in cold months. and more intense during warm months - with one interesting exception: July. Is there anything special about July? Is traffic significantly less heavy in July each year? Let's check. In [15]: daytime [ 'year' ] = daytime [ 'date_time' ] . dt . year july_data = daytime [ daytime [ 'month' ] == 7 ] july_data_grouped = july_data . groupby ( 'year' ) . mean () july_data_grouped . head () Out[15]: temp rain_1h snow_1h clouds_all traffic_volume month year 2013 296.796832 0.529838 0.0 24.245946 4914.354054 7.0 2014 296.225777 0.296686 0.0 52.542522 4871.598240 7.0 2015 296.657288 0.457241 0.0 35.301887 4613.146226 7.0 2016 297.018584 24.633670 0.0 52.534483 3924.923645 7.0 2017 297.139799 0.000000 0.0 41.724311 4734.120301 7.0 In [16]: plt . plot ( july_data_grouped . index , july_data_grouped [ 'traffic_volume' ]) plt . show () Typically, the traffic is pretty heavy in July, similar to the other warm months. The only exception we see is 2016, which had a high decrease in traffic volume. One possible reason for this is road construction. As a tentative conclusion here, we can say that warm months generally show heavier traffic compared to cold months. In a warm month, you can can expect for each hour of daytime a traffic volume close to 5,000 cars. Now, let's get the traffic volume averages for each day of the week. In [17]: daytime [ 'dayofweek' ] = daytime [ 'date_time' ] . dt . dayofweek by_dayofweek = daytime . groupby ( 'dayofweek' ) . mean () by_dayofweek [ 'traffic_volume' ] # 0 is Monday, 6 is Sunday Out[17]: dayofweek 0 4893.551286 1 5189.004782 2 5284.454282 3 5311.303730 4 5291.600829 5 3927.249558 6 3436.541789 Name: traffic_volume, dtype: float64 In [18]: plt . plot ( by_dayofweek . index , by_dayofweek [ 'traffic_volume' ]) plt . xticks ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 ], [ 'Monday' , 'Tuesday' , 'Wednesday' , 'Thursday' , 'Friday' , 'Saturday' , 'Sunday' ], rotation = 30 ) plt . show () From this figure, we have found that the traffic volume is significantly heavier on business days compared to the weekends. We'll now generate a line plot for the time of day. The weekends, however, will drag down the average values, so we're going to look at the averages separately. To do that, we'll start by splitting the data based on the day type: business day or weekend. In [19]: daytime [ 'hour' ] = daytime [ 'date_time' ] . dt . hour bussiness_days = daytime . copy ()[ daytime [ 'dayofweek' ] <= 4 ] # 4 == Friday weekend = daytime . copy ()[ daytime [ 'dayofweek' ] >= 5 ] # 5 == Saturday by_hour_business = bussiness_days . groupby ( 'hour' ) . mean () by_hour_weekend = weekend . groupby ( 'hour' ) . mean () plt . figure ( figsize = ( 8 , 3 )) plt . subplot ( 1 , 2 , 1 ) plt . plot ( by_hour_business . index , by_hour_business [ 'traffic_volume' ]) plt . title ( 'Business days' ) plt . xlabel ( 'Hour' ) plt . ylabel ( 'Traffic volume' ) plt . ylim ([ 1000 , 7000 ]) plt . subplot ( 1 , 2 , 2 ) plt . plot ( by_hour_weekend . index , by_hour_weekend [ 'traffic_volume' ]) plt . title ( 'Weekends' ) plt . xlabel ( 'Hour' ) plt . ylabel ( 'Traffic volume' ) plt . ylim ([ 1000 , 7000 ]) plt . tight_layout () plt . show () At each hour of the day, the traffic volume is generally higher during business days compared to the weekends. As somehow expected, the rush hours are around 7 and 16 — when most people travel from home to work and back. We see volumes of over 6,000 cars at rush hours. To summarize, we found a few time-related indicators of heavy traffic: The traffic is usually heavier during warm months (March–October) compared to cold months (November–February). The traffic is usually heavier on business days compared to weekends. On business days, the rush hours are around 7 and 16. Another possible indicator of heavy traffic is weather. The dataset provides us with a few useful columns about weather: temp, rain_1h, snow_1h, clouds_all, weather_main, weather_description. A few of these columns are numerical, so let's start by looking up their correlation values with traffic_volume. In [20]: daytime . corr ()[ 'traffic_volume' ] Out[20]: temp 0.128317 rain_1h 0.003697 snow_1h 0.001265 clouds_all -0.032932 traffic_volume 1.000000 month -0.022337 year -0.003557 dayofweek -0.416453 hour 0.172704 Name: traffic_volume, dtype: float64 Temperature shows the strongest correlation with a value of just +0.13. The other relevant columns (rain_1h, snow_1h, clouds_all) don't show any strong correlation with traffic_value. Let's generate a scatter plot to visualize the correlation between temp and traffic_volume. In [21]: plt . scatter ( daytime [ 'temp' ], daytime [ 'traffic_volume' ]) plt . show () Obviously, two outliers at 0k need to be removed. Let's generate the scatter plot without the outliers. In [22]: plt . scatter ( daytime [ 'temp' ], daytime [ 'traffic_volume' ]) plt . xlim ([ 230 , 320 ]) plt . show () We can conclude that temperature doesn't look like a solid indicator of heavy traffic. Let's now look at the other weather-related columns: weather_main and weather_description. In [23]: by_weather_main = daytime . groupby ( 'weather_main' ) . mean () by_weather_description = daytime . groupby ( 'weather_description' ) . mean () In [24]: by_weather_main . plot . barh ( y = 'traffic_volume' , legend = None ) plt . show () It looks like there's no weather type where traffic volume exceeds 5,000 cars. This makes finding a heavy traffic indicator more difficult. Let's also group by weather_description, which has a more granular weather classification. In [25]: by_weather_description . plot . barh ( y = 'traffic_volume' , legend = None , figsize = ( 5 , 10 )) plt . show () It looks like there are three weather types where traffic volume exceeds 5,000: Shower snow Light rain and snow Proximity thunderstorm with drizzle It's not clear why these weather types have the highest average traffic values — this is bad weather, but not that bad. Perhaps more people take their cars out of the garage when the weather is bad instead of riding a bike or walking. Conclusions In this project, we tried to find a few indicators of heavy traffic on the I-94 Interstate highway. We managed to find two types of indicators: Time indicators The traffic is usually heavier during warm months (March–October) compared to cold months (November–February). The traffic is usually heavier on business days compared to the weekends. On business days, the rush hours are around 7 and 16. Weather indicators Shower snow Light rain and snow Proximity thunderstorm with drizzle","tags":"Data Visualization","url":"https://apurba-saha.github.io/finding-heavy-traffic-indicators","loc":"https://apurba-saha.github.io/finding-heavy-traffic-indicators"},{"title":"Exploring Hacker News Post","text":"Goal of the project Hacker News is a site, where user-submitted stories (known as \"posts\") are voted and commented upon, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result. We're specifically interested in posts whose titles begin with either Ask HN or Show HN . Users submit Ask HN posts to ask the Hacker News community a specific question. Likewise, users submit Show HN posts to show the Hacker News community a project, product, or just generally something interesting. The goal of this project is to answer the following two questions: Do Ask HN or Show HN recieve more comments on average? Do posts created at a certain time receive more comments on average? Data Collection To find answers for our questions, we will analyse the hacker news post dataset, which can be downloaded from this link . Note that the dataset has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments, and then randomly sampling from the remaining submissions. Let's first read the dataset as a list of list. In [1]: from csv import reader opened_file = open ( \"hacker_news.csv\" ) read_file = reader ( opened_file ) hn = list ( read_file ) hn_header = hn [ 0 ] # Seperate the header from the dataset for future analysis hn = hn [ 1 :] In [2]: def explore_data ( dataset , header = [], rows_and_column = True ): if header : print ( header ) print ( ' \\n ' ) for row in dataset [: 5 ]: print ( row ) print ( ' \\n ' ) if rows_and_column : total_column = len ( header ) if header else len ( dataset [ 0 ]) print ( \"The number of rows: \" , len ( dataset )) print ( \"The number of columns: \" , total_column ) explore_data ( hn , hn_header ) ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] ['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'] ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'] ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'] ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01'] ['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12'] The number of rows: 20100 The number of columns: 7 Below are the descriptions of the columns: Column Description id The unique identifier from Hacker News for the post title The title of the post url The URL that the posts links to, if the post has a URL num_points The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes num_comments The number of comments that were made on the post author The username of the person who submitted the post created_at The date and time at which the post was submitted (the time zone is Eastern Time in the US) Data cleaning In this project, we are only interested in those posts that starts with either Ask HN or Show HN . Thus we only create a new lists of lists containing just the data for those titles. In [3]: ask_posts = [] show_posts = [] other_posts = [] for row in hn : title = row [ 1 ] title = title . lower () if title . startswith ( 'ask hn' ): ask_posts . append ( row ) elif title . startswith ( 'show hn' ): show_posts . append ( row ) else : other_posts . append ( row ) print ( \"Exploring ASK posts...\" ) explore_data ( ask_posts ) print ( ' \\n ' ) print ( \"Exploring SHOW posts...\" ) explore_data ( show_posts ) Exploring ASK posts... ['12296411', 'Ask HN: How to improve my personal website?', '', '2', '6', 'ahmedbaracat', '8/16/2016 9:55'] ['10610020', 'Ask HN: Am I the only one outraged by Twitter shutting down share counts?', '', '28', '29', 'tkfx', '11/22/2015 13:43'] ['11610310', 'Ask HN: Aby recent changes to CSS that broke mobile?', '', '1', '1', 'polskibus', '5/2/2016 10:14'] ['12210105', 'Ask HN: Looking for Employee #3 How do I do it?', '', '1', '3', 'sph130', '8/2/2016 14:20'] ['10394168', 'Ask HN: Someone offered to buy my browser extension from me. What now?', '', '28', '17', 'roykolak', '10/15/2015 16:38'] The number of rows: 1744 The number of columns: 7 Exploring SHOW posts... ['10627194', 'Show HN: Wio Link ESP8266 Based Web of Things Hardware Development Platform', 'https://iot.seeed.cc', '26', '22', 'kfihihc', '11/25/2015 14:03'] ['10646440', 'Show HN: Something pointless I made', 'http://dn.ht/picklecat/', '747', '102', 'dhotson', '11/29/2015 22:46'] ['11590768', 'Show HN: Shanhu.io, a programming playground powered by e8vm', 'https://shanhu.io', '1', '1', 'h8liu', '4/28/2016 18:05'] ['12178806', 'Show HN: Webscope Easy way for web developers to communicate with Clients', 'http://webscopeapp.com', '3', '3', 'fastbrick', '7/28/2016 7:11'] ['10872799', 'Show HN: GeoScreenshot Easily test Geo-IP based web pages', 'https://www.geoscreenshot.com/', '1', '9', 'kpsychwave', '1/9/2016 20:45'] The number of rows: 1162 The number of columns: 7 Now let's check if there are any rows with missing data. If data is missing from any row, we will delete that row In [4]: def check_and_remove ( dataset , total_column ): removed_items = [] for row in dataset : if len ( row ) < total_column : removed_items . append ( row ) dataset . remove ( row ) return removed_items removed_ask_post_rows = check_and_remove ( ask_posts , 7 ) print ( \"Total rows deleted from ask_posts: \" , len ( removed_ask_post_rows )) removed_show_post_rows = check_and_remove ( show_posts , 7 ) print ( \"Total rows deleted from show_posts: \" , len ( removed_show_post_rows )) Total rows deleted from ask_posts: 0 Total rows deleted from show_posts: 0 Excellent! No data is missing from any rows. Next, we need to check if there is any duplicate data. In [5]: def check_duplicate ( dataset ): duplicate_rows = [] already_added = [] for row in dataset : id = row [ 0 ] if id not in already_added : already_added . append ( id ) else : duplicate_rows . append ( row ) return duplicate_rows duplicate_ask_posts = check_duplicate ( ask_posts ) duplicate_show_posts = check_duplicate ( show_posts ) print ( \"Total duplicates found in ask_post: \" , len ( duplicate_ask_posts )) print ( \"Total duplicates found in show_post: \" , len ( duplicate_show_posts )) Total duplicates found in ask_post: 0 Total duplicates found in show_post: 0 Bravo! Even no duplicates found in the dataset. That makes our life easier, isn't it? However, out dataset is almost clean. Even the dates in created_at column follows a consistent format. So, there is no necessity to standardize the date entries. Data analysis As we have cleaned the data, we will now start analyze the dataset to find answers for our questions. At first, let's determine if ask posts or show posts receive more comments on average. In [6]: def find_avg_comment ( dataset ): total_comments = 0 for post in dataset : num_comment = int ( post [ 4 ]) total_comments += num_comment avg_comments = total_comments / len ( dataset ) return avg_comments In [7]: avg_ask_comments = find_avg_comment ( ask_posts ) print ( \"The average number of comments on ask posts: {:.2f} \" . format ( avg_ask_comments )) The average number of comments on ask posts: 14.04 In [8]: avg_show_comments = find_avg_comment ( show_posts ) print ( \"The average number of comments on show posts: {:.2f} \" . format ( avg_show_comments )) The average number of comments on show posts: 10.32 In [9]: avg_other_comments = find_avg_comment ( other_posts ) print ( \"The average number of comments on other posts: {:.2f} \" . format ( avg_other_comments )) The average number of comments on other posts: 26.87 As we can observe, the ask posts recieve more comments than show posts on average. Since ask posts are more likely to receive comments, we'll focus our remaining analysis just on these posts. Next, we'll determine if ask posts created at a certain time are more likely to attract comments. We'll use the following steps to perform this analysis: Calculate the amount of ask posts created in each hour of the day, along with the number of comments received. Calculate the average number of comments ask posts receive by hour created In [10]: import datetime as dt result_list = [] for post in ask_posts : created_at = post [ 6 ] num_comment = post [ 4 ] result_list . append ([ created_at , num_comment ]) print ( result_list [: 5 ]) [['8/16/2016 9:55', '6'], ['11/22/2015 13:43', '29'], ['5/2/2016 10:14', '1'], ['8/2/2016 14:20', '3'], ['10/15/2015 16:38', '17']] In [11]: counts_by_hour = {} comments_by_hour = {} for row in result_list : date = row [ 0 ] comment_num = int ( row [ 1 ]) dt_object = dt . datetime . strptime ( date , \"%m/ %d /%Y %H:%M\" ) post_hour = dt_object . hour if post_hour not in counts_by_hour : counts_by_hour [ post_hour ] = 1 comments_by_hour [ post_hour ] = comment_num else : counts_by_hour [ post_hour ] += 1 comments_by_hour [ post_hour ] += comment_num In [12]: # print the number of posts by hour in descending order print ({ k : v for k , v in sorted ( counts_by_hour . items (), key = lambda elem : elem [ 1 ], reverse = True )}) {15: 116, 19: 110, 21: 109, 18: 109, 16: 108, 14: 107, 17: 100, 13: 85, 20: 80, 12: 73, 22: 71, 23: 68, 1: 60, 10: 59, 2: 58, 11: 58, 0: 55, 3: 54, 8: 48, 4: 47, 5: 46, 9: 45, 6: 44, 7: 34} In [13]: # print the number of comments by hour in descending order print ({ k : v for k , v in sorted ( comments_by_hour . items (), key = lambda elem : elem [ 1 ], reverse = True )}) {15: 4477, 16: 1814, 21: 1745, 20: 1722, 18: 1439, 14: 1416, 2: 1381, 13: 1253, 19: 1188, 17: 1146, 10: 793, 12: 687, 1: 683, 11: 641, 23: 543, 8: 492, 22: 479, 5: 464, 0: 447, 3: 421, 6: 397, 4: 337, 7: 267, 9: 251} In [14]: avg_by_hour = [] for key , val in comments_by_hour . items (): avg = val / counts_by_hour [ key ] avg_by_hour . append ([ key , avg ]) print ( avg_by_hour [: 4 ]) [[9, 5.5777777777777775], [13, 14.741176470588234], [10, 13.440677966101696], [14, 13.233644859813085]] In [15]: sorted_by_avg_comment = sorted ( avg_by_hour , key = lambda elem : elem [ 1 ], reverse = True ) print ( \"Top 5 hours for Ask Posts Comments:\" ) for elem in sorted_by_avg_comment [: 5 ]: dt_object = dt . datetime . strptime ( str ( elem [ 0 ]), \"%H\" ) hour = dt_object . strftime ( \"%H:%M\" ) avg_comment = elem [ 1 ] print ( \" {0} : {1:.2f} average comments per post\" . format ( hour , avg_comment )) Top 5 hours for Ask Posts Comments: 15:00: 38.59 average comments per post 02:00: 23.81 average comments per post 20:00: 21.52 average comments per post 16:00: 16.80 average comments per post 21:00: 16.01 average comments per post Conclusions Now, we know which hours we should create a post during to have a higher chance of recieving comments. But if we refer back to the documentation for the data set, we will find that the Eastern time zone has been followed in the given dataset. So, we need to convert the time zone we live in. For example, I live in Arizona, USA. Hence, 3.00pm in Eastern timezone is equivalent to 12.00pm in my timezone. So, I may submit an ask post on HN at 12.00pm to maximize the chance of recieving comments.","tags":"Data Analysis","url":"https://apurba-saha.github.io/exploring-hacker-news-post","loc":"https://apurba-saha.github.io/exploring-hacker-news-post"},{"title":"Profitable App Profiles for the App Store and Google Play Markets","text":"Analysis of Google Play and Apple store apps The goal of this project is analyzing apps in Google Play and Apple store to understand the type of apps that attract more users. Based on the analysis, a new app will be developed for english speaking users, which will be available for free on the popular app stores. The developers will earn revenue through in-app ads. The more users that see and engage with adds, the better. In [1]: # Open the datasets and store them as lists from csv import reader applestore_dataset = open ( 'AppleStore.csv' , encoding = 'utf8' ) googleplay_dataset = open ( 'googleplaystore.csv' , encoding = 'utf8' ) apple_data = list ( reader ( applestore_dataset )) google_data = list ( reader ( googleplay_dataset )) In [2]: # A function for exploring data from a dataset. The function # assumes that the dataset parameter doesn't have a header row def explore_data ( dataset , start , end , rows_and_columns = False ): dataset_slice = dataset [ start : end ] for row in dataset_slice : print ( row ) print ( ' \\n ' ) # adds a new (empty) line after each row if rows_and_columns : print ( 'Number of rows:' , len ( dataset )) print ( 'Number of columns:' , len ( dataset [ 0 ])) In [3]: print ( \"Printing first five rows of AppleStore dataset...\" ) explore_data ( apple_data [ 1 :], 0 , 5 , True ) print ( ' \\n ' ) print ( \"Now printing first five rows of GooglePlay dataset...\" ) explore_data ( google_data [ 1 :], 0 , 5 , True ) Printing first five rows of AppleStore dataset... ['284882215', 'Facebook', '389879808', 'USD', '0.0', '2974676', '212', '3.5', '3.5', '95.0', '4+', 'Social Networking', '37', '1', '29', '1'] ['389801252', 'Instagram', '113954816', 'USD', '0.0', '2161558', '1289', '4.5', '4.0', '10.23', '12+', 'Photo & Video', '37', '0', '29', '1'] ['529479190', 'Clash of Clans', '116476928', 'USD', '0.0', '2130805', '579', '4.5', '4.5', '9.24.12', '9+', 'Games', '38', '5', '18', '1'] ['420009108', 'Temple Run', '65921024', 'USD', '0.0', '1724546', '3842', '4.5', '4.0', '1.6.2', '9+', 'Games', '40', '5', '1', '1'] ['284035177', 'Pandora - Music & Radio', '130242560', 'USD', '0.0', '1126879', '3594', '4.0', '4.5', '8.4.1', '12+', 'Music', '37', '4', '1', '1'] Number of rows: 7197 Number of columns: 16 Now printing first five rows of GooglePlay dataset... ['Photo Editor & Candy Camera & Grid & ScrapBook', 'ART_AND_DESIGN', '4.1', '159', '19M', '10,000+', 'Free', '0', 'Everyone', 'Art & Design', 'January 7, 2018', '1.0.0', '4.0.3 and up'] ['Coloring book moana', 'ART_AND_DESIGN', '3.9', '967', '14M', '500,000+', 'Free', '0', 'Everyone', 'Art & Design;Pretend Play', 'January 15, 2018', '2.0.0', '4.0.3 and up'] ['U Launcher Lite – FREE Live Cool Themes, Hide Apps', 'ART_AND_DESIGN', '4.7', '87510', '8.7M', '5,000,000+', 'Free', '0', 'Everyone', 'Art & Design', 'August 1, 2018', '1.2.4', '4.0.3 and up'] ['Sketch - Draw & Paint', 'ART_AND_DESIGN', '4.5', '215644', '25M', '50,000,000+', 'Free', '0', 'Teen', 'Art & Design', 'June 8, 2018', 'Varies with device', '4.2 and up'] ['Pixel Draw - Number Art Coloring Book', 'ART_AND_DESIGN', '4.3', '967', '2.8M', '100,000+', 'Free', '0', 'Everyone', 'Art & Design;Creativity', 'June 20, 2018', '1.1', '4.4 and up'] Number of rows: 10841 Number of columns: 13 At this point, it is clear that each element of this dataset is a string. So, datatype conversion may be necessary in future for performing any arithmatic operation on numbers. Also, the number of columns are different. Let's find out what the colums represent. In [4]: # print the header rows of each dataset print ( apple_data [ 0 ]) print ( ' \\n ' ) print ( google_data [ 0 ]) ['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic'] ['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver'] It is not clear from the header rows what each column describes. Let's check the documentation of these two datasets to get a better idea. We can find the documentation of the Google App dataset here . Similarly, the documentation of the AppleStore dataset is available in this link . Now it's fun time! Let's check if the data is clean i.e. we need to check if the dataset contains any wrong data or duplicate data. At first, let's try to clean the Google Play dataset. If we carefully read this discussion section, we will find that the row with index level 10473 (considering header row) has wrong data. In [5]: print ( google_data [ 10473 ]) print ( 'Length of the incorrect row: ' , len ( google_data [ 10473 ])) ['Life Made WI-Fi Touchscreen Photo Frame', '1.9', '19', '3.0M', '1,000+', 'Free', '0', 'Everyone', '', 'February 11, 2018', '1.0.19', '4.0 and up'] Length of the incorrect row: 12 Clearly, one data is missing from this row, as the length of header row is 13. After some deep observation, we find that the data for the 'Category' column is missing from the row. So, we delete this row with missing data. In [6]: del google_data [ 10473 ] print ( google_data [ 10473 ]) ['osmino Wi-Fi: free WiFi', 'TOOLS', '4.2', '134203', '4.1M', '10,000,000+', 'Free', '0', 'Everyone', 'Tools', 'August 7, 2018', '6.06.14', '4.4 and up'] Now, it's time to check for duplicate data. The following check_duplicate() function will check for dulicates in a dataset. If any duplicate is found, it puts the duplicate rows inside the list called duplicate_apps . In [7]: def check_duplicates ( dataset , appname_index , header = True ): unique_apps = [] duplicate_apps = [] start = 1 if header else 0 for app in dataset [ start :]: app_name = app [ appname_index ] if app_name not in unique_apps : unique_apps . append ( app_name ) else : duplicate_apps . append ( app ) return duplicate_apps duplicates = check_duplicates ( google_data , 0 ) print ( duplicates [: 5 ]) [['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80805', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up'], ['Box', 'BUSINESS', '4.2', '159872', 'Varies with device', '10,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 31, 2018', 'Varies with device', 'Varies with device'], ['Google My Business', 'BUSINESS', '4.4', '70991', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 24, 2018', '2.19.0.204537701', '4.4 and up'], ['ZOOM Cloud Meetings', 'BUSINESS', '4.4', '31614', '37M', '10,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 20, 2018', '4.1.28165.0716', '4.0 and up'], ['join.me - Simple Meetings', 'BUSINESS', '4.0', '6989', 'Varies with device', '1,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 16, 2018', '4.3.0.508', '4.4 and up']] In [8]: print ( 'Total number of duplicates: ' , len ( duplicates )) Total number of duplicates: 1181 Now, it's time for decision making!! We need to keep at least one of the duplicates for each app and discard the others. Instead of deleting duplicate rows randomly, we need to choose a selection criterion for this purpose. Let's take a look at the duplicate rows for some apps and try to find out if there is any difference. In [9]: some_duplicate_apps = duplicates [: 3 ] sample_app_names = [] for apps in some_duplicate_apps : app_name = apps [ 0 ] sample_app_names . append ( app_name ) for app in google_data [ 1 :]: if app [ 0 ] in sample_app_names : print ( app ) ['Google My Business', 'BUSINESS', '4.4', '70991', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 24, 2018', '2.19.0.204537701', '4.4 and up'] ['Box', 'BUSINESS', '4.2', '159872', 'Varies with device', '10,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 31, 2018', 'Varies with device', 'Varies with device'] ['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80805', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up'] ['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80805', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up'] ['Box', 'BUSINESS', '4.2', '159872', 'Varies with device', '10,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 31, 2018', 'Varies with device', 'Varies with device'] ['Google My Business', 'BUSINESS', '4.4', '70991', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 24, 2018', '2.19.0.204537701', '4.4 and up'] ['Box', 'BUSINESS', '4.2', '159872', 'Varies with device', '10,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 31, 2018', 'Varies with device', 'Varies with device'] ['Google My Business', 'BUSINESS', '4.4', '70991', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'July 24, 2018', '2.19.0.204537701', '4.4 and up'] ['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80804', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up'] If we notice the duplicate entries for 'Quick PDF Scanner' App, the main difference happen on the fourth position of each row, which corresponds to the number of views. The different numbers show the data was collected at different times. We can use this information to build a criterion for removing the duplicates. We won't remove rows randomly, but rather we'll keep the rows that have the highest number of reviews because the higher the number of reviews, the more reliable the ratings. To do that, we will: Create a dictionary where each key is a unique app name, and the value is the highest number of reviews of that app Use the dictionary to create a new data set, which will have only one entry per app (and we only select the apps with the highest number of reviews) In [10]: reviews_max = {} for app in google_data [ 1 :]: name = app [ 0 ] n_reviews = float ( app [ 3 ]) if name in reviews_max and n_reviews > reviews_max [ name ]: reviews_max [ name ] = n_reviews if name not in reviews_max : reviews_max [ name ] = n_reviews print ( len ( reviews_max )) 9659 Now, let's use the reviews_max dictionary to remove the duplicates. For the duplicate cases, we'll only keep the entries with the highest number of reviews. In the code cell below: We start by initializing two empty lists, android_clean and already_added. We loop through the android data set, and for every iteration: We isolate the name of the app and the number of reviews. We add the current row (app) to the android_clean list, and the app name (name) to the already_added list if: The number of reviews of the current app matches the number of reviews of that app as described in the reviews_max dictionary; and The name of the app is not already in the already_added list. We need to add this supplementary condition to account for those cases where the highest number of reviews of a duplicate app is the same for more than one entry (for example, the Box app has three entries, and the number of reviews is the same). If we just check for reviews_max[name] == n_reviews, we'll still end up with duplicate entries for some apps. In [11]: # The cleaned android dataset doesn't contain any header android_clean = [] already_added = [] for app in google_data [ 1 :]: name = app [ 0 ] n_reviews = float ( app [ 3 ]) if n_reviews == reviews_max [ name ] and name not in already_added : android_clean . append ( app ) already_added . append ( name ) print ( len ( android_clean )) 9659 Now let's check if there is any wrong data in App Store dataset. After reading the discussion section in this link , we didn't notice any issue with missing data. However, to make sure, we check if the length of each row in the App Store dataset is the same. If the length of a row is different, it is probably missing some data. If such a row is found, we will remove them from the dataset. In [12]: def check_rows_with_missing_data ( dataset , header_length , header = True ): start = 1 if header else 0 index = 0 row_index_with_missing_data = [] for app in dataset [ start :]: if len ( app ) < header_length : row_index_with_missing_data . append ( index ) index += 1 print ( \"Total number of rows with incorrect data: \" , \\ len ( row_index_with_missing_data )) return row_index_with_missing_data incorrect_rows = check_rows_with_missing_data ( apple_data , len ( apple_data [ 0 ])) Total number of rows with incorrect data: 0 So, there is no rows in the App store dataset that is missing data. Now, we will check for duplicate rows. In [13]: print ( apple_data [ 0 ]) ['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic'] In [14]: appname_index = 0 duplicate_appstore_app = check_duplicates ( apple_data , appname_index ) print ( \"Total number of dupicate apps: \" , len ( duplicate_appstore_app )) Total number of dupicate apps: 0 So, we don't need to do extra work for removing the duplicate data in App Store dataset. However, if we remember correctly, the target audiences for our apps are native english speaker. Therefore, we only want to analyze those apps that are directed toward English-speaking audience. If we explore the data long enough, we'll find that both data sets have apps with names that suggest they are not directed toward an English-speaking audience. We're not interested in keeping these apps, so we'll remove them. One way to go about this is to remove each app with a name containing a symbol that is not commonly used in English text. The numbers corresponding to the characters we commonly use in an English text are all in the range 0 to 127, according to the ASCII (American Standard Code for Information Interchange) system. Based on this number range, we can build a function that detects whether a character belongs to the set of common English characters or not. If the number is equal to or less than 127, then the character belongs to the set of common English characters. In [15]: ''' Checks if there's any character in the app name that doesn't belong to the set of common English characters. If less than three nonenglish character are found, returns True; otherwise, returns false. ''' def is_english_name ( app_name ): number_nonenglish_char = 0 for char in app_name : if ord ( char ) > 127 : number_nonenglish_char += 1 return True if number_nonenglish_char <= 3 else False # Test is_english_name() function print ( is_english_name ( \"Instragram\" )) print ( is_english_name ( '爱奇艺PPS -《欢乐颂2》电视剧热播' )) print ( is_english_name ( 'Docs To Go™ Free Office Suite' )) print ( is_english_name ( 'Instachat 😜' )) True False True True Now let's use the is_english_name() function to filter out non-english apps from both data sets. In [16]: english_android_apps = [] for app in android_clean : app_name = app [ 0 ] if is_english_name ( app_name ): english_android_apps . append ( app ) print ( \"Total english apps in Google play store: \" , len ( english_android_apps )) Total english apps in Google play store: 9614 In [17]: english_ios_apps = [] for app in apple_data [ 1 :]: app_name = app [ 1 ] if is_english_name ( app_name ): english_ios_apps . append ( app ) print ( \"Total english apps in Apple store: \" , len ( english_ios_apps )) Total english apps in Apple store: 6183 As we have already mentioned, we only build apps that are free to download and install, and our main source of revenue consists of in-app ads. Our data sets contain both free and non-free apps; we'll need to isolate only the free apps for our analysis.Isolating the free apps will be our last step in the data cleaning process. In [18]: print ( \"Some samples of Google Playstore Dataset: \\n \" ) print ( google_data [ 0 ]) print ( \" \\n \" ) explore_data ( english_android_apps , 0 , 3 ) print ( \" \\n \" ) print ( \"The columns of Apple store Dataset: \\n \" ) print ( apple_data [ 0 ]) print ( \" \\n \" ) explore_data ( english_ios_apps , 0 , 3 ) Some samples of Google Playstore Dataset: ['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver'] ['Photo Editor & Candy Camera & Grid & ScrapBook', 'ART_AND_DESIGN', '4.1', '159', '19M', '10,000+', 'Free', '0', 'Everyone', 'Art & Design', 'January 7, 2018', '1.0.0', '4.0.3 and up'] ['U Launcher Lite – FREE Live Cool Themes, Hide Apps', 'ART_AND_DESIGN', '4.7', '87510', '8.7M', '5,000,000+', 'Free', '0', 'Everyone', 'Art & Design', 'August 1, 2018', '1.2.4', '4.0.3 and up'] ['Sketch - Draw & Paint', 'ART_AND_DESIGN', '4.5', '215644', '25M', '50,000,000+', 'Free', '0', 'Teen', 'Art & Design', 'June 8, 2018', 'Varies with device', '4.2 and up'] The columns of Apple store Dataset: ['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic'] ['284882215', 'Facebook', '389879808', 'USD', '0.0', '2974676', '212', '3.5', '3.5', '95.0', '4+', 'Social Networking', '37', '1', '29', '1'] ['389801252', 'Instagram', '113954816', 'USD', '0.0', '2161558', '1289', '4.5', '4.0', '10.23', '12+', 'Photo & Video', '37', '0', '29', '1'] ['529479190', 'Clash of Clans', '116476928', 'USD', '0.0', '2130805', '579', '4.5', '4.5', '9.24.12', '9+', 'Games', '38', '5', '18', '1'] In [19]: free_android_english = [] free_ios_english = [] for app in english_android_apps : price = app [ 7 ] if price == '0' : free_android_english . append ( app ) for app in english_ios_apps : price = app [ 4 ] if price == '0.0' : free_ios_english . append ( app ) print ( \"The number of free english apps in Google Playstore: \" , \\ len ( free_android_english )) print ( \"The number of free ios apps in Apple Playstore: \" , \\ len ( free_ios_english )) The number of free english apps in Google Playstore: 8864 The number of free ios apps in Apple Playstore: 3222 Now, we have successfully cleaned the data. Now it's time to do some real job! Our aim is to determine the kinds of apps that are likely to attract more users because our revenue is highly influenced by the number of people using our apps. Because our end goal is to add the app on both Google Play and the App Store, we need to find app profiles that are successful on both markets. Let's begin the analysis by getting a sense of what are the most common genres for each market. For this, we'll need to build frequency tables for a few columns in our data sets. At first, let's try to find out what the most common genres in each market are. In [20]: def freq_table ( dataset , index ): ''' The function returns a frequeny table as a dictionary for the column specified by @parameter index. It assumes that the dataset has no header row. ''' freq_dict = {} freq_as_percentage = {} total_app = len ( dataset ) for row in dataset : key = row [ index ] if key not in freq_dict : freq_dict [ key ] = 1 else : freq_dict [ key ] += 1 for key , val in freq_dict . items (): freq_as_percentage [ key ] = ( val / total_app ) * 100 return freq_dict , freq_as_percentage In [21]: def display_table ( dataset , index ): table = freq_table ( dataset , index )[ 1 ] table_display = [] for key in table : key_val_as_tuple = ( table [ key ], key ) table_display . append ( key_val_as_tuple ) table_sorted = sorted ( table_display , reverse = True ) for entry in table_sorted : print ( \" {0} : {1:.2f} \" . format ( entry [ 1 ], entry [ 0 ])) In [22]: display_table ( free_ios_english , 11 ) Games : 58.16 Entertainment : 7.88 Photo & Video : 4.97 Education : 3.66 Social Networking : 3.29 Shopping : 2.61 Utilities : 2.51 Sports : 2.14 Music : 2.05 Health & Fitness : 2.02 Productivity : 1.74 Lifestyle : 1.58 News : 1.33 Travel : 1.24 Finance : 1.12 Weather : 0.87 Food & Drink : 0.81 Reference : 0.56 Business : 0.53 Book : 0.43 Navigation : 0.19 Medical : 0.19 Catalogs : 0.12 We can see that among the free English apps, more than a half (58.16%) are games. Entertainment apps are close to 8%, followed by photo and video apps, which are close to 5%. Only 3.66% of the apps are designed for education, followed by social networking apps which amount for 3.29% of the apps in our data set. The general impression is that App Store (at least the part containing free English apps) is dominated by apps that are designed for fun (games, entertainment, photo and video, social networking, sports, music, etc.), while apps with practical purposes (education, shopping, utilities, productivity, lifestyle, etc.) are more rare. However, the fact that fun apps are the most numerous doesn't also imply that they also have the greatest number of users — the demand might not be the same as the offer. Let's continue by examining the Genres and Category columns of the Google Play data set (two columns which seem to be related). In [23]: display_table ( free_android_english , 9 ) Tools : 8.45 Entertainment : 6.07 Education : 5.35 Business : 4.59 Productivity : 3.89 Lifestyle : 3.89 Finance : 3.70 Medical : 3.53 Sports : 3.46 Personalization : 3.32 Communication : 3.24 Action : 3.10 Health & Fitness : 3.08 Photography : 2.94 News & Magazines : 2.80 Social : 2.66 Travel & Local : 2.32 Shopping : 2.25 Books & Reference : 2.14 Simulation : 2.04 Dating : 1.86 Arcade : 1.85 Video Players & Editors : 1.77 Casual : 1.76 Maps & Navigation : 1.40 Food & Drink : 1.24 Puzzle : 1.13 Racing : 0.99 Role Playing : 0.94 Libraries & Demo : 0.94 Auto & Vehicles : 0.93 Strategy : 0.91 House & Home : 0.82 Weather : 0.80 Events : 0.71 Adventure : 0.68 Comics : 0.61 Beauty : 0.60 Art & Design : 0.60 Parenting : 0.50 Card : 0.45 Casino : 0.43 Trivia : 0.42 Educational;Education : 0.39 Board : 0.38 Educational : 0.37 Education;Education : 0.34 Word : 0.26 Casual;Pretend Play : 0.24 Music : 0.20 Racing;Action & Adventure : 0.17 Puzzle;Brain Games : 0.17 Entertainment;Music & Video : 0.17 Casual;Brain Games : 0.14 Casual;Action & Adventure : 0.14 Arcade;Action & Adventure : 0.12 Action;Action & Adventure : 0.10 Educational;Pretend Play : 0.09 Simulation;Action & Adventure : 0.08 Parenting;Education : 0.08 Entertainment;Brain Games : 0.08 Board;Brain Games : 0.08 Parenting;Music & Video : 0.07 Educational;Brain Games : 0.07 Casual;Creativity : 0.07 Art & Design;Creativity : 0.07 Education;Pretend Play : 0.06 Role Playing;Pretend Play : 0.05 Education;Creativity : 0.05 Role Playing;Action & Adventure : 0.03 Puzzle;Action & Adventure : 0.03 Entertainment;Creativity : 0.03 Entertainment;Action & Adventure : 0.03 Educational;Creativity : 0.03 Educational;Action & Adventure : 0.03 Education;Music & Video : 0.03 Education;Brain Games : 0.03 Education;Action & Adventure : 0.03 Adventure;Action & Adventure : 0.03 Video Players & Editors;Music & Video : 0.02 Sports;Action & Adventure : 0.02 Simulation;Pretend Play : 0.02 Puzzle;Creativity : 0.02 Music;Music & Video : 0.02 Entertainment;Pretend Play : 0.02 Casual;Education : 0.02 Board;Action & Adventure : 0.02 Video Players & Editors;Creativity : 0.01 Trivia;Education : 0.01 Travel & Local;Action & Adventure : 0.01 Tools;Education : 0.01 Strategy;Education : 0.01 Strategy;Creativity : 0.01 Strategy;Action & Adventure : 0.01 Simulation;Education : 0.01 Role Playing;Brain Games : 0.01 Racing;Pretend Play : 0.01 Puzzle;Education : 0.01 Parenting;Brain Games : 0.01 Music & Audio;Music & Video : 0.01 Lifestyle;Pretend Play : 0.01 Lifestyle;Education : 0.01 Health & Fitness;Education : 0.01 Health & Fitness;Action & Adventure : 0.01 Entertainment;Education : 0.01 Communication;Creativity : 0.01 Comics;Creativity : 0.01 Casual;Music & Video : 0.01 Card;Action & Adventure : 0.01 Books & Reference;Education : 0.01 Art & Design;Pretend Play : 0.01 Art & Design;Action & Adventure : 0.01 Arcade;Pretend Play : 0.01 Adventure;Education : 0.01 The landscape seems significantly different on Google Play: there are not that many apps designed for fun, and it seems that a good number of apps are designed for practical purposes (family, tools, business, lifestyle, productivity, etc.). However, if we investigate this further, we can see that the family category (which accounts for almost 19% of the apps) means mostly games for kids. Even so, practical apps seem to have a better representation on Google Play compared to App Store. This picture is also confirmed by the frequency table we see for the Genres column: In [24]: display_table ( free_android_english , 1 ) FAMILY : 18.91 GAME : 9.72 TOOLS : 8.46 BUSINESS : 4.59 LIFESTYLE : 3.90 PRODUCTIVITY : 3.89 FINANCE : 3.70 MEDICAL : 3.53 SPORTS : 3.40 PERSONALIZATION : 3.32 COMMUNICATION : 3.24 HEALTH_AND_FITNESS : 3.08 PHOTOGRAPHY : 2.94 NEWS_AND_MAGAZINES : 2.80 SOCIAL : 2.66 TRAVEL_AND_LOCAL : 2.34 SHOPPING : 2.25 BOOKS_AND_REFERENCE : 2.14 DATING : 1.86 VIDEO_PLAYERS : 1.79 MAPS_AND_NAVIGATION : 1.40 FOOD_AND_DRINK : 1.24 EDUCATION : 1.16 ENTERTAINMENT : 0.96 LIBRARIES_AND_DEMO : 0.94 AUTO_AND_VEHICLES : 0.93 HOUSE_AND_HOME : 0.82 WEATHER : 0.80 EVENTS : 0.71 PARENTING : 0.65 ART_AND_DESIGN : 0.64 COMICS : 0.62 BEAUTY : 0.60 The difference between the Genres and the Category columns is not crystal clear, but one thing we can notice is that the Genres column is much more granular (it has more categories). We're only looking for the bigger picture at the moment, so we'll only work with the Category column moving forward. Up to this point, we found that the App Store is dominated by apps designed for fun, while Google Play shows a more balanced landscape of both practical and for-fun apps. Now we'd like to get an idea about the kind of apps that have most users. One way to find out what genres are the most popular (have the most users) is to calculate the average number of installs for each app genre. For the Google Play data set, we can find this information in the Installs column, but for the App Store data set this information is missing. As a workaround, we'll take the total number of user ratings as a proxy, which we can find in the rating_count_tot app. Below, we calculate the average number of user ratings per app genre on the App Store: In [25]: genre_freq_table = freq_table ( free_ios_english , 11 )[ 0 ] print ( genre_freq_table ) {'Social Networking': 106, 'Photo & Video': 160, 'Games': 1874, 'Music': 66, 'Reference': 18, 'Health & Fitness': 65, 'Weather': 28, 'Utilities': 81, 'Travel': 40, 'Shopping': 84, 'News': 43, 'Navigation': 6, 'Lifestyle': 51, 'Entertainment': 254, 'Food & Drink': 26, 'Sports': 69, 'Book': 14, 'Finance': 36, 'Education': 118, 'Productivity': 56, 'Business': 17, 'Catalogs': 4, 'Medical': 6} In [26]: map_genre_rating = {} for app in free_ios_english : genre = app [ 11 ] user_rating = float ( app [ 5 ]) if genre not in map_genre_rating : map_genre_rating [ genre ] = user_rating else : map_genre_rating [ genre ] += user_rating In [27]: most_popular_genre = [] for genre , number in genre_freq_table . items (): total = map_genre_rating [ genre ] average_rating_per_genre = total / number most_popular_genre . append (( genre , average_rating_per_genre )) sorted_popular_genre = sorted ( most_popular_genre , key = lambda elem : elem [ 1 ], reverse = True ) for elem in sorted_popular_genre : genre_name = elem [ 0 ] average_ratng = elem [ 1 ] print ( ' {} : {:.2f} ' . format ( genre_name , average_ratng )) Navigation:86090.33 Reference:74942.11 Social Networking:71548.35 Music:57326.53 Weather:52279.89 Book:39758.50 Food & Drink:33333.92 Finance:31467.94 Photo & Video:28441.54 Travel:28243.80 Shopping:26919.69 Health & Fitness:23298.02 Sports:23008.90 Games:22788.67 News:21248.02 Productivity:21028.41 Utilities:18684.46 Lifestyle:16485.76 Entertainment:14029.83 Business:7491.12 Education:7003.98 Catalogs:4004.00 Medical:612.00 On average, navigation apps have the highest number of user reviews, but this figure is heavily influenced by Waze and Google Maps, which have close to half a million user reviews together: In [28]: for app in free_ios_english : if app [ - 5 ] == 'Navigation' : print ( app [ 1 ], ':' , app [ 5 ]) Waze - GPS Navigation, Maps & Real-time Traffic : 345046 Google Maps - Navigation & Transit : 154911 Geocaching® : 12811 CoPilot GPS – Car Navigation & Offline Maps : 3582 ImmobilienScout24: Real Estate Search in Germany : 187 Railway Route Search : 5 The same pattern applies to social networking apps, where the average number is heavily influenced by a few giants like Facebook, Pinterest, Skype, etc. Same applies to music apps, where a few big players like Pandora, Spotify, and Shazam heavily influence the average number. Our aim is to find popular genres, but navigation, social networking or music apps might seem more popular than they really are. The average number of ratings seem to be skewed by very few apps which have hundreds of thousands of user ratings, while the other apps may struggle to get past the 10,000 threshold Reference apps have 74,942 user ratings on average, but it's actually the Bible and Dictionary.com which skew up the average rating: In [29]: for app in free_ios_english : if app [ - 5 ] == 'Reference' : print ( app [ 1 ], ':' , app [ 5 ]) Bible : 985920 Dictionary.com Dictionary & Thesaurus : 200047 Dictionary.com Dictionary & Thesaurus for iPad : 54175 Google Translate : 26786 Muslim Pro: Ramadan 2017 Prayer Times, Azan, Quran : 18418 New Furniture Mods - Pocket Wiki & Game Tools for Minecraft PC Edition : 17588 Merriam-Webster Dictionary : 16849 Night Sky : 12122 City Maps for Minecraft PE - The Best Maps for Minecraft Pocket Edition (MCPE) : 8535 LUCKY BLOCK MOD ™ for Minecraft PC Edition - The Best Pocket Wiki & Mods Installer Tools : 4693 GUNS MODS for Minecraft PC Edition - Mods Tools : 1497 Guides for Pokémon GO - Pokemon GO News and Cheats : 826 WWDC : 762 Horror Maps for Minecraft PE - Download The Scariest Maps for Minecraft Pocket Edition (MCPE) Free : 718 VPN Express : 14 Real Bike Traffic Rider Virtual Reality Glasses : 8 教えて!goo : 0 Jishokun-Japanese English Dictionary & Translator : 0 However, this niche seems to show some potential. One thing we could do is take another popular book and turn it into an app where we could add different features besides the raw version of the book. This might include daily quotes from the book, an audio version of the book, quizzes about the book, etc. On top of that, we could also embed a dictionary within the app, so users don't need to exit our app to look up words in an external app. This idea seems to fit well with the fact that the App Store is dominated by for-fun apps. This suggests the market might be a bit saturated with for-fun apps, which means a practical app might have more of a chance to stand out among the huge number of apps on the App Store. Now let's analyze the Google Play market a bit. For the Google Play market, we actually have data about the number of installs, so we should be able to get a clearer picture about genre popularity. However, the install numbers don't seem precise enough — we can see that most values are open-ended (100+, 1,000+, 5,000+, etc.): In [30]: display_table ( free_android_english , 5 ) 1,000,000+ : 15.73 100,000+ : 11.55 10,000,000+ : 10.55 10,000+ : 10.20 1,000+ : 8.39 100+ : 6.92 5,000,000+ : 6.83 500,000+ : 5.56 50,000+ : 4.77 5,000+ : 4.51 10+ : 3.54 500+ : 3.25 50,000,000+ : 2.30 100,000,000+ : 2.13 50+ : 1.92 5+ : 0.79 1+ : 0.51 500,000,000+ : 0.27 1,000,000,000+ : 0.23 0+ : 0.05 0 : 0.01 One problem with this data is that is not precise. For instance, we don't know whether an app with 100,000+ installs has 100,000 installs, 200,000, or 350,000. However, we don't need very precise data for our purposes — we only want to get an idea which app genres attract the most users, and we don't need perfect precision with respect to the number of users. We're going to leave the numbers as they are, which means that we'll consider that an app with 100,000+ installs has 100,000 installs, and an app with 1,000,000+ installs has 1,000,000 installs, and so on. To perform computations, however, we'll need to convert each install number to float — this means that we need to remove the commas and the plus characters, otherwise the conversion will fail and raise an error. We'll do this directly in the loop below, where we also compute the average number of installs for each genre (category). In [31]: category_freq_table = freq_table ( free_android_english , 1 )[ 0 ] print ( category_freq_table ) {'ART_AND_DESIGN': 57, 'AUTO_AND_VEHICLES': 82, 'BEAUTY': 53, 'BOOKS_AND_REFERENCE': 190, 'BUSINESS': 407, 'COMICS': 55, 'COMMUNICATION': 287, 'DATING': 165, 'EDUCATION': 103, 'ENTERTAINMENT': 85, 'EVENTS': 63, 'FINANCE': 328, 'FOOD_AND_DRINK': 110, 'HEALTH_AND_FITNESS': 273, 'HOUSE_AND_HOME': 73, 'LIBRARIES_AND_DEMO': 83, 'LIFESTYLE': 346, 'GAME': 862, 'FAMILY': 1676, 'MEDICAL': 313, 'SOCIAL': 236, 'SHOPPING': 199, 'PHOTOGRAPHY': 261, 'SPORTS': 301, 'TRAVEL_AND_LOCAL': 207, 'TOOLS': 750, 'PERSONALIZATION': 294, 'PRODUCTIVITY': 345, 'PARENTING': 58, 'WEATHER': 71, 'VIDEO_PLAYERS': 159, 'NEWS_AND_MAGAZINES': 248, 'MAPS_AND_NAVIGATION': 124} In [32]: def clean_convert_install ( install ): install = install . replace ( \",\" , \"\" ) install = install . replace ( \"+\" , \"\" ) return float ( install ) In [33]: map_category_rating = {} for app in free_android_english : category = app [ 1 ] install = clean_convert_install ( app [ 5 ]) if category not in map_category_rating : map_category_rating [ category ] = install else : map_category_rating [ category ] += install In [34]: most_popular_category = [] for category , number in category_freq_table . items (): total = map_category_rating [ category ] average_rating_per_category = total / number most_popular_category . append (( category , average_rating_per_category )) sorted_popular_category = sorted ( most_popular_category , key = lambda elem : elem [ 1 ], reverse = True ) for elem in sorted_popular_category : category_name = elem [ 0 ] average_ratng = elem [ 1 ] print ( ' {} : {:.2f} ' . format ( category_name , average_ratng )) COMMUNICATION:38456119.17 VIDEO_PLAYERS:24727872.45 SOCIAL:23253652.13 PHOTOGRAPHY:17840110.40 PRODUCTIVITY:16787331.34 GAME:15588015.60 TRAVEL_AND_LOCAL:13984077.71 ENTERTAINMENT:11640705.88 TOOLS:10801391.30 NEWS_AND_MAGAZINES:9549178.47 BOOKS_AND_REFERENCE:8767811.89 SHOPPING:7036877.31 PERSONALIZATION:5201482.61 WEATHER:5074486.20 HEALTH_AND_FITNESS:4188821.99 MAPS_AND_NAVIGATION:4056941.77 FAMILY:3695641.82 SPORTS:3638640.14 ART_AND_DESIGN:1986335.09 FOOD_AND_DRINK:1924897.74 EDUCATION:1833495.15 BUSINESS:1712290.15 LIFESTYLE:1437816.27 FINANCE:1387692.48 HOUSE_AND_HOME:1331540.56 DATING:854028.83 COMICS:817657.27 AUTO_AND_VEHICLES:647317.82 LIBRARIES_AND_DEMO:638503.73 PARENTING:542603.62 BEAUTY:513151.89 EVENTS:253542.22 MEDICAL:120550.62 On average, communication apps have the most installs: 38,456,119. This number is heavily skewed up by a few apps that have over one billion installs (WhatsApp, Facebook Messenger, Skype, Google Chrome, Gmail, and Hangouts), and a few others with over 100 and 500 million installs: In [35]: for app in free_android_english : if app [ 1 ] == 'COMMUNICATION' and ( app [ 5 ] == '1,000,000,000+' or app [ 5 ] == '500,000,000+' or app [ 5 ] == '100,000,000+' ): print ( app [ 0 ], ':' , app [ 5 ]) WhatsApp Messenger : 1,000,000,000+ imo beta free calls and text : 100,000,000+ Android Messages : 100,000,000+ Google Duo - High Quality Video Calls : 500,000,000+ Messenger – Text and Video Chat for Free : 1,000,000,000+ imo free video calls and chat : 500,000,000+ Skype - free IM & video calls : 1,000,000,000+ Who : 100,000,000+ GO SMS Pro - Messenger, Free Themes, Emoji : 100,000,000+ LINE: Free Calls & Messages : 500,000,000+ Google Chrome: Fast & Secure : 1,000,000,000+ Firefox Browser fast & private : 100,000,000+ UC Browser - Fast Download Private & Secure : 500,000,000+ Gmail : 1,000,000,000+ Hangouts : 1,000,000,000+ Messenger Lite: Free Calls & Messages : 100,000,000+ Kik : 100,000,000+ KakaoTalk: Free Calls & Text : 100,000,000+ Opera Mini - fast web browser : 100,000,000+ Opera Browser: Fast and Secure : 100,000,000+ Telegram : 100,000,000+ Truecaller: Caller ID, SMS spam blocking & Dialer : 100,000,000+ UC Browser Mini -Tiny Fast Private & Secure : 100,000,000+ Viber Messenger : 500,000,000+ WeChat : 100,000,000+ Yahoo Mail – Stay Organized : 100,000,000+ BBM - Free Calls & Messages : 100,000,000+ We see the same pattern for the video players category, which is the runner-up with 24,727,872 installs. The market is dominated by apps like Youtube, Google Play Movies & TV, or MX Player. The pattern is repeated for social apps (where we have giants like Facebook, Instagram, Google+, etc.), photography apps (Google Photos and other popular photo editors), or productivity apps (Microsoft Word, Dropbox, Google Calendar, Evernote, etc.). Again, the main concern is that these app genres might seem more popular than they really are. Moreover, these niches seem to be dominated by a few giants who are hard to compete against. The game genre seems pretty popular, but previously we found out this part of the market seems a bit saturated, so we'd like to come up with a different app recommendation if possible. The books and reference genre looks fairly popular as well, with an average number of installs of 8,767,811. It's interesting to explore this in more depth, since we found this genre has some potential to work well on the App Store, and our aim is to recommend an app genre that shows potential for being profitable on both the App Store and Google Play In [36]: for app in free_android_english : if app [ 1 ] == 'BOOKS_AND_REFERENCE' : print ( app [ 0 ], ':' , app [ 5 ]) E-Book Read - Read Book for free : 50,000+ Download free book with green book : 100,000+ Wikipedia : 10,000,000+ Cool Reader : 10,000,000+ Free Panda Radio Music : 100,000+ Book store : 1,000,000+ FBReader: Favorite Book Reader : 10,000,000+ English Grammar Complete Handbook : 500,000+ Free Books - Spirit Fanfiction and Stories : 1,000,000+ Google Play Books : 1,000,000,000+ AlReader -any text book reader : 5,000,000+ Offline English Dictionary : 100,000+ Offline: English to Tagalog Dictionary : 500,000+ FamilySearch Tree : 1,000,000+ Cloud of Books : 1,000,000+ Recipes of Prophetic Medicine for free : 500,000+ ReadEra – free ebook reader : 1,000,000+ Anonymous caller detection : 10,000+ Ebook Reader : 5,000,000+ Litnet - E-books : 100,000+ Read books online : 5,000,000+ English to Urdu Dictionary : 500,000+ eBoox: book reader fb2 epub zip : 1,000,000+ English Persian Dictionary : 500,000+ Flybook : 500,000+ All Maths Formulas : 1,000,000+ Ancestry : 5,000,000+ HTC Help : 10,000,000+ English translation from Bengali : 100,000+ Pdf Book Download - Read Pdf Book : 100,000+ Free Book Reader : 100,000+ eBoox new: Reader for fb2 epub zip books : 50,000+ Only 30 days in English, the guideline is guaranteed : 500,000+ Moon+ Reader : 10,000,000+ SH-02J Owner's Manual (Android 8.0) : 50,000+ English-Myanmar Dictionary : 1,000,000+ Golden Dictionary (EN-AR) : 1,000,000+ All Language Translator Free : 1,000,000+ Azpen eReader : 500,000+ URBANO V 02 instruction manual : 100,000+ Bible : 100,000,000+ C Programs and Reference : 50,000+ C Offline Tutorial : 1,000+ C Programs Handbook : 50,000+ Amazon Kindle : 100,000,000+ Aab e Hayat Full Novel : 100,000+ Aldiko Book Reader : 10,000,000+ Google I/O 2018 : 500,000+ R Language Reference Guide : 10,000+ Learn R Programming Full : 5,000+ R Programing Offline Tutorial : 1,000+ Guide for R Programming : 5+ Learn R Programming : 10+ R Quick Reference Big Data : 1,000+ V Made : 100,000+ Wattpad 📖 Free Books : 100,000,000+ Dictionary - WordWeb : 5,000,000+ Guide (for X-MEN) : 100,000+ AC Air condition Troubleshoot,Repair,Maintenance : 5,000+ AE Bulletins : 1,000+ Ae Allah na Dai (Rasa) : 10,000+ 50000 Free eBooks & Free AudioBooks : 5,000,000+ Ag PhD Field Guide : 10,000+ Ag PhD Deficiencies : 10,000+ Ag PhD Planting Population Calculator : 1,000+ Ag PhD Soybean Diseases : 1,000+ Fertilizer Removal By Crop : 50,000+ A-J Media Vault : 50+ Al-Quran (Free) : 10,000,000+ Al Quran (Tafsir & by Word) : 500,000+ Al Quran Indonesia : 10,000,000+ Al'Quran Bahasa Indonesia : 10,000,000+ Al Quran Al karim : 1,000,000+ Al-Muhaffiz : 50,000+ Al Quran : EAlim - Translations & MP3 Offline : 5,000,000+ Al-Quran 30 Juz free copies : 500,000+ Koran Read &MP3 30 Juz Offline : 1,000,000+ Hafizi Quran 15 lines per page : 1,000,000+ Quran for Android : 10,000,000+ Surah Al-Waqiah : 100,000+ Hisnul Al Muslim - Hisn Invocations & Adhkaar : 100,000+ Satellite AR : 1,000,000+ Audiobooks from Audible : 100,000,000+ Kinot & Eichah for Tisha B'Av : 10,000+ AW Tozer Devotionals - Daily : 5,000+ Tozer Devotional -Series 1 : 1,000+ The Pursuit of God : 1,000+ AY Sing : 5,000+ Ay Hasnain k Nana Milad Naat : 10,000+ Ay Mohabbat Teri Khatir Novel : 10,000+ Arizona Statutes, ARS (AZ Law) : 1,000+ Oxford A-Z of English Usage : 1,000,000+ BD Fishpedia : 1,000+ BD All Sim Offer : 10,000+ Youboox - Livres, BD et magazines : 500,000+ B&H Kids AR : 10,000+ B y H Niños ES : 5,000+ Dictionary.com: Find Definitions for English Words : 10,000,000+ English Dictionary - Offline : 10,000,000+ Bible KJV : 5,000,000+ Borneo Bible, BM Bible : 10,000+ MOD Black for BM : 100+ BM Box : 1,000+ Anime Mod for BM : 100+ NOOK: Read eBooks & Magazines : 10,000,000+ NOOK Audiobooks : 500,000+ NOOK App for NOOK Devices : 500,000+ Browsery by Barnes & Noble : 5,000+ bp e-store : 1,000+ Brilliant Quotes: Life, Love, Family & Motivation : 1,000,000+ BR Ambedkar Biography & Quotes : 10,000+ BU Alsace : 100+ Catholic La Bu Zo Kam : 500+ Khrifa Hla Bu (Solfa) : 10+ Kristian Hla Bu : 10,000+ SA HLA BU : 1,000+ Learn SAP BW : 500+ Learn SAP BW on HANA : 500+ CA Laws 2018 (California Laws and Codes) : 5,000+ Bootable Methods(USB-CD-DVD) : 10,000+ cloudLibrary : 100,000+ SDA Collegiate Quarterly : 500+ Sabbath School : 100,000+ Cypress College Library : 100+ Stats Royale for Clash Royale : 1,000,000+ GATE 21 years CS Papers(2011-2018 Solved) : 50+ Learn CT Scan Of Head : 5,000+ Easy Cv maker 2018 : 10,000+ How to Write CV : 100,000+ CW Nuclear : 1,000+ CY Spray nozzle : 10+ BibleRead En Cy Zh Yue : 5+ CZ-Help : 5+ Modlitební knížka CZ : 500+ Guide for DB Xenoverse : 10,000+ Guide for DB Xenoverse 2 : 10,000+ Guide for IMS DB : 10+ DC HSEMA : 5,000+ DC Public Library : 1,000+ Painting Lulu DC Super Friends : 1,000+ Dictionary : 10,000,000+ Fix Error Google Playstore : 1,000+ D. H. Lawrence Poems FREE : 1,000+ Bilingual Dictionary Audio App : 5,000+ DM Screen : 10,000+ wikiHow: how to do anything : 1,000,000+ Dr. Doug's Tips : 1,000+ Bible du Semeur-BDS (French) : 50,000+ La citadelle du musulman : 50,000+ DV 2019 Entry Guide : 10,000+ DV 2019 - EDV Photo & Form : 50,000+ DV 2018 Winners Guide : 1,000+ EB Annual Meetings : 1,000+ EC - AP & Telangana : 5,000+ TN Patta Citta & EC : 10,000+ AP Stamps and Registration : 10,000+ CompactiMa EC pH Calibration : 100+ EGW Writings 2 : 100,000+ EGW Writings : 1,000,000+ Bible with EGW Comments : 100,000+ My Little Pony AR Guide : 1,000,000+ SDA Sabbath School Quarterly : 500,000+ Duaa Ek Ibaadat : 5,000+ Spanish English Translator : 10,000,000+ Dictionary - Merriam-Webster : 10,000,000+ JW Library : 10,000,000+ Oxford Dictionary of English : Free : 10,000,000+ English Hindi Dictionary : 10,000,000+ English to Hindi Dictionary : 5,000,000+ EP Research Service : 1,000+ Hymnes et Louanges : 100,000+ EU Charter : 1,000+ EU Data Protection : 1,000+ EU IP Codes : 100+ EW PDF : 5+ BakaReader EX : 100,000+ EZ Quran : 50,000+ FA Part 1 & 2 Past Papers Solved Free – Offline : 5,000+ La Fe de Jesus : 1,000+ La Fe de Jesús : 500+ Le Fe de Jesus : 500+ Florida - Pocket Brainbook : 1,000+ Florida Statutes (FL Code) : 1,000+ English To Shona Dictionary : 10,000+ Greek Bible FP (Audio) : 1,000+ Golden Dictionary (FR-AR) : 500,000+ Fanfic-FR : 5,000+ Bulgarian French Dictionary Fr : 10,000+ Chemin (fr) : 1,000+ The SCP Foundation DB fr nn5n : 1,000+ The book and reference genre includes a variety of apps: software for processing and reading ebooks, various collections of libraries, dictionaries, tutorials on programming or languages, etc. It seems there's still a small number of extremely popular apps that skew the average: In [37]: for app in free_android_english : if app [ 1 ] == 'BOOKS_AND_REFERENCE' and ( app [ 5 ] == '1,000,000,000+' or app [ 5 ] == '500,000,000+' or app [ 5 ] == '100,000,000+' ): print ( app [ 0 ], ':' , app [ 5 ]) Google Play Books : 1,000,000,000+ Bible : 100,000,000+ Amazon Kindle : 100,000,000+ Wattpad 📖 Free Books : 100,000,000+ Audiobooks from Audible : 100,000,000+ However, it looks like there are only a few very popular apps, so this market still shows potential. Let's try to get some app ideas based on the kind of apps that are somewhere in the middle in terms of popularity (between 1,000,000 and 100,000,000 downloads): In [38]: for app in free_android_english : if app [ 1 ] == 'BOOKS_AND_REFERENCE' and ( app [ 5 ] == '1,000,000+' or app [ 5 ] == '5,000,000+' or app [ 5 ] == '10,000,000+' or app [ 5 ] == '50,000,000+' ): print ( app [ 0 ], ':' , app [ 5 ]) Wikipedia : 10,000,000+ Cool Reader : 10,000,000+ Book store : 1,000,000+ FBReader: Favorite Book Reader : 10,000,000+ Free Books - Spirit Fanfiction and Stories : 1,000,000+ AlReader -any text book reader : 5,000,000+ FamilySearch Tree : 1,000,000+ Cloud of Books : 1,000,000+ ReadEra – free ebook reader : 1,000,000+ Ebook Reader : 5,000,000+ Read books online : 5,000,000+ eBoox: book reader fb2 epub zip : 1,000,000+ All Maths Formulas : 1,000,000+ Ancestry : 5,000,000+ HTC Help : 10,000,000+ Moon+ Reader : 10,000,000+ English-Myanmar Dictionary : 1,000,000+ Golden Dictionary (EN-AR) : 1,000,000+ All Language Translator Free : 1,000,000+ Aldiko Book Reader : 10,000,000+ Dictionary - WordWeb : 5,000,000+ 50000 Free eBooks & Free AudioBooks : 5,000,000+ Al-Quran (Free) : 10,000,000+ Al Quran Indonesia : 10,000,000+ Al'Quran Bahasa Indonesia : 10,000,000+ Al Quran Al karim : 1,000,000+ Al Quran : EAlim - Translations & MP3 Offline : 5,000,000+ Koran Read &MP3 30 Juz Offline : 1,000,000+ Hafizi Quran 15 lines per page : 1,000,000+ Quran for Android : 10,000,000+ Satellite AR : 1,000,000+ Oxford A-Z of English Usage : 1,000,000+ Dictionary.com: Find Definitions for English Words : 10,000,000+ English Dictionary - Offline : 10,000,000+ Bible KJV : 5,000,000+ NOOK: Read eBooks & Magazines : 10,000,000+ Brilliant Quotes: Life, Love, Family & Motivation : 1,000,000+ Stats Royale for Clash Royale : 1,000,000+ Dictionary : 10,000,000+ wikiHow: how to do anything : 1,000,000+ EGW Writings : 1,000,000+ My Little Pony AR Guide : 1,000,000+ Spanish English Translator : 10,000,000+ Dictionary - Merriam-Webster : 10,000,000+ JW Library : 10,000,000+ Oxford Dictionary of English : Free : 10,000,000+ English Hindi Dictionary : 10,000,000+ English to Hindi Dictionary : 5,000,000+ This niche seems to be dominated by software for processing and reading ebooks, as well as various collections of libraries and dictionaries, so it's probably not a good idea to build similar apps since there'll be some significant competition. We also notice there are quite a few apps built around the book Quran, which suggests that building an app around a popular book can be profitable. It seems that taking a popular book (perhaps a more recent book) and turning it into an app could be profitable for both the Google Play and the App Store markets. However, it looks like the market is already full of libraries, so we need to add some special features besides the raw version of the book. This might include daily quotes from the book, an audio version of the book, quizzes on the book, a forum where people can discuss the book, etc. Conclusions In this project, we analyzed data about the App Store and Google Play mobile apps with the goal of recommending an app profile that can be profitable for both markets. We concluded that taking a popular book (perhaps a more recent book) and turning it into an app could be profitable for both the Google Play and the App Store markets. The markets are already full of libraries, so we need to add some special features besides the raw version of the book. This might include daily quotes from the book, an audio version of the book, quizzes on the book, a forum where people can discuss the book, etc.","tags":"Data Analysis","url":"https://apurba-saha.github.io/profitable-app-profiles-for-the-app-store-and-google-play-markets","loc":"https://apurba-saha.github.io/profitable-app-profiles-for-the-app-store-and-google-play-markets"}]};