{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the project\n",
    "Hacker News is a site, where user-submitted stories (known as \"posts\") are voted and commented upon, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result. We're specifically interested in posts whose titles begin with either `Ask HN` or `Show HN`. Users submit `Ask HN` posts to ask the Hacker News community a specific question. Likewise, users submit `Show HN` posts to show the Hacker News community a project, product, or just generally something interesting. The goal of this project is to answer the following two questions:\n",
    "* Do `Ask HN` or `Show HN` recieve more comments on average?\n",
    "* Do posts created at a certain time receive more comments on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "To find answers for our questions, we will analyse the hacker news post dataset, which can be downloaded from [this link](https://www.kaggle.com/hacker-news/hacker-news-posts). Note that the dataset has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments, and then randomly sampling from the remaining submissions. Let's first read the dataset as a list of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "opened_file = open(\"hacker_news.csv\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "hn_header = hn[0] # Seperate the header from the dataset for future analysis\n",
    "hn = hn[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "\n",
      "\n",
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "\n",
      "\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "\n",
      "\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "\n",
      "\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "\n",
      "\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n",
      "\n",
      "\n",
      "The number of rows:  20100\n",
      "The number of columns:  7\n"
     ]
    }
   ],
   "source": [
    "def explore_data(dataset, header = [], rows_and_column = True):\n",
    "    if header:\n",
    "        print(header)\n",
    "    print('\\n')\n",
    "    \n",
    "    for row in dataset[:5]:\n",
    "        print(row)\n",
    "        print('\\n')\n",
    "        \n",
    "    if rows_and_column:\n",
    "        total_column = len(header) if header else len(dataset[0])\n",
    "        print(\"The number of rows: \", len(dataset))\n",
    "        print(\"The number of columns: \", total_column)\n",
    "        \n",
    "explore_data(hn, hn_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the descriptions of the columns:\n",
    "\n",
    "| Column       | Description|\n",
    "|:--------------|:-----------------------------------------------|\n",
    "| id| The unique identifier from Hacker News for the post|\n",
    "| title        | The title of the post|\n",
    "| url| The URL that the posts links to, if the post has a URL|\n",
    "| num_points | The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes|\n",
    "| num_comments | The number of comments that were made on the post|\n",
    "| author       | The username of the person who submitted the post|\n",
    "| created_at   | The date and time at which the post was submitted  (the time zone is Eastern Time in the US) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "In this project, we are only interested in those posts that starts with either `Ask HN` or `Show HN`. Thus we only create a new lists of lists containing just the data for those titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring ASK posts...\n",
      "\n",
      "\n",
      "['12296411', 'Ask HN: How to improve my personal website?', '', '2', '6', 'ahmedbaracat', '8/16/2016 9:55']\n",
      "\n",
      "\n",
      "['10610020', 'Ask HN: Am I the only one outraged by Twitter shutting down share counts?', '', '28', '29', 'tkfx', '11/22/2015 13:43']\n",
      "\n",
      "\n",
      "['11610310', 'Ask HN: Aby recent changes to CSS that broke mobile?', '', '1', '1', 'polskibus', '5/2/2016 10:14']\n",
      "\n",
      "\n",
      "['12210105', 'Ask HN: Looking for Employee #3 How do I do it?', '', '1', '3', 'sph130', '8/2/2016 14:20']\n",
      "\n",
      "\n",
      "['10394168', 'Ask HN: Someone offered to buy my browser extension from me. What now?', '', '28', '17', 'roykolak', '10/15/2015 16:38']\n",
      "\n",
      "\n",
      "The number of rows:  1744\n",
      "The number of columns:  7\n",
      "\n",
      "\n",
      "Exploring SHOW posts...\n",
      "\n",
      "\n",
      "['10627194', 'Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform', 'https://iot.seeed.cc', '26', '22', 'kfihihc', '11/25/2015 14:03']\n",
      "\n",
      "\n",
      "['10646440', 'Show HN: Something pointless I made', 'http://dn.ht/picklecat/', '747', '102', 'dhotson', '11/29/2015 22:46']\n",
      "\n",
      "\n",
      "['11590768', 'Show HN: Shanhu.io, a programming playground powered by e8vm', 'https://shanhu.io', '1', '1', 'h8liu', '4/28/2016 18:05']\n",
      "\n",
      "\n",
      "['12178806', 'Show HN: Webscope  Easy way for web developers to communicate with Clients', 'http://webscopeapp.com', '3', '3', 'fastbrick', '7/28/2016 7:11']\n",
      "\n",
      "\n",
      "['10872799', 'Show HN: GeoScreenshot  Easily test Geo-IP based web pages', 'https://www.geoscreenshot.com/', '1', '9', 'kpsychwave', '1/9/2016 20:45']\n",
      "\n",
      "\n",
      "The number of rows:  1162\n",
      "The number of columns:  7\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    title = title.lower()\n",
    "    \n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "print(\"Exploring ASK posts...\")\n",
    "explore_data(ask_posts)\n",
    "print('\\n')\n",
    "print(\"Exploring SHOW posts...\")\n",
    "explore_data(show_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if there are any rows with missing data. If data is missing from any row, we will delete that row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows deleted from ask_posts:  0\n",
      "Total rows deleted from show_posts:  0\n"
     ]
    }
   ],
   "source": [
    "def check_and_remove(dataset, total_column):\n",
    "    removed_items = []\n",
    "    for row in dataset:\n",
    "        if len(row) < total_column:\n",
    "            removed_items.append(row)\n",
    "            dataset.remove(row)\n",
    "            \n",
    "    return removed_items\n",
    "            \n",
    "removed_ask_post_rows = check_and_remove(ask_posts, 7)\n",
    "print(\"Total rows deleted from ask_posts: \", \n",
    "      len(removed_ask_post_rows))\n",
    "removed_show_post_rows = check_and_remove(show_posts, 7)\n",
    "print(\"Total rows deleted from show_posts: \", \n",
    "      len(removed_show_post_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! No data is missing from any rows. Next, we need to check if there is any duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicates found in ask_post:  0\n",
      "Total duplicates found in show_post:  0\n"
     ]
    }
   ],
   "source": [
    "def check_duplicate(dataset):\n",
    "    duplicate_rows = []\n",
    "    already_added = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        id = row[0]\n",
    "        if id not in already_added:\n",
    "            already_added.append(id)\n",
    "        else:\n",
    "            duplicate_rows.append(row)\n",
    "            \n",
    "    return duplicate_rows\n",
    "\n",
    "duplicate_ask_posts = check_duplicate(ask_posts)\n",
    "duplicate_show_posts = check_duplicate(show_posts)\n",
    "\n",
    "print(\"Total duplicates found in ask_post: \", len(duplicate_ask_posts))\n",
    "print(\"Total duplicates found in show_post: \", len(duplicate_show_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo! Even no duplicates found in the dataset. That makes our life easier, isn't it? However, out dataset is almost clean. Even the dates in created_at column follows a consistent format. So, there is no necessity to standardize the date entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "As we have cleaned the data, we will now start analyze the dataset to find answers for our questions. At first, let's determine if ask posts or show posts receive more comments on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_avg_comment(dataset):\n",
    "    total_comments = 0\n",
    "    for post in dataset:\n",
    "        num_comment = int(post[4])\n",
    "        total_comments += num_comment\n",
    "\n",
    "    avg_comments = total_comments / len(dataset)\n",
    "    return avg_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments on ask posts: 14.04\n"
     ]
    }
   ],
   "source": [
    "avg_ask_comments = find_avg_comment(ask_posts)\n",
    "print(\"The average number of comments on ask posts: {:.2f}\".format(avg_ask_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments on show posts: 10.32\n"
     ]
    }
   ],
   "source": [
    "avg_show_comments = find_avg_comment(show_posts)\n",
    "print(\"The average number of comments on show posts: {:.2f}\".format(avg_show_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments on other posts: 26.87\n"
     ]
    }
   ],
   "source": [
    "avg_other_comments = find_avg_comment(other_posts)\n",
    "print(\"The average number of comments on other posts: {:.2f}\".format(avg_other_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the ask posts recieve  more comments than show posts on average. Since ask posts are more likely to receive comments, we'll focus our remaining analysis just on these posts. Next, we'll determine if ask posts created at a certain time are more likely to attract comments. We'll use the following steps to perform this analysis:\n",
    "* Calculate the amount of ask posts created in each hour of the day, along with the number of comments received.\n",
    "* Calculate the average number of comments ask posts receive by hour created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8/16/2016 9:55', '6'], ['11/22/2015 13:43', '29'], ['5/2/2016 10:14', '1'], ['8/2/2016 14:20', '3'], ['10/15/2015 16:38', '17']]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list = []\n",
    "\n",
    "for post in ask_posts:\n",
    "    created_at = post[6]\n",
    "    num_comment = post[4]\n",
    "    result_list.append([created_at, num_comment])\n",
    "    \n",
    "print(result_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    comment_num = int(row[1])\n",
    "    dt_object = dt.datetime.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    post_hour = dt_object.hour\n",
    "    \n",
    "    if post_hour not in counts_by_hour:\n",
    "        counts_by_hour[post_hour] = 1\n",
    "        comments_by_hour[post_hour] = comment_num\n",
    "    else:\n",
    "        counts_by_hour[post_hour] += 1\n",
    "        comments_by_hour[post_hour] += comment_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15: 116, 19: 110, 21: 109, 18: 109, 16: 108, 14: 107, 17: 100, 13: 85, 20: 80, 12: 73, 22: 71, 23: 68, 1: 60, 10: 59, 2: 58, 11: 58, 0: 55, 3: 54, 8: 48, 4: 47, 5: 46, 9: 45, 6: 44, 7: 34}\n"
     ]
    }
   ],
   "source": [
    "# print the number of posts by hour in descending order\n",
    "print({k:v for k, v in sorted(counts_by_hour.items(), \n",
    "                              key = lambda elem: elem[1], reverse = True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15: 4477, 16: 1814, 21: 1745, 20: 1722, 18: 1439, 14: 1416, 2: 1381, 13: 1253, 19: 1188, 17: 1146, 10: 793, 12: 687, 1: 683, 11: 641, 23: 543, 8: 492, 22: 479, 5: 464, 0: 447, 3: 421, 6: 397, 4: 337, 7: 267, 9: 251}\n"
     ]
    }
   ],
   "source": [
    "# print the number of comments by hour in descending order\n",
    "print({k:v for k, v in sorted(comments_by_hour.items(), \n",
    "                              key = lambda elem: elem[1], reverse = True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 5.5777777777777775], [13, 14.741176470588234], [10, 13.440677966101696], [14, 13.233644859813085]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "for key, val in comments_by_hour.items():\n",
    "    avg = val / counts_by_hour[key]\n",
    "    avg_by_hour.append([key, avg])\n",
    "    \n",
    "print(avg_by_hour[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hours for Ask Posts Comments:\n",
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.80 average comments per post\n",
      "21:00: 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "sorted_by_avg_comment = sorted(avg_by_hour, key = lambda elem: elem[1], \n",
    "                               reverse = True)\n",
    "\n",
    "print(\"Top 5 hours for Ask Posts Comments:\")\n",
    "\n",
    "for elem in sorted_by_avg_comment[:5]:\n",
    "    dt_object = dt.datetime.strptime(str(elem[0]), \"%H\")\n",
    "    hour = dt_object.strftime(\"%H:%M\")\n",
    "    avg_comment = elem[1]\n",
    "    print(\"{0}: {1:.2f} average comments per post\".format(hour, avg_comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Now, we know which hours we should create a post during to have a higher chance of recieving comments. But if we refer back to the documentation for the data set, we will find that the Eastern time zone has been followed in the given dataset. So, we need to convert the time zone we live in. For example, I live in Arizona, USA. Hence, 3.00pm in Eastern timezone is equivalent to 12.00pm in my timezone. So, I may submit an ask post on HN at 12.00pm to maximize the chance of recieving comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
